{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6A7Q1EKppw8L"
      },
      "source": [
        "In this code, we are building a deep learning model using the Xception architecture as a feature extractor, followed by an SVM (Support Vector Machine) for classification. Initially, we load the pre-trained Xception model (without the top classification layer) and apply global average pooling on the output to reduce its dimensionality. Then, we create a new output layer with 2048 units and a ReLU activation function, which serves as an intermediate feature representation.\n",
        "\n",
        "We then train the model to extract these features from images, specifically categorizing images as either 'Necrosis' or 'Non_Necrosis.' The features from the Xception model are saved as CSV files, along with the image labels and filenames. These features are then used to train an SVM classifier, which is evaluated based on performance metrics such as precision, recall, F1 score, accuracy, sensitivity, specificity, and AUC-ROC.\n",
        "\n",
        "For multiple batches, the code processes training and testing image data, extracts features using Xception, trains the SVM model, and stores the evaluation results in CSV files. The overall goal is to combine deep learning feature extraction with a traditional machine learning classifier (SVM) to detect and classify images based on the presence of necrosis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4zAqxp6xurFk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IKUkkiopvUu_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking for zip file at: /home/llmPathoUser/pathologyStudentsAug25/pathologyStudentsAug25/necrosisDataSeg/WR_Necrosis 3.0.zip\n",
            "Will extract to: /home/llmPathoUser/pathologyStudentsAug25/pathologyStudentsAug25/WR_Necrosis_Seg 3.0\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/home/llmPathoUser/pathologyStudentsAug25/pathologyStudentsAug25/necrosisDataSeg/WR_Necrosis 3.0.zip'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLooking for zip file at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_to_zip_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWill extract to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextract_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mzipfile\u001b[49m\u001b[43m.\u001b[49m\u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_to_zip_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m zip_ref:\n\u001b[32m     21\u001b[39m     zip_ref.extractall(extract_dir)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/zipfile/__init__.py:1347\u001b[39m, in \u001b[36mZipFile.__init__\u001b[39m\u001b[34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[39m\n\u001b[32m   1345\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m   1346\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1347\u001b[39m         \u001b[38;5;28mself\u001b[39m.fp = \u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilemode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1348\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[32m   1349\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m filemode \u001b[38;5;129;01min\u001b[39;00m modeDict:\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/home/llmPathoUser/pathologyStudentsAug25/pathologyStudentsAug25/necrosisDataSeg/WR_Necrosis 3.0.zip'"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Get the current notebook's directory and navigate to the correct location\n",
        "current_dir = os.path.dirname(os.path.abspath('__file__'))\n",
        "# Go up to first pathologyStudentsAug25\n",
        "parent_dir = os.path.dirname(current_dir)\n",
        "# Go up to Downloads\n",
        "root_dir = os.path.dirname(parent_dir)\n",
        "# Construct path to zip file\n",
        "path_to_zip_file = os.path.join(root_dir, 'pathologyStudentsAug25', 'necrosisDataSeg', 'WR_Necrosis 3.0.zip')\n",
        "\n",
        "# Create extraction directory\n",
        "extract_dir = os.path.join(root_dir, 'pathologyStudentsAug25', 'WR_Necrosis_Seg 3.0')\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "print(f\"Looking for zip file at: {path_to_zip_file}\")\n",
        "print(f\"Will extract to: {extract_dir}\")\n",
        "\n",
        "with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmhNkymAvUx7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking for zip file at: d:\\pathologyStudentsAug25\\pathologyStudentsAug25\\necrosisData\\WR_Non Necrosis 3.0.zip\n",
            "Will extract to: d:\\pathologyStudentsAug25\\pathologyStudentsAug25\\WR_Non_Necrosis 3.0\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Get the current notebook's directory and navigate to the correct location\n",
        "current_dir = os.path.dirname(os.path.abspath('__file__'))\n",
        "# Go up to first pathologyStudentsAug25\n",
        "parent_dir = os.path.dirname(current_dir)\n",
        "# Go up to Downloads\n",
        "root_dir = os.path.dirname(parent_dir)\n",
        "# Construct path to zip file\n",
        "path_to_zip_file = os.path.join(root_dir, 'pathologyStudentsAug25', 'necrosisDataSeg', 'WR_Non Necrosis 3.0.zip')\n",
        "\n",
        "# Create extraction directory\n",
        "extract_dir = os.path.join(root_dir, 'pathologyStudentsAug25', 'WR_Non_Necrosis_Seg 3.0')\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "print(f\"Looking for zip file at: {path_to_zip_file}\")\n",
        "print(f\"Will extract to: {extract_dir}\")\n",
        "\n",
        "with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yi9vZLPLvU0W"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import shutil\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Eui56bGcvYGX"
      },
      "outputs": [],
      "source": [
        "def batch_preparation(batch_number,N_train,NN_train,N_test,NN_test,N_train_per,NN_train_per,N_test_per,NN_test_per):\n",
        "  # Get the current notebook's directory and navigate to the correct location\n",
        "  current_dir = os.path.dirname(os.path.abspath('__file__'))\n",
        "  parent_dir = os.path.dirname(current_dir)\n",
        "  root_dir = os.path.dirname(parent_dir)\n",
        "  \n",
        "  # Create batch directory structure\n",
        "  batch_dir = os.path.join(root_dir, 'pathologyStudentsAug25','Segmented_Batch',f'Batch_{batch_number}')\n",
        "  os.makedirs(os.path.join(batch_dir, 'Training', 'Necrosis'), exist_ok=True)\n",
        "  os.makedirs(os.path.join(batch_dir, 'Training', 'Non_Necrosis'), exist_ok=True)\n",
        "  os.makedirs(os.path.join(batch_dir, 'Testing', 'Necrosis'), exist_ok=True)\n",
        "  os.makedirs(os.path.join(batch_dir, 'Testing', 'Non_Necrosis'), exist_ok=True)\n",
        "\n",
        "  # Define source directories\n",
        "  necrosis_dir = os.path.join(root_dir, 'pathologyStudentsAug25', 'segmented_unetpp','necrosis')\n",
        "  non_necrosis_dir = os.path.join(root_dir, 'pathologyStudentsAug25','segmented_unetpp','non_necrosis')\n",
        "  runtime_dir = os.path.join(root_dir, 'pathologyStudentsAug25', 'Run_time_Folder_Seg')\n",
        "\n",
        "  for i in range(len(N_train)):\n",
        "    N_train[i]=str(N_train[i])\n",
        "    if N_train_per[i]==1:\n",
        "      print('Processing Necrosis in Training for Sample Number {} '.format(N_train[i]))\n",
        "      print('\\n')\n",
        "      src_dir = os.path.join(necrosis_dir, 'Sample' + N_train[i])\n",
        "      dst_dir = os.path.join(batch_dir, 'Training', 'Necrosis')\n",
        "      for jpgfile in glob.iglob(os.path.join(src_dir, \"*.png\")):\n",
        "        shutil.copy(jpgfile, dst_dir)\n",
        "    else:\n",
        "      print('Processing Necrosis in Testing for Sample Number {}'.format(N_train[i]))\n",
        "      print('\\n')\n",
        "      src_dir = os.path.join(necrosis_dir, 'Sample' + N_train[i])\n",
        "      files_names = os.listdir(src_dir)\n",
        "      files_count = len(files_names)\n",
        "      selected_number_of_files = int(N_train_per[i]*files_count)\n",
        "      selected_files = files_names[0:selected_number_of_files]\n",
        "      os.makedirs(runtime_dir, exist_ok=True)\n",
        "      for j in range(selected_number_of_files):\n",
        "        shutil.copy(os.path.join(src_dir, selected_files[j]), runtime_dir)\n",
        "      dst_dir = os.path.join(batch_dir, 'Training', 'Necrosis')\n",
        "      for jpgfile in glob.iglob(os.path.join(runtime_dir, \"*.png\")):\n",
        "        shutil.copy(jpgfile, dst_dir)\n",
        "      shutil.rmtree(runtime_dir, ignore_errors=True)\n",
        "\n",
        "  for i in range(len(NN_train)):\n",
        "    NN_train[i]=str(NN_train[i])\n",
        "    if NN_train_per[i]==1:\n",
        "      print('Processing Non_Necrosis in Training for Sample Number {}'.format(NN_train[i]))\n",
        "      print('\\n')\n",
        "      src_dir = os.path.join(non_necrosis_dir, 'Sample' + NN_train[i])\n",
        "      dst_dir = os.path.join(batch_dir, 'Training', 'Non_Necrosis')\n",
        "      for jpgfile in glob.iglob(os.path.join(src_dir, \"*.png\")):\n",
        "        shutil.copy(jpgfile, dst_dir)\n",
        "    else:\n",
        "      print('Processing Non_Necrosis in Testing for Sample Number {}'.format(NN_train[i]))\n",
        "      print('\\n')\n",
        "      src_dir = os.path.join(non_necrosis_dir, 'Sample' + NN_train[i])\n",
        "      files_names = os.listdir(src_dir)\n",
        "      files_count = len(files_names)\n",
        "      selected_number_of_files = int(NN_train_per[i]*files_count)\n",
        "      selected_files = files_names[0:selected_number_of_files]\n",
        "      os.makedirs(runtime_dir, exist_ok=True)\n",
        "      for j in range(selected_number_of_files):\n",
        "        shutil.copy(os.path.join(src_dir, selected_files[j]), runtime_dir)\n",
        "      dst_dir = os.path.join(batch_dir, 'Training', 'Non_Necrosis')\n",
        "      for jpgfile in glob.iglob(os.path.join(runtime_dir, \"*.png\")):\n",
        "        shutil.copy(jpgfile, dst_dir)\n",
        "      shutil.rmtree(runtime_dir, ignore_errors=True)\n",
        "\n",
        "  for i in range(len(N_test)):\n",
        "    N_test[i]=str(N_test[i])\n",
        "    if N_test_per[i]==1:\n",
        "      print('Processing Necrosis in Testing for Sample Number {}'.format(N_test[i]))\n",
        "      print('\\n')\n",
        "      src_dir = os.path.join(necrosis_dir, 'Sample' + N_test[i])\n",
        "      dst_dir = os.path.join(batch_dir, 'Testing', 'Necrosis')\n",
        "      for jpgfile in glob.iglob(os.path.join(src_dir, \"*.png\")):\n",
        "        shutil.copy(jpgfile, dst_dir)\n",
        "    else:\n",
        "      print('Processing Necrosis in Testing for Sample Number {}'.format(N_test[i]))\n",
        "      print('\\n')\n",
        "      src_dir = os.path.join(necrosis_dir, 'Sample' + N_test[i])\n",
        "      files_names = os.listdir(src_dir)\n",
        "      files_count = len(files_names)\n",
        "      selected_number_of_files = int(N_test_per[i]*files_count)\n",
        "      selected_files = files_names[selected_number_of_files:files_count]\n",
        "      os.makedirs(runtime_dir, exist_ok=True)\n",
        "      for j in range(selected_number_of_files):\n",
        "        shutil.copy(os.path.join(src_dir, selected_files[j]), runtime_dir)\n",
        "      dst_dir = os.path.join(batch_dir, 'Testing', 'Necrosis')\n",
        "      for jpgfile in glob.iglob(os.path.join(runtime_dir, \"*.png\")):\n",
        "        shutil.copy(jpgfile, dst_dir)\n",
        "      shutil.rmtree(runtime_dir, ignore_errors=True)\n",
        "\n",
        "  for i in range(len(NN_test)):\n",
        "    NN_test[i]=str(NN_test[i])\n",
        "    if NN_test_per[i]==1:\n",
        "      print('Processing Non Necrosis in Testing for Sample Number {}'.format(NN_test[i]))\n",
        "      print('\\n')\n",
        "      src_dir = os.path.join(non_necrosis_dir, 'Sample' + NN_test[i])\n",
        "      dst_dir = os.path.join(batch_dir, 'Testing', 'Non_Necrosis')\n",
        "      for jpgfile in glob.iglob(os.path.join(src_dir, \"*.png\")):\n",
        "        shutil.copy(jpgfile, dst_dir)\n",
        "    else:\n",
        "      print('Processing Non Necrosis in Testing for Sample Number {}'.format(NN_test[i]))\n",
        "      src_dir = os.path.join(non_necrosis_dir, 'Sample' + NN_test[i])\n",
        "      files_names = os.listdir(src_dir)\n",
        "      files_count = len(files_names)\n",
        "      selected_number_of_files = int(NN_test_per[i]*files_count)\n",
        "      selected_files = files_names[selected_number_of_files:files_count]\n",
        "      os.makedirs(runtime_dir, exist_ok=True)\n",
        "      for j in range(selected_number_of_files):\n",
        "        shutil.copy(os.path.join(src_dir, selected_files[j]), runtime_dir)\n",
        "      dst_dir = os.path.join(batch_dir, 'Testing', 'Non_Necrosis')\n",
        "      for jpgfile in glob.iglob(os.path.join(runtime_dir, \"*.png\")):\n",
        "        shutil.copy(jpgfile, dst_dir)\n",
        "      shutil.rmtree(runtime_dir, ignore_errors=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZeG6CjRTvacq"
      },
      "outputs": [],
      "source": [
        "Batches=[1,2,3,4,5,6,7]\n",
        "\n",
        "\n",
        "B1_Train_N=[7,6,4]\n",
        "B1_Train_Per=[0.8,0.8,0.8]\n",
        "B1_Train_NN=[2,4,5,6,8,9]\n",
        "B1_Train_NN_Per=[0.8,0.8,0.8,0.8,0.8,0.8]\n",
        "B1_Test_N=[2]\n",
        "B1_Test_Per=[0.2]\n",
        "B1_Test_NN=[1]\n",
        "B1_Test_NN_Per=[0.2]\n",
        "\n",
        "B2_Train_N=[7,6,2]\n",
        "B2_Train_Per=[0.8,0.8,0.8]\n",
        "B2_Train_NN=[2,1,4,6,8,9]\n",
        "B2_Train_NN_Per=[0.8,0.8,0.8,0.8,0.8,0.8]\n",
        "B2_Test_N=[4]\n",
        "B2_Test_Per=[0.2]\n",
        "B2_Test_NN=[5]\n",
        "B2_Test_NN_Per=[0.2]\n",
        "\n",
        "B3_Train_N=[7,4,2]\n",
        "B3_Train_Per=[0.8,0.8,0.8]\n",
        "B3_Train_NN=[2,4,5,9,8,1]\n",
        "B3_Train_NN_Per=[0.8,0.8,0.8,0.8,0.8,0.8]\n",
        "B3_Test_N=[6]\n",
        "B3_Test_Per=[0.2]\n",
        "B3_Test_NN=[6]\n",
        "B3_Test_NN_Per=[0.2]\n",
        "\n",
        "B4_Train_N=[7,6,4,2]\n",
        "B4_Train_Per=[0.8,0.8,0.8,0.8]\n",
        "B4_Train_NN=[2,4,5,6,1,9]\n",
        "B4_Train_NN_Per=[0.8,0.8,0.8,0.8,0.8,0.8]\n",
        "B4_Test_N=[7]\n",
        "B4_Test_Per=[0.2]\n",
        "B4_Test_NN=[8]\n",
        "B4_Test_NN_Per=[0.2]\n",
        "\n",
        "B5_Train_N=[7,6,4,2]\n",
        "B5_Train_Per=[0.8,0.8,0.8,0.8]\n",
        "B5_Train_NN=[2,4,5,6,8,9,1]\n",
        "B5_Train_NN_Per=[0.8,0.8,0.8,0.8,0.8,0.8,0.8]\n",
        "B5_Test_N=[7]\n",
        "B5_Test_Per=[0.2]\n",
        "B5_Test_NN=[2]\n",
        "B5_Test_NN_Per=[0.2]\n",
        "\n",
        "\n",
        "B6_Train_N=[7,6,4]\n",
        "B6_Train_Per=[0.8,0.8,0.8]\n",
        "B6_Train_NN=[2,4,5,6,9,1,8]\n",
        "B6_Train_NN_Per=[0.8,0.8,0.8,0.8,0.8,0.8,0.8]\n",
        "B6_Test_N=[2]\n",
        "B6_Test_Per=[0.2]\n",
        "B6_Test_NN=[4]\n",
        "B6_Test_NN_Per=[0.2]\n",
        "\n",
        "B7_Train_N=[7,6,4]\n",
        "B7_Train_Per=[0.8,0.8,0.8]\n",
        "B7_Train_NN=[2,4,5,9,1,8]\n",
        "B7_Train_NN_Per=[0.8,0.8,0.8,0.8,0.8,0.8]\n",
        "B7_Test_N=[2]\n",
        "B7_Test_Per=[0.2]\n",
        "B7_Test_NN=[6]\n",
        "B7_Test_NN_Per=[0.2]\n",
        "\n",
        "Batches=[8,9,10,11,12,13,14,15]\n",
        "\n",
        "B8_Train_N=[2,4,7]\n",
        "B8_Train_Per=[0.8,0.8,0.8]\n",
        "B8_Train_NN=[1,2,4,5,8,9]\n",
        "B8_Train_NN_Per=[0.8,0.8,0.8,0.8,0.8,0.8]\n",
        "B8_Test_N=[6]\n",
        "B8_Test_Per=[0.2]\n",
        "B8_Test_NN=[6]\n",
        "B8_Test_NN_Per=[0.2]\n",
        "\n",
        "\n",
        "\n",
        "B9_Train_N=[6,4,7]\n",
        "B9_Train_Per=[0.8,0.8,0.8]\n",
        "B9_Train_NN=[1,4,5,8,9,6]\n",
        "B9_Train_NN_Per=[0.8,0.8,0.8,0.8,0.8,0.8]\n",
        "B9_Test_N=[2]\n",
        "B9_Test_Per=[0.2]\n",
        "B9_Test_NN=[2]\n",
        "B9_Test_NN_Per=[0.2]\n",
        "\n",
        "\n",
        "B10_Train_N=[6,2,7]\n",
        "B10_Train_Per=[0.8,0.8,0.8]\n",
        "B10_Train_NN=[1,2,5,8,9,6]\n",
        "B10_Train_NN_Per=[0.8,0.8,0.8,0.8,0.8,0.8]\n",
        "B10_Test_N=[4]\n",
        "B10_Test_Per=[0.2]\n",
        "B10_Test_NN=[4]\n",
        "B10_Test_NN_Per=[0.2]\n",
        "\n",
        "B11_Train_N=[6,2,4]\n",
        "B11_Train_Per=[0.8,0.8,0.8]\n",
        "B11_Train_NN=[4,2,5,8,9,6]\n",
        "B11_Train_NN_Per=[0.8,0.8,0.8,0.8,0.8,0.8]\n",
        "B11_Test_N=[7]\n",
        "B11_Test_Per=[0.2]\n",
        "B11_Test_NN=[1]\n",
        "B11_Test_NN_Per=[0.2]\n",
        "\n",
        "\n",
        "B12_Train_N=[6,2,7]\n",
        "B12_Train_Per=[0.8,0.8,0.8]\n",
        "B12_Train_NN=[1,2,5,9,6]\n",
        "B12_Train_NN_Per=[0.8,0.8,0.8,0.8,0.8]\n",
        "B12_Test_N=[4]\n",
        "B12_Test_Per=[0.2]\n",
        "B12_Test_NN=[8]\n",
        "B12_Test_NN_Per=[0.2]\n",
        "\n",
        "\n",
        "B13_Train_N=[6,2,7]\n",
        "B13_Train_Per=[0.8,0.8,0.8]\n",
        "B13_Train_NN=[1,2,5,8,6]\n",
        "B13_Train_NN_Per=[0.8,0.8,0.8,0.8,0.8]\n",
        "B13_Test_N=[4]\n",
        "B13_Test_Per=[0.2]\n",
        "B13_Test_NN=[9]\n",
        "B13_Test_NN_Per=[0.2]\n",
        "\n",
        "\n",
        "B14_Train_N=[6,2,7]\n",
        "B14_Train_Per=[0.8,0.8,0.8]\n",
        "B14_Train_NN=[9,2,5,8,6]\n",
        "B14_Train_NN_Per=[0.8,0.8,0.8,0.8,0.8,0.8]\n",
        "B14_Test_N=[4]\n",
        "B14_Test_Per=[0.2]\n",
        "B14_Test_NN=[1]\n",
        "B14_Test_NN_Per=[0.2]\n",
        "\n",
        "\n",
        "B15_Train_N=[6,2,7]\n",
        "B15_Train_Per=[0.8,0.8,0.8]\n",
        "B15_Train_NN=[1,9,2,8,6]\n",
        "B15_Train_NN_Per=[0.8,0.8,0.8,0.8,0.8]\n",
        "B15_Test_N=[4]\n",
        "B15_Test_Per=[0.2]\n",
        "B15_Test_NN=[5]\n",
        "B15_Test_NN_Per=[0.2]\n",
        "\n",
        "B16_Train_N=[7,6,4]\n",
        "B16_Train_Per=[1,1,1]\n",
        "B16_Train_NN=[1,4,5,6,8,9]\n",
        "B16_Train_NN_Per=[1,1,1,1,1,1]\n",
        "B16_Test_N=[2]\n",
        "B16_Test_Per=[1]\n",
        "B16_Test_NN=[2]\n",
        "B16_Test_NN_Per=[1]\n",
        "\n",
        "B17_Train_N=[7,6,2]\n",
        "B17_Train_Per=[1,1,1]\n",
        "B17_Train_NN=[1,2,5,6,8,9]\n",
        "B17_Train_NN_Per=[1,1,1,1,1,1]\n",
        "B17_Test_N=[4]\n",
        "B17_Test_Per=[1]\n",
        "B17_Test_NN=[4]\n",
        "B17_Test_NN_Per=[1]\n",
        "\n",
        "B18_Train_N=[2,4,7]\n",
        "B18_Train_Per=[1,1,1]\n",
        "B18_Train_NN=[1,2,4,5,8,9]\n",
        "B18_Train_NN_Per=[1,1,1,1,1,1]\n",
        "B18_Test_N=[6]\n",
        "B18_Test_Per=[1]\n",
        "B18_Test_NN=[6]\n",
        "B18_Test_NN_Per=[1]\n",
        "\n",
        "B19_Train_N=[2,4,6]\n",
        "B19_Train_Per=[1,1,1]\n",
        "B19_Train_NN=[1,2,4,5,8,9]\n",
        "B19_Train_NN_Per=[1,1,1,1,1,1]\n",
        "B19_Test_N=[7]\n",
        "B19_Test_Per=[0.2]\n",
        "B19_Test_NN=[7]\n",
        "B19_Test_NN_Per=[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kbUTmLSvi2X",
        "outputId": "df5230c6-ea8b-4d7c-83e9-ed6c71ae5d2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing Necrosis in Testing for Sample Number 7\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 5\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 5\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 8\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 9\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Non Necrosis in Testing for Sample Number 1\n",
            "Processing Necrosis in Testing for Sample Number 7\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 8\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 9\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Non Necrosis in Testing for Sample Number 1\n",
            "Processing Necrosis in Testing for Sample Number 7\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 1\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 8\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 9\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Non Necrosis in Testing for Sample Number 5\n",
            "Processing Necrosis in Testing for Sample Number 7\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 1\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 8\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 9\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Non Necrosis in Testing for Sample Number 5\n",
            "Processing Necrosis in Testing for Sample Number 7\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 5\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 5\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 9\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 8\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 1\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Non Necrosis in Testing for Sample Number 6\n",
            "Processing Necrosis in Testing for Sample Number 7\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 9\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 8\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 1\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Non Necrosis in Testing for Sample Number 6\n",
            "Processing Necrosis in Testing for Sample Number 7\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 5\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 5\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 1\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 9\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 7\n",
            "\n",
            "\n",
            "Processing Non Necrosis in Testing for Sample Number 8\n",
            "Processing Necrosis in Testing for Sample Number 7\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 1\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 9\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 7\n",
            "\n",
            "\n",
            "Processing Non Necrosis in Testing for Sample Number 8\n",
            "Processing Necrosis in Testing for Sample Number 7\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 5\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 5\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 8\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 9\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 1\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 7\n",
            "\n",
            "\n",
            "Processing Non Necrosis in Testing for Sample Number 2\n",
            "Processing Non_Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 8\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 9\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 1\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 7\n",
            "\n",
            "\n",
            "Processing Non Necrosis in Testing for Sample Number 2\n",
            "Processing Necrosis in Testing for Sample Number 7\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 7\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 5\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 5\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 9\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 1\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 8\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Non Necrosis in Testing for Sample Number 4\n",
            "Processing Necrosis in Testing for Sample Number 7\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 9\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 1\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 8\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Non Necrosis in Testing for Sample Number 4\n",
            "Processing Necrosis in Testing for Sample Number 7\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 5\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 5\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 9\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 1\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 8\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Non Necrosis in Testing for Sample Number 6\n",
            "Processing Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 7\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 9\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 1\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 8\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Non Necrosis in Testing for Sample Number 6\n",
            "Processing Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 7\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 1\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 1\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 5\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 5\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 8\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 9\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Non Necrosis in Testing for Sample Number 6\n",
            "Processing Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 7\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 8\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 9\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Non Necrosis in Testing for Sample Number 6\n",
            "Processing Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 7\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 1\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 5\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 1\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 5\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 8\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 9\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Non Necrosis in Testing for Sample Number 2\n",
            "Processing Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 7\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 8\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 9\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Non Necrosis in Testing for Sample Number 2\n",
            "Processing Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 7\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 1\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 1\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 5\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 8\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 9\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Non Necrosis in Testing for Sample Number 4\n",
            "Processing Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 5\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 8\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 9\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Non Necrosis in Testing for Sample Number 4\n",
            "Processing Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 5\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 8\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 9\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 7\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 5\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 8\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 9\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 7\n",
            "\n",
            "\n",
            "Processing Non Necrosis in Testing for Sample Number 1\n",
            "Processing Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 7\n",
            "\n",
            "\n",
            "Processing Non Necrosis in Testing for Sample Number 1\n",
            "Processing Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 7\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 1\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 1\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 5\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 9\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Non Necrosis in Testing for Sample Number 8\n",
            "Processing Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 7\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 5\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 9\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Non Necrosis in Testing for Sample Number 8\n",
            "Processing Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 7\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 1\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 1\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 5\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 8\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Non Necrosis in Testing for Sample Number 9\n",
            "Processing Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 7\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 5\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 8\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Non Necrosis in Testing for Sample Number 9\n",
            "Processing Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 7\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 9\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 9\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 5\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 8\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Non Necrosis in Testing for Sample Number 1\n",
            "Processing Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 7\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 5\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 8\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Non Necrosis in Testing for Sample Number 1\n",
            "Processing Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 7\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 1\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 9\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 1\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 9\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 8\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Non Necrosis in Testing for Sample Number 5\n",
            "Processing Necrosis in Training for Sample Number 7 \n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 8\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Non Necrosis in Testing for Sample Number 5\n",
            "Processing Necrosis in Training for Sample Number 7 \n",
            "\n",
            "\n",
            "Processing Necrosis in Training for Sample Number 6 \n",
            "\n",
            "\n",
            "Processing Necrosis in Training for Sample Number 4 \n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Training for Sample Number 1\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Training for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Training for Sample Number 5\n",
            "\n",
            "\n",
            "Processing Necrosis in Training for Sample Number 6 \n",
            "\n",
            "\n",
            "Processing Necrosis in Training for Sample Number 4 \n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Training for Sample Number 1\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Training for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Training for Sample Number 5\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Training for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Training for Sample Number 8\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Training for Sample Number 9\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Non Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Training for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Training for Sample Number 8\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Training for Sample Number 9\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Non Necrosis in Testing for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Necrosis in Training for Sample Number 7 \n",
            "\n",
            "\n",
            "Processing Necrosis in Training for Sample Number 6 \n",
            "\n",
            "\n",
            "Processing Necrosis in Training for Sample Number 2 \n",
            "\n",
            "\n",
            "Processing Necrosis in Training for Sample Number 7 \n",
            "\n",
            "\n",
            "Processing Necrosis in Training for Sample Number 6 \n",
            "\n",
            "\n",
            "Processing Necrosis in Training for Sample Number 2 \n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Training for Sample Number 1\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Training for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Training for Sample Number 1\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Training for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Training for Sample Number 5\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Training for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Training for Sample Number 8\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Training for Sample Number 9\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Non Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Necrosis in Training for Sample Number 2 \n",
            "\n",
            "\n",
            "Processing Necrosis in Training for Sample Number 4 \n",
            "\n",
            "\n",
            "Processing Necrosis in Training for Sample Number 7 \n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Training for Sample Number 5\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Training for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Training for Sample Number 8\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Training for Sample Number 9\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Non Necrosis in Testing for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Necrosis in Training for Sample Number 2 \n",
            "\n",
            "\n",
            "Processing Necrosis in Training for Sample Number 4 \n",
            "\n",
            "\n",
            "Processing Necrosis in Training for Sample Number 7 \n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Training for Sample Number 1\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Training for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Training for Sample Number 1\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Training for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Training for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Training for Sample Number 5\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Training for Sample Number 8\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Training for Sample Number 9\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Non Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Necrosis in Training for Sample Number 2 \n",
            "\n",
            "\n",
            "Processing Necrosis in Training for Sample Number 4 \n",
            "\n",
            "\n",
            "Processing Necrosis in Training for Sample Number 6 \n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Training for Sample Number 1\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Training for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Training for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Training for Sample Number 5\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Training for Sample Number 8\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Training for Sample Number 9\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Non Necrosis in Testing for Sample Number 6\n",
            "\n",
            "\n",
            "Processing Necrosis in Training for Sample Number 2 \n",
            "\n",
            "\n",
            "Processing Necrosis in Training for Sample Number 4 \n",
            "\n",
            "\n",
            "Processing Necrosis in Training for Sample Number 6 \n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Training for Sample Number 1\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Training for Sample Number 2\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Training for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Training for Sample Number 5\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Training for Sample Number 8\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Training for Sample Number 9\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 7\n",
            "\n",
            "\n",
            "Processing Non Necrosis in Testing for Sample Number 7\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Training for Sample Number 4\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Training for Sample Number 5\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Training for Sample Number 8\n",
            "\n",
            "\n",
            "Processing Non_Necrosis in Training for Sample Number 9\n",
            "\n",
            "\n",
            "Processing Necrosis in Testing for Sample Number 7\n",
            "\n",
            "\n",
            "Processing Non Necrosis in Testing for Sample Number 7\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "batch_preparation(1,B1_Train_N,B1_Train_NN,B1_Test_N,B1_Test_NN,B1_Train_Per,B1_Train_NN_Per,B1_Test_Per,B1_Test_NN_Per)\n",
        "batch_preparation(2,B2_Train_N,B2_Train_NN,B2_Test_N,B2_Test_NN,B2_Train_Per,B2_Train_NN_Per,B2_Test_Per,B2_Test_NN_Per)\n",
        "batch_preparation(3,B3_Train_N,B3_Train_NN,B3_Test_N,B3_Test_NN,B3_Train_Per,B3_Train_NN_Per,B3_Test_Per,B3_Test_NN_Per)\n",
        "batch_preparation(4,B4_Train_N,B4_Train_NN,B4_Test_N,B4_Test_NN,B4_Train_Per,B4_Train_NN_Per,B4_Test_Per,B4_Test_NN_Per)\n",
        "batch_preparation(5,B5_Train_N,B5_Train_NN,B5_Test_N,B5_Test_NN,B5_Train_Per,B5_Train_NN_Per,B5_Test_Per,B5_Test_NN_Per)\n",
        "batch_preparation(6,B6_Train_N,B6_Train_NN,B6_Test_N,B6_Test_NN,B6_Train_Per,B6_Train_NN_Per,B6_Test_Per,B6_Test_NN_Per)\n",
        "batch_preparation(7,B7_Train_N,B7_Train_NN,B7_Test_N,B7_Test_NN,B7_Train_Per,B7_Train_NN_Per,B7_Test_Per,B7_Test_NN_Per)\n",
        "batch_preparation(8,B8_Train_N,B8_Train_NN,B8_Test_N,B8_Test_NN,B8_Train_Per,B8_Train_NN_Per,B8_Test_Per,B8_Test_NN_Per)\n",
        "batch_preparation(9,B9_Train_N,B9_Train_NN,B9_Test_N,B9_Test_NN,B9_Train_Per,B9_Train_NN_Per,B9_Test_Per,B9_Test_NN_Per)\n",
        "batch_preparation(10,B10_Train_N,B10_Train_NN,B10_Test_N,B10_Test_NN,B10_Train_Per,B10_Train_NN_Per,B10_Test_Per,B10_Test_NN_Per)\n",
        "batch_preparation(11,B11_Train_N,B11_Train_NN,B11_Test_N,B11_Test_NN,B11_Train_Per,B11_Train_NN_Per,B11_Test_Per,B11_Test_NN_Per)\n",
        "batch_preparation(12,B12_Train_N,B12_Train_NN,B12_Test_N,B12_Test_NN,B12_Train_Per,B12_Train_NN_Per,B12_Test_Per,B12_Test_NN_Per)\n",
        "batch_preparation(13,B13_Train_N,B13_Train_NN,B13_Test_N,B13_Test_NN,B13_Train_Per,B13_Train_NN_Per,B13_Test_Per,B13_Test_NN_Per)\n",
        "batch_preparation(14,B14_Train_N,B14_Train_NN,B14_Test_N,B14_Test_NN,B14_Train_Per,B14_Train_NN_Per,B14_Test_Per,B14_Test_NN_Per)\n",
        "batch_preparation(15,B15_Train_N,B15_Train_NN,B15_Test_N,B15_Test_NN,B15_Train_Per,B15_Train_NN_Per,B15_Test_Per,B15_Test_NN_Per)\n",
        "batch_preparation(16,B16_Train_N,B16_Train_NN,B16_Test_N,B16_Test_NN,B16_Train_Per,B16_Train_NN_Per,B16_Test_Per,B16_Test_NN_Per)\n",
        "batch_preparation(17,B17_Train_N,B17_Train_NN,B17_Test_N,B17_Test_NN,B17_Train_Per,B17_Train_NN_Per,B17_Test_Per,B17_Test_NN_Per)\n",
        "batch_preparation(18,B18_Train_N,B18_Train_NN,B18_Test_N,B18_Test_NN,B18_Train_Per,B18_Train_NN_Per,B18_Test_Per,B18_Test_NN_Per)\n",
        "batch_preparation(19,B19_Train_N,B19_Train_NN,B19_Test_N,B19_Test_NN,B19_Train_Per,B19_Train_NN_Per,B19_Test_Per,B19_Test_NN_Per)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VI-q8tnWvlSW",
        "outputId": "68875add-df66-4a83-c818-da5d35f9b29f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 23:09:44.470447: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-02 23:09:44.554791: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-12-02 23:09:45.979406: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-02 23:09:45.979406: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU memory growth enabled.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1764697186.435067   46027 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14029 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:04:00.0, compute capability: 8.6\n"
          ]
        }
      ],
      "source": [
        "# Import basic libraries\n",
        "import gc\n",
        "import os\n",
        "import glob\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Data manipulation and visualization\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Image processing\n",
        "import cv2\n",
        "from skimage.io import imread, imsave\n",
        "from skimage.transform import resize\n",
        "from skimage.feature import hog\n",
        "import skimage.feature as feature\n",
        "\n",
        "# Machine learning\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, recall_score, f1_score, precision_score,\n",
        "    cohen_kappa_score, matthews_corrcoef, roc_auc_score,\n",
        "    confusion_matrix\n",
        ")\n",
        "\n",
        "# Deep learning\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.applications import (\n",
        "    DenseNet169, Xception, MobileNet, ResNet50, DenseNet121,\n",
        "    EfficientNetB0, VGG16, MobileNetV2, ResNet101,\n",
        "    InceptionResNetV2, InceptionV3, NASNetMobile\n",
        ")\n",
        "from tensorflow.keras.layers import (\n",
        "    Conv2D, MaxPooling2D, Dense, Flatten,\n",
        "    Dropout, BatchNormalization, GlobalAveragePooling2D\n",
        ")\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
        "from tensorflow.keras.callbacks import (\n",
        "    EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        ")\n",
        "\n",
        "# Enable eager execution for TensorFlow\n",
        "tf.config.run_functions_eagerly(True)\n",
        "\n",
        "# gc.collect()\n",
        "# tf.keras.backend.clear_session()\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Enable dynamic memory growth\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:        # <-- FIXED HERE\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(\"GPU memory growth enabled.\")\n",
        "    except Exception as e:\n",
        "        print(\"Memory growth error:\", e)\n",
        "else:\n",
        "    print(\"No GPU detected.\")\n",
        "\n",
        "# Create the Xception model with feature extraction\n",
        "base_model_Xception = Xception(\n",
        "    input_shape=(256, 256, 3),\n",
        "    weights='imagenet',\n",
        "    include_top=False\n",
        ")\n",
        "\n",
        "# Add global average pooling\n",
        "output_1 = GlobalAveragePooling2D()(base_model_Xception.output)\n",
        "\n",
        "# Add dense layer for feature extraction\n",
        "output_4 = Dense(2048, activation='relu')(output_1)\n",
        "model_Xception_features_2048 = Model(base_model_Xception.input, output_4)\n",
        "\n",
        "# Add final classification layer\n",
        "output = Dense(2, activation='softmax')(output_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            " STARTING OPTIMIZED FEATURE EXTRACTION (ReLU + TRAINED)\n",
            "============================================================\n",
            "\n",
            "Found 4586 images belonging to 2 classes.\n",
            "Found 3514 images belonging to 2 classes.\n",
            "Found 3514 images belonging to 2 classes.\n",
            "Found 4586 images belonging to 2 classes.\n",
            "Found 4586 images belonging to 2 classes.\n",
            "Found 3514 images belonging to 2 classes.\n",
            "Found 3514 images belonging to 2 classes.\n",
            "[INFO] Building model for first time\n",
            "[INFO] Building model for first time\n",
            "[INFO] Model created  Base frozen + Trainable Dense(2048, ReLU)\n",
            "\n",
            "[TRAINING]  Training Dense(2048, ReLU) layer \n",
            "[INFO] Model created  Base frozen + Trainable Dense(2048, ReLU)\n",
            "\n",
            "[TRAINING]  Training Dense(2048, ReLU) layer \n",
            "Epoch 1/3\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 23:10:02.420738: E tensorflow/core/util/util.cc:131] oneDNN supports DT_HALF only on platforms with AVX-512. Falling back to the default Eigen-based implementation if present.\n",
            "2025-12-02 23:10:02.558758: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91002\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m287/287\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 703ms/step - accuracy: 0.9666 - loss: 0.0932 - val_accuracy: 0.8802 - val_loss: 0.3319\n",
            "Epoch 2/3\n",
            "\u001b[1m287/287\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 703ms/step - accuracy: 0.9666 - loss: 0.0932 - val_accuracy: 0.8802 - val_loss: 0.3319\n",
            "Epoch 2/3\n",
            "\u001b[1m287/287\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 491ms/step - accuracy: 0.9730 - loss: 0.0765 - val_accuracy: 0.8876 - val_loss: 0.3361\n",
            "Epoch 3/3\n",
            "\u001b[1m287/287\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 491ms/step - accuracy: 0.9730 - loss: 0.0765 - val_accuracy: 0.8876 - val_loss: 0.3361\n",
            "Epoch 3/3\n",
            "\u001b[1m287/287\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 574ms/step - accuracy: 0.9741 - loss: 0.0670 - val_accuracy: 0.8162 - val_loss: 0.5413\n",
            "\u001b[1m287/287\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 574ms/step - accuracy: 0.9741 - loss: 0.0670 - val_accuracy: 0.8162 - val_loss: 0.5413\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAINING]  Dense layer training finished!\n",
            "\n",
            "[INFO]  Feature extraction model cached\n",
            "\n",
            "[STEP 1] Extracting training features for Batch_16\n",
            "[INFO] Extracting 4586 images in 144 steps\n",
            "[INFO]  Feature extraction model cached\n",
            "\n",
            "[STEP 1] Extracting training features for Batch_16\n",
            "[INFO] Extracting 4586 images in 144 steps\n",
            "  [Progress] 10/144\n",
            "  [Progress] 10/144\n",
            "  [Progress] 20/144\n",
            "  [Progress] 20/144\n",
            "  [Progress] 30/144\n",
            "  [Progress] 30/144\n",
            "  [Progress] 40/144\n",
            "  [Progress] 40/144\n",
            "  [Progress] 50/144\n",
            "  [Progress] 50/144\n",
            "  [Progress] 60/144\n",
            "  [Progress] 60/144\n",
            "  [Progress] 70/144\n",
            "  [Progress] 70/144\n",
            "  [Progress] 80/144\n",
            "  [Progress] 80/144\n",
            "  [Progress] 90/144\n",
            "  [Progress] 90/144\n",
            "  [Progress] 100/144\n",
            "  [Progress] 100/144\n",
            "  [Progress] 110/144\n",
            "  [Progress] 110/144\n",
            "  [Progress] 120/144\n",
            "  [Progress] 120/144\n",
            "  [Progress] 130/144\n",
            "  [Progress] 130/144\n",
            "  [Progress] 140/144\n",
            "  [Progress] 140/144\n",
            "[INFO]  Final features shape: (4586, 2048)\n",
            "\n",
            "[STEP 2] Extracting test features for Batch_16\n",
            "[INFO] Extracting 3514 images in 110 steps\n",
            "[INFO]  Final features shape: (4586, 2048)\n",
            "\n",
            "[STEP 2] Extracting test features for Batch_16\n",
            "[INFO] Extracting 3514 images in 110 steps\n",
            "  [Progress] 10/110\n",
            "  [Progress] 10/110\n",
            "  [Progress] 20/110\n",
            "  [Progress] 20/110\n",
            "  [Progress] 30/110\n",
            "  [Progress] 30/110\n",
            "  [Progress] 40/110\n",
            "  [Progress] 40/110\n",
            "  [Progress] 50/110\n",
            "  [Progress] 50/110\n",
            "  [Progress] 60/110\n",
            "  [Progress] 60/110\n",
            "  [Progress] 70/110\n",
            "  [Progress] 70/110\n",
            "  [Progress] 80/110\n",
            "  [Progress] 80/110\n",
            "  [Progress] 90/110\n",
            "  [Progress] 90/110\n",
            "  [Progress] 100/110\n",
            "  [Progress] 100/110\n",
            "  [Progress] 110/110\n",
            "[INFO]  Final features shape: (3514, 2048)\n",
            "  [Progress] 110/110\n",
            "[INFO]  Final features shape: (3514, 2048)\n",
            "[DONE] Features saved for Batch_16\n",
            "\n",
            "[DONE] Features saved for Batch_16\n",
            "\n",
            "Found 7433 images belonging to 2 classes.\n",
            "Found 7433 images belonging to 2 classes.\n",
            "Found 667 images belonging to 2 classes.\n",
            "Found 667 images belonging to 2 classes.\n",
            "Found 7433 images belonging to 2 classes.\n",
            "Found 7433 images belonging to 2 classes.\n",
            "Found 667 images belonging to 2 classes.\n",
            "Found 667 images belonging to 2 classes.\n",
            "\n",
            "[STEP 1] Extracting training features for Batch_17\n",
            "[INFO] Extracting 7433 images in 233 steps\n",
            "\n",
            "[STEP 1] Extracting training features for Batch_17\n",
            "[INFO] Extracting 7433 images in 233 steps\n",
            "  [Progress] 10/233\n",
            "  [Progress] 10/233\n",
            "  [Progress] 20/233\n",
            "  [Progress] 20/233\n",
            "  [Progress] 30/233\n",
            "  [Progress] 30/233\n",
            "  [Progress] 40/233\n",
            "  [Progress] 40/233\n",
            "  [Progress] 50/233\n",
            "  [Progress] 50/233\n",
            "  [Progress] 60/233\n",
            "  [Progress] 60/233\n",
            "  [Progress] 70/233\n",
            "  [Progress] 70/233\n",
            "  [Progress] 80/233\n",
            "  [Progress] 80/233\n",
            "  [Progress] 90/233\n",
            "  [Progress] 90/233\n",
            "  [Progress] 100/233\n",
            "  [Progress] 100/233\n",
            "  [Progress] 110/233\n",
            "  [Progress] 110/233\n",
            "  [Progress] 120/233\n",
            "  [Progress] 120/233\n",
            "  [Progress] 130/233\n",
            "  [Progress] 130/233\n",
            "  [Progress] 140/233\n",
            "  [Progress] 140/233\n",
            "  [Progress] 150/233\n",
            "  [Progress] 150/233\n",
            "  [Progress] 160/233\n",
            "  [Progress] 160/233\n",
            "  [Progress] 170/233\n",
            "  [Progress] 170/233\n",
            "  [Progress] 180/233\n",
            "  [Progress] 180/233\n",
            "  [Progress] 190/233\n",
            "  [Progress] 190/233\n",
            "  [Progress] 200/233\n",
            "  [Progress] 200/233\n",
            "  [Progress] 210/233\n",
            "  [Progress] 210/233\n",
            "  [Progress] 220/233\n",
            "  [Progress] 220/233\n",
            "  [Progress] 230/233\n",
            "  [Progress] 230/233\n",
            "[INFO]  Final features shape: (7433, 2048)\n",
            "\n",
            "[STEP 2] Extracting test features for Batch_17\n",
            "[INFO] Extracting 667 images in 21 steps\n",
            "[INFO]  Final features shape: (7433, 2048)\n",
            "\n",
            "[STEP 2] Extracting test features for Batch_17\n",
            "[INFO] Extracting 667 images in 21 steps\n",
            "  [Progress] 10/21\n",
            "  [Progress] 10/21\n",
            "  [Progress] 20/21\n",
            "  [Progress] 20/21\n",
            "[INFO]  Final features shape: (667, 2048)\n",
            "[INFO]  Final features shape: (667, 2048)\n",
            "[DONE] Features saved for Batch_17\n",
            "\n",
            "Found 7895 images belonging to 2 classes.\n",
            "[DONE] Features saved for Batch_17\n",
            "\n",
            "Found 7895 images belonging to 2 classes.\n",
            "Found 205 images belonging to 2 classes.\n",
            "Found 205 images belonging to 2 classes.\n",
            "Found 7895 images belonging to 2 classes.\n",
            "Found 7895 images belonging to 2 classes.\n",
            "Found 205 images belonging to 2 classes.\n",
            "Found 205 images belonging to 2 classes.\n",
            "\n",
            "[STEP 1] Extracting training features for Batch_18\n",
            "[INFO] Extracting 7895 images in 247 steps\n",
            "\n",
            "[STEP 1] Extracting training features for Batch_18\n",
            "[INFO] Extracting 7895 images in 247 steps\n",
            "  [Progress] 10/247\n",
            "  [Progress] 10/247\n",
            "  [Progress] 20/247\n",
            "  [Progress] 20/247\n",
            "  [Progress] 30/247\n",
            "  [Progress] 30/247\n",
            "  [Progress] 40/247\n",
            "  [Progress] 40/247\n",
            "  [Progress] 50/247\n",
            "  [Progress] 50/247\n",
            "  [Progress] 60/247\n",
            "  [Progress] 60/247\n",
            "  [Progress] 70/247\n",
            "  [Progress] 70/247\n",
            "  [Progress] 80/247\n",
            "  [Progress] 80/247\n",
            "  [Progress] 90/247\n",
            "  [Progress] 90/247\n",
            "  [Progress] 100/247\n",
            "  [Progress] 100/247\n",
            "  [Progress] 110/247\n",
            "  [Progress] 110/247\n",
            "  [Progress] 120/247\n",
            "  [Progress] 120/247\n",
            "  [Progress] 130/247\n",
            "  [Progress] 130/247\n",
            "  [Progress] 140/247\n",
            "  [Progress] 140/247\n",
            "  [Progress] 150/247\n",
            "  [Progress] 150/247\n",
            "  [Progress] 160/247\n",
            "  [Progress] 160/247\n",
            "  [Progress] 170/247\n",
            "  [Progress] 170/247\n",
            "  [Progress] 180/247\n",
            "  [Progress] 180/247\n",
            "  [Progress] 190/247\n",
            "  [Progress] 190/247\n",
            "  [Progress] 200/247\n",
            "  [Progress] 200/247\n",
            "  [Progress] 210/247\n",
            "  [Progress] 210/247\n",
            "  [Progress] 220/247\n",
            "  [Progress] 220/247\n",
            "  [Progress] 230/247\n",
            "  [Progress] 230/247\n",
            "  [Progress] 240/247\n",
            "  [Progress] 240/247\n",
            "[INFO]  Final features shape: (7895, 2048)\n",
            "\n",
            "[STEP 2] Extracting test features for Batch_18\n",
            "[INFO] Extracting 205 images in 7 steps\n",
            "[INFO]  Final features shape: (7895, 2048)\n",
            "\n",
            "[STEP 2] Extracting test features for Batch_18\n",
            "[INFO] Extracting 205 images in 7 steps\n",
            "[INFO]  Final features shape: (205, 2048)\n",
            "[INFO]  Final features shape: (205, 2048)\n",
            "[DONE] Features saved for Batch_18\n",
            "\n",
            "Found 5233 images belonging to 2 classes.\n",
            "[DONE] Features saved for Batch_18\n",
            "\n",
            "Found 5233 images belonging to 2 classes.\n",
            "Found 543 images belonging to 2 classes.\n",
            "Found 543 images belonging to 2 classes.\n",
            "Found 5233 images belonging to 2 classes.\n",
            "Found 5233 images belonging to 2 classes.\n",
            "Found 543 images belonging to 2 classes.\n",
            "Found 543 images belonging to 2 classes.\n",
            "\n",
            "[STEP 1] Extracting training features for Batch_19\n",
            "[INFO] Extracting 5233 images in 164 steps\n",
            "\n",
            "[STEP 1] Extracting training features for Batch_19\n",
            "[INFO] Extracting 5233 images in 164 steps\n",
            "  [Progress] 10/164\n",
            "  [Progress] 10/164\n",
            "  [Progress] 20/164\n",
            "  [Progress] 20/164\n",
            "  [Progress] 30/164\n",
            "  [Progress] 30/164\n",
            "  [Progress] 40/164\n",
            "  [Progress] 40/164\n",
            "  [Progress] 50/164\n",
            "  [Progress] 50/164\n",
            "  [Progress] 60/164\n",
            "  [Progress] 60/164\n",
            "  [Progress] 70/164\n",
            "  [Progress] 70/164\n",
            "  [Progress] 80/164\n",
            "  [Progress] 80/164\n",
            "  [Progress] 90/164\n",
            "  [Progress] 90/164\n",
            "  [Progress] 100/164\n",
            "  [Progress] 100/164\n",
            "  [Progress] 110/164\n",
            "  [Progress] 110/164\n",
            "  [Progress] 120/164\n",
            "  [Progress] 120/164\n",
            "  [Progress] 130/164\n",
            "  [Progress] 130/164\n",
            "  [Progress] 140/164\n",
            "  [Progress] 140/164\n",
            "  [Progress] 150/164\n",
            "  [Progress] 150/164\n",
            "  [Progress] 160/164\n",
            "  [Progress] 160/164\n",
            "[INFO]  Final features shape: (5233, 2048)\n",
            "\n",
            "[STEP 2] Extracting test features for Batch_19\n",
            "[INFO] Extracting 543 images in 17 steps\n",
            "[INFO]  Final features shape: (5233, 2048)\n",
            "\n",
            "[STEP 2] Extracting test features for Batch_19\n",
            "[INFO] Extracting 543 images in 17 steps\n",
            "  [Progress] 10/17\n",
            "  [Progress] 10/17\n",
            "[INFO]  Final features shape: (543, 2048)\n",
            "[INFO]  Final features shape: (543, 2048)\n",
            "[DONE] Features saved for Batch_19\n",
            "\n",
            "\n",
            "============================================================\n",
            " ALL BATCHES PROCESSED WITH TRAINED RELU FEATURES!\n",
            "============================================================\n",
            "[DONE] Features saved for Batch_19\n",
            "\n",
            "\n",
            "============================================================\n",
            " ALL BATCHES PROCESSED WITH TRAINED RELU FEATURES!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import Xception\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
        "import gc\n",
        "\n",
        "# =========================================\n",
        "#  GPU MEMORY MANAGEMENT\n",
        "# =========================================\n",
        "from tensorflow.keras import mixed_precision\n",
        "mixed_precision.set_global_policy('mixed_float16')   # reduce memory by ~40%\n",
        "\n",
        "# =========================================\n",
        "#  GLOBAL MODEL CACHE\n",
        "# =========================================\n",
        "_cached_model = None\n",
        "\n",
        "\n",
        "# =============================================================\n",
        "# 1 BUILD FEATURE EXTRACTOR (WITH TRAINABLE RELU LAYER)\n",
        "# =============================================================\n",
        "def build_feature_extractor_with_relu():\n",
        "    base_model = Xception(\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        input_shape=(256, 256, 3)\n",
        "    )\n",
        "\n",
        "    # Freeze the Xception base model\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # Add trainable Dense(2048, relu)\n",
        "    x = GlobalAveragePooling2D()(base_model.output)\n",
        "    x = Dense(2048, activation='relu', dtype='float32', name=\"feature_layer\",trainable=True)(x)\n",
        "\n",
        "    output = Dense(2, activation='softmax', dtype='float32')(x)\n",
        "\n",
        "    model = Model(inputs=base_model.input, outputs=output)\n",
        "    print(\"[INFO] Model created  Base frozen + Trainable Dense(2048, ReLU)\")\n",
        "    return model\n",
        "\n",
        "\n",
        "# =============================================================\n",
        "# 2 TRAIN ONLY THE DENSE(2048, RELU) LAYER\n",
        "# =============================================================\n",
        "def train_dense_layer(model, train_generator, val_generator, epochs=3):\n",
        "    \"\"\"Train the trainable Dense layer to learn better features.\"\"\"\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    print(\"\\n[TRAINING]  Training Dense(2048, ReLU) layer \")\n",
        "\n",
        "    model.fit(\n",
        "        train_generator,\n",
        "        validation_data=val_generator,\n",
        "        epochs=epochs\n",
        "    )\n",
        "\n",
        "    print(\"[TRAINING]  Dense layer training finished!\\n\")\n",
        "    return model\n",
        "\n",
        "\n",
        "# =============================================================\n",
        "# 3 GET CACHED MODEL OR TRAIN NEW ONE\n",
        "# =============================================================\n",
        "def get_feature_extraction_model(train_gen=None, val_gen=None):\n",
        "      \n",
        "    global _cached_model\n",
        "\n",
        "    if _cached_model is None:\n",
        "        print(\"[INFO] Building model for first time\")\n",
        "\n",
        "        # Build model\n",
        "        model = build_feature_extractor_with_relu()\n",
        "\n",
        "        # Train Dense layer (only once)\n",
        "        if train_gen is not None and val_gen is not None:\n",
        "            model = train_dense_layer(model, train_gen, val_gen, epochs=3)\n",
        "\n",
        "        trained_feature_model = Model(\n",
        "            inputs=model.input,\n",
        "            outputs=model.get_layer(\"feature_layer\").output\n",
        "        )\n",
        "\n",
        "        feature_save_path = os.path.join(\"saved_models\", \"Xception_feature_extractor_unetpp.h5\")\n",
        "        trained_feature_model.save(feature_save_path)\n",
        "\n",
        "        _cached_model = trained_feature_model\n",
        "        print(\"[INFO]  Feature extraction model cached\")\n",
        "\n",
        "    return _cached_model\n",
        "\n",
        "\n",
        "# =============================================================\n",
        "# 4 CHUNK-BASED FEATURE EXTRACTION\n",
        "# =============================================================\n",
        "def extract_features_in_chunks(generator, model, chunk_size=32):\n",
        "    all_features = []\n",
        "    all_labels = []\n",
        "    all_filenames = []\n",
        "\n",
        "    total_samples = len(generator.filenames)\n",
        "    steps = int(np.ceil(total_samples / generator.batch_size))\n",
        "\n",
        "    print(f\"[INFO] Extracting {total_samples} images in {steps} steps\")\n",
        "\n",
        "    for step in range(steps):\n",
        "        try:\n",
        "            batch_x, batch_y = next(generator)\n",
        "            batch_features = model.predict(batch_x, verbose=0)\n",
        "\n",
        "            all_features.append(batch_features)\n",
        "            all_labels.extend(np.argmax(batch_y, axis=1))\n",
        "            all_filenames.extend(generator.filenames[step*generator.batch_size:(step+1)*generator.batch_size])\n",
        "\n",
        "            if (step + 1) % 10 == 0:\n",
        "                print(f\"  [Progress] {step + 1}/{steps}\")\n",
        "                gc.collect()\n",
        "\n",
        "        except StopIteration:\n",
        "            break\n",
        "\n",
        "    features = np.vstack(all_features)\n",
        "    print(f\"[INFO]  Final features shape: {features.shape}\")\n",
        "    return features, np.array(all_labels), all_filenames[:len(all_labels)]\n",
        "\n",
        "\n",
        "# =============================================================\n",
        "# 5 MAIN EXTRACTION FUNCTION\n",
        "# =============================================================\n",
        "def XceptionNet2048_Features_Optimized(train_generator, test_generator, Batch):\n",
        "\n",
        "    # TRAINING DIRECTORIES FOR DENSE LAYER\n",
        "    # (You MUST create separate augmentation generators)\n",
        "    train_aug = ImageDataGenerator(rescale=1/255, rotation_range=20, horizontal_flip=True)\n",
        "    val_aug = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "    train_gen_dense = train_aug.flow_from_directory(\n",
        "        train_generator.directory,\n",
        "        target_size=(256, 256),\n",
        "        batch_size=16,\n",
        "        class_mode='categorical',\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    val_gen_dense = val_aug.flow_from_directory(\n",
        "        test_generator.directory,\n",
        "        target_size=(256, 256),\n",
        "        batch_size=16,\n",
        "        class_mode='categorical',\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    # Load or build model\n",
        "    model = get_feature_extraction_model(train_gen_dense, val_gen_dense)\n",
        "\n",
        "    # Create output directory\n",
        "    current_dir = os.path.dirname(os.path.abspath('__file__'))\n",
        "    parent_dir = os.path.dirname(current_dir)\n",
        "    root_dir = os.path.dirname(parent_dir)\n",
        "    save_drive_dir = os.path.join(root_dir, 'pathologyStudentsAug25', 'Results', 'Segmented_unett_XceptionNet2048+SVM')\n",
        "    batch_output_dir = os.path.join(save_drive_dir, Batch)\n",
        "    os.makedirs(batch_output_dir, exist_ok=True)\n",
        "\n",
        "    # EXTRACT FEATURES\n",
        "    print(f\"\\n[STEP 1] Extracting training features for {Batch}\")\n",
        "    f_train, y_train, n_train = extract_features_in_chunks(train_generator, model)\n",
        "\n",
        "    print(f\"\\n[STEP 2] Extracting test features for {Batch}\")\n",
        "    f_test, y_test, n_test = extract_features_in_chunks(test_generator, model)\n",
        "\n",
        "    # SAVE FEATURES\n",
        "    cols = [f\"feature_{i}\" for i in range(f_train.shape[1])]\n",
        "\n",
        "    df_train = pd.DataFrame(f_train, columns=cols)\n",
        "    df_train[\"label\"] = y_train\n",
        "    df_train[\"Image_Name\"] = n_train\n",
        "    df_train.to_csv(os.path.join(batch_output_dir, \"Segmented_XceptionNet2048_Training.csv\"), index=False)\n",
        "\n",
        "    df_test = pd.DataFrame(f_test, columns=cols)\n",
        "    df_test[\"label\"] = y_test\n",
        "    df_test[\"Image_Name\"] = n_test\n",
        "    df_test.to_csv(os.path.join(batch_output_dir, \"Segmented_XceptionNet2048_Testing.csv\"), index=False)\n",
        "\n",
        "    print(f\"[DONE] Features saved for {Batch}\\n\")\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "# =============================================================\n",
        "# 6 PROCESS ALL BATCHES\n",
        "# =============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" STARTING OPTIMIZED FEATURE EXTRACTION (ReLU + TRAINED)\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "for x in range(16, 20):\n",
        "\n",
        "    batch_size = 32\n",
        "    Batch_Name = f\"Batch_{x}\"\n",
        "\n",
        "    current_dir = os.path.dirname(os.path.abspath('__file__'))\n",
        "    parent_dir = os.path.dirname(current_dir)\n",
        "    root_dir = os.path.dirname(parent_dir)\n",
        "\n",
        "    Training_Path = os.path.join(root_dir, \"pathologyStudentsAug25\", \"Segmented_Batch\", Batch_Name, \"Training\")\n",
        "    Testing_Path = os.path.join(root_dir, \"pathologyStudentsAug25\", \"Segmented_Batch\", Batch_Name, \"Testing\")\n",
        "\n",
        "    if not os.path.exists(Training_Path):\n",
        "        print(f\"[SKIP] Missing path: {Training_Path}\")\n",
        "        continue\n",
        "\n",
        "    train_datagen = ImageDataGenerator(rescale=1/255)\n",
        "    test_datagen = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        Training_Path,\n",
        "        target_size=(256, 256),\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        classes=[\"Non_Necrosis\", \"Necrosis\"]\n",
        "    )\n",
        "\n",
        "    test_generator = test_datagen.flow_from_directory(\n",
        "        Testing_Path,\n",
        "        target_size=(256, 256),\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        classes=[\"Non_Necrosis\", \"Necrosis\"]\n",
        "    )\n",
        "\n",
        "    XceptionNet2048_Features_Optimized(train_generator, test_generator, Batch_Name)\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" ALL BATCHES PROCESSED WITH TRAINED RELU FEATURES!\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "STARTING OPTIMIZED BATCH PROCESSING - FULL DATA (100%)\n",
            "============================================================\n",
            "Found 6342 images belonging to 2 classes.\n",
            "Found 33 images belonging to 2 classes.\n",
            "Found 33 images belonging to 2 classes.\n",
            "\n",
            "============================================================\n",
            "[BATCH] Processing Batch_1\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "[BATCH] Processing Batch_1\n",
            "============================================================\n",
            "[INFO]  Feature extraction model loaded (cached)\n",
            "\n",
            "[STEP 1/2] Extracting training features...\n",
            "[INFO] Extracting features from 6342 images in 793 steps...\n",
            "[INFO]  Feature extraction model loaded (cached)\n",
            "\n",
            "[STEP 1/2] Extracting training features...\n",
            "[INFO] Extracting features from 6342 images in 793 steps...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 21:59:43.886265: E tensorflow/core/util/util.cc:131] oneDNN supports DT_HALF only on platforms with AVX-512. Falling back to the default Eigen-based implementation if present.\n",
            "2025-11-17 21:59:43.927594: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91300\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  [Progress] 10/793 batches processed...\n",
            "  [Progress] 20/793 batches processed...\n",
            "  [Progress] 20/793 batches processed...\n",
            "  [Progress] 30/793 batches processed...\n",
            "  [Progress] 30/793 batches processed...\n",
            "  [Progress] 40/793 batches processed...\n",
            "  [Progress] 40/793 batches processed...\n",
            "  [Progress] 50/793 batches processed...\n",
            "  [Progress] 50/793 batches processed...\n",
            "  [Progress] 60/793 batches processed...\n",
            "  [Progress] 60/793 batches processed...\n",
            "  [Progress] 70/793 batches processed...\n",
            "  [Progress] 70/793 batches processed...\n",
            "  [Progress] 80/793 batches processed...\n",
            "  [Progress] 80/793 batches processed...\n",
            "  [Progress] 90/793 batches processed...\n",
            "  [Progress] 90/793 batches processed...\n",
            "  [Progress] 100/793 batches processed...\n",
            "  [Progress] 100/793 batches processed...\n",
            "  [Progress] 110/793 batches processed...\n",
            "  [Progress] 110/793 batches processed...\n",
            "  [Progress] 120/793 batches processed...\n",
            "  [Progress] 120/793 batches processed...\n",
            "  [Progress] 130/793 batches processed...\n",
            "  [Progress] 130/793 batches processed...\n",
            "  [Progress] 140/793 batches processed...\n",
            "  [Progress] 140/793 batches processed...\n",
            "  [Progress] 150/793 batches processed...\n",
            "  [Progress] 150/793 batches processed...\n",
            "  [Progress] 160/793 batches processed...\n",
            "  [Progress] 160/793 batches processed...\n",
            "  [Progress] 170/793 batches processed...\n",
            "  [Progress] 170/793 batches processed...\n",
            "  [Progress] 180/793 batches processed...\n",
            "  [Progress] 180/793 batches processed...\n",
            "  [Progress] 190/793 batches processed...\n",
            "  [Progress] 190/793 batches processed...\n",
            "  [Progress] 200/793 batches processed...\n",
            "  [Progress] 200/793 batches processed...\n",
            "  [Progress] 210/793 batches processed...\n",
            "  [Progress] 210/793 batches processed...\n",
            "  [Progress] 220/793 batches processed...\n",
            "  [Progress] 220/793 batches processed...\n",
            "  [Progress] 230/793 batches processed...\n",
            "  [Progress] 230/793 batches processed...\n",
            "  [Progress] 240/793 batches processed...\n",
            "  [Progress] 240/793 batches processed...\n",
            "  [Progress] 250/793 batches processed...\n",
            "  [Progress] 250/793 batches processed...\n",
            "  [Progress] 260/793 batches processed...\n",
            "  [Progress] 260/793 batches processed...\n",
            "  [Progress] 270/793 batches processed...\n",
            "  [Progress] 270/793 batches processed...\n",
            "  [Progress] 280/793 batches processed...\n",
            "  [Progress] 280/793 batches processed...\n",
            "  [Progress] 290/793 batches processed...\n",
            "  [Progress] 290/793 batches processed...\n",
            "  [Progress] 300/793 batches processed...\n",
            "  [Progress] 300/793 batches processed...\n",
            "  [Progress] 310/793 batches processed...\n",
            "  [Progress] 310/793 batches processed...\n",
            "  [Progress] 320/793 batches processed...\n",
            "  [Progress] 320/793 batches processed...\n",
            "  [Progress] 330/793 batches processed...\n",
            "  [Progress] 330/793 batches processed...\n",
            "  [Progress] 340/793 batches processed...\n",
            "  [Progress] 340/793 batches processed...\n",
            "  [Progress] 350/793 batches processed...\n",
            "  [Progress] 350/793 batches processed...\n",
            "  [Progress] 360/793 batches processed...\n",
            "  [Progress] 360/793 batches processed...\n",
            "  [Progress] 370/793 batches processed...\n",
            "  [Progress] 370/793 batches processed...\n",
            "  [Progress] 380/793 batches processed...\n",
            "  [Progress] 380/793 batches processed...\n",
            "  [Progress] 390/793 batches processed...\n",
            "  [Progress] 390/793 batches processed...\n",
            "  [Progress] 400/793 batches processed...\n",
            "  [Progress] 400/793 batches processed...\n",
            "  [Progress] 410/793 batches processed...\n",
            "  [Progress] 410/793 batches processed...\n",
            "  [Progress] 420/793 batches processed...\n",
            "  [Progress] 420/793 batches processed...\n",
            "  [Progress] 430/793 batches processed...\n",
            "  [Progress] 430/793 batches processed...\n",
            "  [Progress] 440/793 batches processed...\n",
            "  [Progress] 440/793 batches processed...\n",
            "  [Progress] 450/793 batches processed...\n",
            "  [Progress] 450/793 batches processed...\n",
            "  [Progress] 460/793 batches processed...\n",
            "  [Progress] 460/793 batches processed...\n",
            "  [Progress] 470/793 batches processed...\n",
            "  [Progress] 470/793 batches processed...\n",
            "  [Progress] 480/793 batches processed...\n",
            "  [Progress] 480/793 batches processed...\n",
            "  [Progress] 490/793 batches processed...\n",
            "  [Progress] 490/793 batches processed...\n",
            "  [Progress] 500/793 batches processed...\n",
            "  [Progress] 500/793 batches processed...\n",
            "  [Progress] 510/793 batches processed...\n",
            "  [Progress] 510/793 batches processed...\n",
            "  [Progress] 520/793 batches processed...\n",
            "  [Progress] 520/793 batches processed...\n",
            "  [Progress] 530/793 batches processed...\n",
            "  [Progress] 530/793 batches processed...\n",
            "  [Progress] 540/793 batches processed...\n",
            "  [Progress] 540/793 batches processed...\n",
            "  [Progress] 550/793 batches processed...\n",
            "  [Progress] 550/793 batches processed...\n",
            "  [Progress] 560/793 batches processed...\n",
            "  [Progress] 560/793 batches processed...\n",
            "  [Progress] 570/793 batches processed...\n",
            "  [Progress] 570/793 batches processed...\n",
            "  [Progress] 580/793 batches processed...\n",
            "  [Progress] 580/793 batches processed...\n",
            "  [Progress] 590/793 batches processed...\n",
            "  [Progress] 590/793 batches processed...\n",
            "  [Progress] 600/793 batches processed...\n",
            "  [Progress] 600/793 batches processed...\n",
            "  [Progress] 610/793 batches processed...\n",
            "  [Progress] 610/793 batches processed...\n",
            "  [Progress] 620/793 batches processed...\n",
            "  [Progress] 620/793 batches processed...\n",
            "  [Progress] 630/793 batches processed...\n",
            "  [Progress] 630/793 batches processed...\n",
            "  [Progress] 640/793 batches processed...\n",
            "  [Progress] 640/793 batches processed...\n",
            "  [Progress] 650/793 batches processed...\n",
            "  [Progress] 650/793 batches processed...\n",
            "  [Progress] 660/793 batches processed...\n",
            "  [Progress] 660/793 batches processed...\n",
            "  [Progress] 670/793 batches processed...\n",
            "  [Progress] 670/793 batches processed...\n",
            "  [Progress] 680/793 batches processed...\n",
            "  [Progress] 680/793 batches processed...\n",
            "  [Progress] 690/793 batches processed...\n",
            "  [Progress] 690/793 batches processed...\n",
            "  [Progress] 700/793 batches processed...\n",
            "  [Progress] 700/793 batches processed...\n",
            "  [Progress] 710/793 batches processed...\n",
            "  [Progress] 710/793 batches processed...\n",
            "  [Progress] 720/793 batches processed...\n",
            "  [Progress] 720/793 batches processed...\n",
            "  [Progress] 730/793 batches processed...\n",
            "  [Progress] 730/793 batches processed...\n",
            "  [Progress] 740/793 batches processed...\n",
            "  [Progress] 740/793 batches processed...\n",
            "  [Progress] 750/793 batches processed...\n",
            "  [Progress] 750/793 batches processed...\n",
            "  [Progress] 760/793 batches processed...\n",
            "  [Progress] 760/793 batches processed...\n",
            "  [Progress] 770/793 batches processed...\n",
            "  [Progress] 770/793 batches processed...\n",
            "  [Progress] 780/793 batches processed...\n",
            "  [Progress] 780/793 batches processed...\n",
            "  [Progress] 790/793 batches processed...\n",
            "  [Progress] 790/793 batches processed...\n",
            "[INFO]  Total features extracted: (6342, 2048)\n",
            "\n",
            "[STEP 1/2] Extracting testing features...\n",
            "[INFO] Extracting features from 33 images in 5 steps...\n",
            "[INFO]  Total features extracted: (6342, 2048)\n",
            "\n",
            "[STEP 1/2] Extracting testing features...\n",
            "[INFO] Extracting features from 33 images in 5 steps...\n",
            "[INFO]  Total features extracted: (33, 2048)\n",
            "[INFO]  Total features extracted: (33, 2048)\n",
            "[STEP 2/2]  Training features saved: 6342 samples\n",
            "[STEP 2/2]  Testing features saved: 33 samples\n",
            "[STEP 2/2]  Training features saved: 6342 samples\n",
            "[STEP 2/2]  Testing features saved: 33 samples\n",
            "\n",
            "[INFO]  Batch_1 completed successfully!\n",
            "\n",
            "Found 5741 images belonging to 2 classes.\n",
            "\n",
            "[INFO]  Batch_1 completed successfully!\n",
            "\n",
            "Found 5741 images belonging to 2 classes.\n",
            "Found 183 images belonging to 2 classes.\n",
            "Found 183 images belonging to 2 classes.\n",
            "\n",
            "============================================================\n",
            "[BATCH] Processing Batch_2\n",
            "============================================================\n",
            "\n",
            "[STEP 1/2] Extracting training features...\n",
            "[INFO] Extracting features from 5741 images in 718 steps...\n",
            "\n",
            "============================================================\n",
            "[BATCH] Processing Batch_2\n",
            "============================================================\n",
            "\n",
            "[STEP 1/2] Extracting training features...\n",
            "[INFO] Extracting features from 5741 images in 718 steps...\n",
            "  [Progress] 10/718 batches processed...\n",
            "  [Progress] 10/718 batches processed...\n",
            "  [Progress] 20/718 batches processed...\n",
            "  [Progress] 20/718 batches processed...\n",
            "  [Progress] 30/718 batches processed...\n",
            "  [Progress] 30/718 batches processed...\n",
            "  [Progress] 40/718 batches processed...\n",
            "  [Progress] 40/718 batches processed...\n",
            "  [Progress] 50/718 batches processed...\n",
            "  [Progress] 50/718 batches processed...\n",
            "  [Progress] 60/718 batches processed...\n",
            "  [Progress] 60/718 batches processed...\n",
            "  [Progress] 70/718 batches processed...\n",
            "  [Progress] 70/718 batches processed...\n",
            "  [Progress] 80/718 batches processed...\n",
            "  [Progress] 80/718 batches processed...\n",
            "  [Progress] 90/718 batches processed...\n",
            "  [Progress] 90/718 batches processed...\n",
            "  [Progress] 100/718 batches processed...\n",
            "  [Progress] 100/718 batches processed...\n",
            "  [Progress] 110/718 batches processed...\n",
            "  [Progress] 110/718 batches processed...\n",
            "  [Progress] 120/718 batches processed...\n",
            "  [Progress] 120/718 batches processed...\n",
            "  [Progress] 130/718 batches processed...\n",
            "  [Progress] 130/718 batches processed...\n",
            "  [Progress] 140/718 batches processed...\n",
            "  [Progress] 140/718 batches processed...\n",
            "  [Progress] 150/718 batches processed...\n",
            "  [Progress] 150/718 batches processed...\n",
            "  [Progress] 160/718 batches processed...\n",
            "  [Progress] 160/718 batches processed...\n",
            "  [Progress] 170/718 batches processed...\n",
            "  [Progress] 170/718 batches processed...\n",
            "  [Progress] 180/718 batches processed...\n",
            "  [Progress] 180/718 batches processed...\n",
            "  [Progress] 190/718 batches processed...\n",
            "  [Progress] 190/718 batches processed...\n",
            "  [Progress] 200/718 batches processed...\n",
            "  [Progress] 200/718 batches processed...\n",
            "  [Progress] 210/718 batches processed...\n",
            "  [Progress] 210/718 batches processed...\n",
            "  [Progress] 220/718 batches processed...\n",
            "  [Progress] 220/718 batches processed...\n",
            "  [Progress] 230/718 batches processed...\n",
            "  [Progress] 230/718 batches processed...\n",
            "  [Progress] 240/718 batches processed...\n",
            "  [Progress] 240/718 batches processed...\n",
            "  [Progress] 250/718 batches processed...\n",
            "  [Progress] 250/718 batches processed...\n",
            "  [Progress] 260/718 batches processed...\n",
            "  [Progress] 260/718 batches processed...\n",
            "  [Progress] 270/718 batches processed...\n",
            "  [Progress] 270/718 batches processed...\n",
            "  [Progress] 280/718 batches processed...\n",
            "  [Progress] 280/718 batches processed...\n",
            "  [Progress] 290/718 batches processed...\n",
            "  [Progress] 290/718 batches processed...\n",
            "  [Progress] 300/718 batches processed...\n",
            "  [Progress] 300/718 batches processed...\n",
            "  [Progress] 310/718 batches processed...\n",
            "  [Progress] 310/718 batches processed...\n",
            "  [Progress] 320/718 batches processed...\n",
            "  [Progress] 320/718 batches processed...\n",
            "  [Progress] 330/718 batches processed...\n",
            "  [Progress] 330/718 batches processed...\n",
            "  [Progress] 340/718 batches processed...\n",
            "  [Progress] 340/718 batches processed...\n",
            "  [Progress] 350/718 batches processed...\n",
            "  [Progress] 350/718 batches processed...\n",
            "  [Progress] 360/718 batches processed...\n",
            "  [Progress] 360/718 batches processed...\n",
            "  [Progress] 370/718 batches processed...\n",
            "  [Progress] 370/718 batches processed...\n",
            "  [Progress] 380/718 batches processed...\n",
            "  [Progress] 380/718 batches processed...\n",
            "  [Progress] 390/718 batches processed...\n",
            "  [Progress] 390/718 batches processed...\n",
            "  [Progress] 400/718 batches processed...\n",
            "  [Progress] 400/718 batches processed...\n",
            "  [Progress] 410/718 batches processed...\n",
            "  [Progress] 410/718 batches processed...\n",
            "  [Progress] 420/718 batches processed...\n",
            "  [Progress] 420/718 batches processed...\n",
            "  [Progress] 430/718 batches processed...\n",
            "  [Progress] 430/718 batches processed...\n",
            "  [Progress] 440/718 batches processed...\n",
            "  [Progress] 440/718 batches processed...\n",
            "  [Progress] 450/718 batches processed...\n",
            "  [Progress] 450/718 batches processed...\n",
            "  [Progress] 460/718 batches processed...\n",
            "  [Progress] 460/718 batches processed...\n",
            "  [Progress] 470/718 batches processed...\n",
            "  [Progress] 470/718 batches processed...\n",
            "  [Progress] 480/718 batches processed...\n",
            "  [Progress] 480/718 batches processed...\n",
            "  [Progress] 490/718 batches processed...\n",
            "  [Progress] 490/718 batches processed...\n",
            "  [Progress] 500/718 batches processed...\n",
            "  [Progress] 500/718 batches processed...\n",
            "  [Progress] 510/718 batches processed...\n",
            "  [Progress] 510/718 batches processed...\n",
            "  [Progress] 520/718 batches processed...\n",
            "  [Progress] 520/718 batches processed...\n",
            "  [Progress] 530/718 batches processed...\n",
            "  [Progress] 530/718 batches processed...\n",
            "  [Progress] 540/718 batches processed...\n",
            "  [Progress] 540/718 batches processed...\n",
            "  [Progress] 550/718 batches processed...\n",
            "  [Progress] 550/718 batches processed...\n",
            "  [Progress] 560/718 batches processed...\n",
            "  [Progress] 560/718 batches processed...\n",
            "  [Progress] 570/718 batches processed...\n",
            "  [Progress] 570/718 batches processed...\n",
            "  [Progress] 580/718 batches processed...\n",
            "  [Progress] 580/718 batches processed...\n",
            "  [Progress] 590/718 batches processed...\n",
            "  [Progress] 590/718 batches processed...\n",
            "  [Progress] 600/718 batches processed...\n",
            "  [Progress] 600/718 batches processed...\n",
            "  [Progress] 610/718 batches processed...\n",
            "  [Progress] 610/718 batches processed...\n",
            "  [Progress] 620/718 batches processed...\n",
            "  [Progress] 620/718 batches processed...\n",
            "  [Progress] 630/718 batches processed...\n",
            "  [Progress] 630/718 batches processed...\n",
            "  [Progress] 640/718 batches processed...\n",
            "  [Progress] 640/718 batches processed...\n",
            "  [Progress] 650/718 batches processed...\n",
            "  [Progress] 650/718 batches processed...\n",
            "  [Progress] 660/718 batches processed...\n",
            "  [Progress] 660/718 batches processed...\n",
            "  [Progress] 670/718 batches processed...\n",
            "  [Progress] 670/718 batches processed...\n",
            "  [Progress] 680/718 batches processed...\n",
            "  [Progress] 680/718 batches processed...\n",
            "  [Progress] 690/718 batches processed...\n",
            "  [Progress] 690/718 batches processed...\n",
            "  [Progress] 700/718 batches processed...\n",
            "  [Progress] 700/718 batches processed...\n",
            "  [Progress] 710/718 batches processed...\n",
            "  [Progress] 710/718 batches processed...\n",
            "[INFO]  Total features extracted: (5741, 2048)\n",
            "\n",
            "[STEP 1/2] Extracting testing features...\n",
            "[INFO] Extracting features from 183 images in 23 steps...\n",
            "[INFO]  Total features extracted: (5741, 2048)\n",
            "\n",
            "[STEP 1/2] Extracting testing features...\n",
            "[INFO] Extracting features from 183 images in 23 steps...\n",
            "  [Progress] 10/23 batches processed...\n",
            "  [Progress] 10/23 batches processed...\n",
            "  [Progress] 20/23 batches processed...\n",
            "  [Progress] 20/23 batches processed...\n",
            "[INFO]  Total features extracted: (183, 2048)\n",
            "[INFO]  Total features extracted: (183, 2048)\n",
            "[STEP 2/2]  Training features saved: 5741 samples\n",
            "[STEP 2/2]  Testing features saved: 183 samples\n",
            "[STEP 2/2]  Training features saved: 5741 samples\n",
            "[STEP 2/2]  Testing features saved: 183 samples\n",
            "\n",
            "[INFO]  Batch_2 completed successfully!\n",
            "\n",
            "Found 6412 images belonging to 2 classes.\n",
            "\n",
            "[INFO]  Batch_2 completed successfully!\n",
            "\n",
            "Found 6412 images belonging to 2 classes.\n",
            "Found 15 images belonging to 2 classes.\n",
            "Found 15 images belonging to 2 classes.\n",
            "\n",
            "============================================================\n",
            "[BATCH] Processing Batch_3\n",
            "============================================================\n",
            "\n",
            "[STEP 1/2] Extracting training features...\n",
            "[INFO] Extracting features from 6412 images in 802 steps...\n",
            "\n",
            "============================================================\n",
            "[BATCH] Processing Batch_3\n",
            "============================================================\n",
            "\n",
            "[STEP 1/2] Extracting training features...\n",
            "[INFO] Extracting features from 6412 images in 802 steps...\n",
            "  [Progress] 10/802 batches processed...\n",
            "  [Progress] 10/802 batches processed...\n",
            "  [Progress] 20/802 batches processed...\n",
            "  [Progress] 20/802 batches processed...\n",
            "  [Progress] 30/802 batches processed...\n",
            "  [Progress] 30/802 batches processed...\n",
            "  [Progress] 40/802 batches processed...\n",
            "  [Progress] 40/802 batches processed...\n",
            "  [Progress] 50/802 batches processed...\n",
            "  [Progress] 50/802 batches processed...\n",
            "  [Progress] 60/802 batches processed...\n",
            "  [Progress] 60/802 batches processed...\n",
            "  [Progress] 70/802 batches processed...\n",
            "  [Progress] 70/802 batches processed...\n",
            "  [Progress] 80/802 batches processed...\n",
            "  [Progress] 80/802 batches processed...\n",
            "  [Progress] 90/802 batches processed...\n",
            "  [Progress] 90/802 batches processed...\n",
            "  [Progress] 100/802 batches processed...\n",
            "  [Progress] 100/802 batches processed...\n",
            "  [Progress] 110/802 batches processed...\n",
            "  [Progress] 110/802 batches processed...\n",
            "  [Progress] 120/802 batches processed...\n",
            "  [Progress] 120/802 batches processed...\n",
            "  [Progress] 130/802 batches processed...\n",
            "  [Progress] 130/802 batches processed...\n",
            "  [Progress] 140/802 batches processed...\n",
            "  [Progress] 140/802 batches processed...\n",
            "  [Progress] 150/802 batches processed...\n",
            "  [Progress] 150/802 batches processed...\n",
            "  [Progress] 160/802 batches processed...\n",
            "  [Progress] 160/802 batches processed...\n",
            "  [Progress] 170/802 batches processed...\n",
            "  [Progress] 170/802 batches processed...\n",
            "  [Progress] 180/802 batches processed...\n",
            "  [Progress] 180/802 batches processed...\n",
            "  [Progress] 190/802 batches processed...\n",
            "  [Progress] 190/802 batches processed...\n",
            "  [Progress] 200/802 batches processed...\n",
            "  [Progress] 200/802 batches processed...\n",
            "  [Progress] 210/802 batches processed...\n",
            "  [Progress] 210/802 batches processed...\n",
            "  [Progress] 220/802 batches processed...\n",
            "  [Progress] 220/802 batches processed...\n",
            "  [Progress] 230/802 batches processed...\n",
            "  [Progress] 230/802 batches processed...\n",
            "  [Progress] 240/802 batches processed...\n",
            "  [Progress] 240/802 batches processed...\n",
            "  [Progress] 250/802 batches processed...\n",
            "  [Progress] 250/802 batches processed...\n",
            "  [Progress] 260/802 batches processed...\n",
            "  [Progress] 260/802 batches processed...\n",
            "  [Progress] 270/802 batches processed...\n",
            "  [Progress] 270/802 batches processed...\n",
            "  [Progress] 280/802 batches processed...\n",
            "  [Progress] 280/802 batches processed...\n",
            "  [Progress] 290/802 batches processed...\n",
            "  [Progress] 290/802 batches processed...\n",
            "  [Progress] 300/802 batches processed...\n",
            "  [Progress] 300/802 batches processed...\n",
            "  [Progress] 310/802 batches processed...\n",
            "  [Progress] 310/802 batches processed...\n",
            "  [Progress] 320/802 batches processed...\n",
            "  [Progress] 320/802 batches processed...\n",
            "  [Progress] 330/802 batches processed...\n",
            "  [Progress] 330/802 batches processed...\n",
            "  [Progress] 340/802 batches processed...\n",
            "  [Progress] 340/802 batches processed...\n",
            "  [Progress] 350/802 batches processed...\n",
            "  [Progress] 350/802 batches processed...\n",
            "  [Progress] 360/802 batches processed...\n",
            "  [Progress] 360/802 batches processed...\n",
            "  [Progress] 370/802 batches processed...\n",
            "  [Progress] 370/802 batches processed...\n",
            "  [Progress] 380/802 batches processed...\n",
            "  [Progress] 380/802 batches processed...\n",
            "  [Progress] 390/802 batches processed...\n",
            "  [Progress] 390/802 batches processed...\n",
            "  [Progress] 400/802 batches processed...\n",
            "  [Progress] 400/802 batches processed...\n",
            "  [Progress] 410/802 batches processed...\n",
            "  [Progress] 410/802 batches processed...\n",
            "  [Progress] 420/802 batches processed...\n",
            "  [Progress] 420/802 batches processed...\n",
            "  [Progress] 430/802 batches processed...\n",
            "  [Progress] 430/802 batches processed...\n",
            "  [Progress] 440/802 batches processed...\n",
            "  [Progress] 440/802 batches processed...\n",
            "  [Progress] 450/802 batches processed...\n",
            "  [Progress] 450/802 batches processed...\n",
            "  [Progress] 460/802 batches processed...\n",
            "  [Progress] 460/802 batches processed...\n",
            "  [Progress] 470/802 batches processed...\n",
            "  [Progress] 470/802 batches processed...\n",
            "  [Progress] 480/802 batches processed...\n",
            "  [Progress] 480/802 batches processed...\n",
            "  [Progress] 490/802 batches processed...\n",
            "  [Progress] 490/802 batches processed...\n",
            "  [Progress] 500/802 batches processed...\n",
            "  [Progress] 500/802 batches processed...\n",
            "  [Progress] 510/802 batches processed...\n",
            "  [Progress] 510/802 batches processed...\n",
            "  [Progress] 520/802 batches processed...\n",
            "  [Progress] 520/802 batches processed...\n",
            "  [Progress] 530/802 batches processed...\n",
            "  [Progress] 530/802 batches processed...\n",
            "  [Progress] 540/802 batches processed...\n",
            "  [Progress] 540/802 batches processed...\n",
            "  [Progress] 550/802 batches processed...\n",
            "  [Progress] 550/802 batches processed...\n",
            "  [Progress] 560/802 batches processed...\n",
            "  [Progress] 560/802 batches processed...\n",
            "  [Progress] 570/802 batches processed...\n",
            "  [Progress] 570/802 batches processed...\n",
            "  [Progress] 580/802 batches processed...\n",
            "  [Progress] 580/802 batches processed...\n",
            "  [Progress] 590/802 batches processed...\n",
            "  [Progress] 590/802 batches processed...\n",
            "  [Progress] 600/802 batches processed...\n",
            "  [Progress] 600/802 batches processed...\n",
            "  [Progress] 610/802 batches processed...\n",
            "  [Progress] 610/802 batches processed...\n",
            "  [Progress] 620/802 batches processed...\n",
            "  [Progress] 620/802 batches processed...\n",
            "  [Progress] 630/802 batches processed...\n",
            "  [Progress] 630/802 batches processed...\n",
            "  [Progress] 640/802 batches processed...\n",
            "  [Progress] 640/802 batches processed...\n",
            "  [Progress] 650/802 batches processed...\n",
            "  [Progress] 650/802 batches processed...\n",
            "  [Progress] 660/802 batches processed...\n",
            "  [Progress] 660/802 batches processed...\n",
            "  [Progress] 670/802 batches processed...\n",
            "  [Progress] 670/802 batches processed...\n",
            "  [Progress] 680/802 batches processed...\n",
            "  [Progress] 680/802 batches processed...\n",
            "  [Progress] 690/802 batches processed...\n",
            "  [Progress] 690/802 batches processed...\n",
            "  [Progress] 700/802 batches processed...\n",
            "  [Progress] 700/802 batches processed...\n",
            "  [Progress] 710/802 batches processed...\n",
            "  [Progress] 710/802 batches processed...\n",
            "  [Progress] 720/802 batches processed...\n",
            "  [Progress] 720/802 batches processed...\n",
            "  [Progress] 730/802 batches processed...\n",
            "  [Progress] 730/802 batches processed...\n",
            "  [Progress] 740/802 batches processed...\n",
            "  [Progress] 740/802 batches processed...\n",
            "  [Progress] 750/802 batches processed...\n",
            "  [Progress] 750/802 batches processed...\n",
            "  [Progress] 760/802 batches processed...\n",
            "  [Progress] 760/802 batches processed...\n",
            "  [Progress] 770/802 batches processed...\n",
            "  [Progress] 770/802 batches processed...\n",
            "  [Progress] 780/802 batches processed...\n",
            "  [Progress] 780/802 batches processed...\n",
            "  [Progress] 790/802 batches processed...\n",
            "  [Progress] 790/802 batches processed...\n",
            "  [Progress] 800/802 batches processed...\n",
            "  [Progress] 800/802 batches processed...\n",
            "[INFO]  Total features extracted: (6412, 2048)\n",
            "\n",
            "[STEP 1/2] Extracting testing features...\n",
            "[INFO] Extracting features from 15 images in 2 steps...\n",
            "[INFO]  Total features extracted: (6412, 2048)\n",
            "\n",
            "[STEP 1/2] Extracting testing features...\n",
            "[INFO] Extracting features from 15 images in 2 steps...\n",
            "[INFO]  Total features extracted: (15, 2048)\n",
            "[INFO]  Total features extracted: (15, 2048)\n",
            "[STEP 2/2]  Training features saved: 6412 samples\n",
            "[STEP 2/2]  Testing features saved: 15 samples\n",
            "[STEP 2/2]  Training features saved: 6412 samples\n",
            "[STEP 2/2]  Testing features saved: 15 samples\n",
            "\n",
            "[INFO]  Batch_3 completed successfully!\n",
            "\n",
            "Found 6388 images belonging to 2 classes.\n",
            "\n",
            "[INFO]  Batch_3 completed successfully!\n",
            "\n",
            "Found 6388 images belonging to 2 classes.\n",
            "Found 564 images belonging to 2 classes.\n",
            "Found 564 images belonging to 2 classes.\n",
            "\n",
            "============================================================\n",
            "[BATCH] Processing Batch_4\n",
            "============================================================\n",
            "\n",
            "[STEP 1/2] Extracting training features...\n",
            "[INFO] Extracting features from 6388 images in 799 steps...\n",
            "\n",
            "============================================================\n",
            "[BATCH] Processing Batch_4\n",
            "============================================================\n",
            "\n",
            "[STEP 1/2] Extracting training features...\n",
            "[INFO] Extracting features from 6388 images in 799 steps...\n",
            "  [Progress] 10/799 batches processed...\n",
            "  [Progress] 10/799 batches processed...\n",
            "  [Progress] 20/799 batches processed...\n",
            "  [Progress] 20/799 batches processed...\n",
            "  [Progress] 30/799 batches processed...\n",
            "  [Progress] 30/799 batches processed...\n",
            "  [Progress] 40/799 batches processed...\n",
            "  [Progress] 40/799 batches processed...\n",
            "  [Progress] 50/799 batches processed...\n",
            "  [Progress] 50/799 batches processed...\n",
            "  [Progress] 60/799 batches processed...\n",
            "  [Progress] 60/799 batches processed...\n",
            "  [Progress] 70/799 batches processed...\n",
            "  [Progress] 70/799 batches processed...\n",
            "  [Progress] 80/799 batches processed...\n",
            "  [Progress] 80/799 batches processed...\n",
            "  [Progress] 90/799 batches processed...\n",
            "  [Progress] 90/799 batches processed...\n",
            "  [Progress] 100/799 batches processed...\n",
            "  [Progress] 100/799 batches processed...\n",
            "  [Progress] 110/799 batches processed...\n",
            "  [Progress] 110/799 batches processed...\n",
            "  [Progress] 120/799 batches processed...\n",
            "  [Progress] 120/799 batches processed...\n",
            "  [Progress] 130/799 batches processed...\n",
            "  [Progress] 130/799 batches processed...\n",
            "  [Progress] 140/799 batches processed...\n",
            "  [Progress] 140/799 batches processed...\n",
            "  [Progress] 150/799 batches processed...\n",
            "  [Progress] 150/799 batches processed...\n",
            "  [Progress] 160/799 batches processed...\n",
            "  [Progress] 160/799 batches processed...\n",
            "  [Progress] 170/799 batches processed...\n",
            "  [Progress] 170/799 batches processed...\n",
            "  [Progress] 180/799 batches processed...\n",
            "  [Progress] 180/799 batches processed...\n",
            "  [Progress] 190/799 batches processed...\n",
            "  [Progress] 190/799 batches processed...\n",
            "  [Progress] 200/799 batches processed...\n",
            "  [Progress] 200/799 batches processed...\n",
            "  [Progress] 210/799 batches processed...\n",
            "  [Progress] 210/799 batches processed...\n",
            "  [Progress] 220/799 batches processed...\n",
            "  [Progress] 220/799 batches processed...\n",
            "  [Progress] 230/799 batches processed...\n",
            "  [Progress] 230/799 batches processed...\n",
            "  [Progress] 240/799 batches processed...\n",
            "  [Progress] 240/799 batches processed...\n",
            "  [Progress] 250/799 batches processed...\n",
            "  [Progress] 250/799 batches processed...\n",
            "  [Progress] 260/799 batches processed...\n",
            "  [Progress] 260/799 batches processed...\n",
            "  [Progress] 270/799 batches processed...\n",
            "  [Progress] 270/799 batches processed...\n",
            "  [Progress] 280/799 batches processed...\n",
            "  [Progress] 280/799 batches processed...\n",
            "  [Progress] 290/799 batches processed...\n",
            "  [Progress] 290/799 batches processed...\n",
            "  [Progress] 300/799 batches processed...\n",
            "  [Progress] 300/799 batches processed...\n",
            "  [Progress] 310/799 batches processed...\n",
            "  [Progress] 310/799 batches processed...\n",
            "  [Progress] 320/799 batches processed...\n",
            "  [Progress] 320/799 batches processed...\n",
            "  [Progress] 330/799 batches processed...\n",
            "  [Progress] 330/799 batches processed...\n",
            "  [Progress] 340/799 batches processed...\n",
            "  [Progress] 340/799 batches processed...\n",
            "  [Progress] 350/799 batches processed...\n",
            "  [Progress] 350/799 batches processed...\n",
            "  [Progress] 360/799 batches processed...\n",
            "  [Progress] 360/799 batches processed...\n",
            "  [Progress] 370/799 batches processed...\n",
            "  [Progress] 370/799 batches processed...\n",
            "  [Progress] 380/799 batches processed...\n",
            "  [Progress] 380/799 batches processed...\n",
            "  [Progress] 390/799 batches processed...\n",
            "  [Progress] 390/799 batches processed...\n",
            "  [Progress] 400/799 batches processed...\n",
            "  [Progress] 400/799 batches processed...\n",
            "  [Progress] 410/799 batches processed...\n",
            "  [Progress] 410/799 batches processed...\n",
            "  [Progress] 420/799 batches processed...\n",
            "  [Progress] 420/799 batches processed...\n",
            "  [Progress] 430/799 batches processed...\n",
            "  [Progress] 430/799 batches processed...\n",
            "  [Progress] 440/799 batches processed...\n",
            "  [Progress] 440/799 batches processed...\n",
            "  [Progress] 450/799 batches processed...\n",
            "  [Progress] 450/799 batches processed...\n",
            "  [Progress] 460/799 batches processed...\n",
            "  [Progress] 460/799 batches processed...\n",
            "  [Progress] 470/799 batches processed...\n",
            "  [Progress] 470/799 batches processed...\n",
            "  [Progress] 480/799 batches processed...\n",
            "  [Progress] 480/799 batches processed...\n",
            "  [Progress] 490/799 batches processed...\n",
            "  [Progress] 490/799 batches processed...\n",
            "  [Progress] 500/799 batches processed...\n",
            "  [Progress] 500/799 batches processed...\n",
            "  [Progress] 510/799 batches processed...\n",
            "  [Progress] 510/799 batches processed...\n",
            "  [Progress] 520/799 batches processed...\n",
            "  [Progress] 520/799 batches processed...\n",
            "  [Progress] 530/799 batches processed...\n",
            "  [Progress] 530/799 batches processed...\n",
            "  [Progress] 540/799 batches processed...\n",
            "  [Progress] 540/799 batches processed...\n",
            "  [Progress] 550/799 batches processed...\n",
            "  [Progress] 550/799 batches processed...\n",
            "  [Progress] 560/799 batches processed...\n",
            "  [Progress] 560/799 batches processed...\n",
            "  [Progress] 570/799 batches processed...\n",
            "  [Progress] 570/799 batches processed...\n",
            "  [Progress] 580/799 batches processed...\n",
            "  [Progress] 580/799 batches processed...\n",
            "  [Progress] 590/799 batches processed...\n",
            "  [Progress] 590/799 batches processed...\n",
            "  [Progress] 600/799 batches processed...\n",
            "  [Progress] 600/799 batches processed...\n",
            "  [Progress] 610/799 batches processed...\n",
            "  [Progress] 610/799 batches processed...\n",
            "  [Progress] 620/799 batches processed...\n",
            "  [Progress] 620/799 batches processed...\n",
            "  [Progress] 630/799 batches processed...\n",
            "  [Progress] 630/799 batches processed...\n",
            "  [Progress] 640/799 batches processed...\n",
            "  [Progress] 640/799 batches processed...\n",
            "  [Progress] 650/799 batches processed...\n",
            "  [Progress] 650/799 batches processed...\n",
            "  [Progress] 660/799 batches processed...\n",
            "  [Progress] 660/799 batches processed...\n",
            "  [Progress] 670/799 batches processed...\n",
            "  [Progress] 670/799 batches processed...\n",
            "  [Progress] 680/799 batches processed...\n",
            "  [Progress] 680/799 batches processed...\n",
            "  [Progress] 690/799 batches processed...\n",
            "  [Progress] 690/799 batches processed...\n",
            "  [Progress] 700/799 batches processed...\n",
            "  [Progress] 700/799 batches processed...\n",
            "  [Progress] 710/799 batches processed...\n",
            "  [Progress] 710/799 batches processed...\n",
            "  [Progress] 720/799 batches processed...\n",
            "  [Progress] 720/799 batches processed...\n",
            "  [Progress] 730/799 batches processed...\n",
            "  [Progress] 730/799 batches processed...\n",
            "  [Progress] 740/799 batches processed...\n",
            "  [Progress] 740/799 batches processed...\n",
            "  [Progress] 750/799 batches processed...\n",
            "  [Progress] 750/799 batches processed...\n",
            "  [Progress] 760/799 batches processed...\n",
            "  [Progress] 760/799 batches processed...\n",
            "  [Progress] 770/799 batches processed...\n",
            "  [Progress] 770/799 batches processed...\n",
            "  [Progress] 780/799 batches processed...\n",
            "  [Progress] 780/799 batches processed...\n",
            "  [Progress] 790/799 batches processed...\n",
            "  [Progress] 790/799 batches processed...\n",
            "[INFO]  Total features extracted: (6388, 2048)\n",
            "\n",
            "[STEP 1/2] Extracting testing features...\n",
            "[INFO] Extracting features from 564 images in 71 steps...\n",
            "[INFO]  Total features extracted: (6388, 2048)\n",
            "\n",
            "[STEP 1/2] Extracting testing features...\n",
            "[INFO] Extracting features from 564 images in 71 steps...\n",
            "  [Progress] 10/71 batches processed...\n",
            "  [Progress] 10/71 batches processed...\n",
            "  [Progress] 20/71 batches processed...\n",
            "  [Progress] 20/71 batches processed...\n",
            "  [Progress] 30/71 batches processed...\n",
            "  [Progress] 30/71 batches processed...\n",
            "  [Progress] 40/71 batches processed...\n",
            "  [Progress] 40/71 batches processed...\n",
            "  [Progress] 50/71 batches processed...\n",
            "  [Progress] 50/71 batches processed...\n",
            "  [Progress] 60/71 batches processed...\n",
            "  [Progress] 60/71 batches processed...\n",
            "  [Progress] 70/71 batches processed...\n",
            "  [Progress] 70/71 batches processed...\n",
            "[INFO]  Total features extracted: (564, 2048)\n",
            "[INFO]  Total features extracted: (564, 2048)\n",
            "[STEP 2/2]  Training features saved: 6388 samples\n",
            "[STEP 2/2]  Training features saved: 6388 samples\n",
            "[STEP 2/2]  Testing features saved: 564 samples\n",
            "\n",
            "[INFO]  Batch_4 completed successfully!\n",
            "\n",
            "Found 6475 images belonging to 2 classes.\n",
            "[STEP 2/2]  Testing features saved: 564 samples\n",
            "\n",
            "[INFO]  Batch_4 completed successfully!\n",
            "\n",
            "Found 6475 images belonging to 2 classes.\n",
            "Found 1215 images belonging to 2 classes.\n",
            "Found 1215 images belonging to 2 classes.\n",
            "\n",
            "============================================================\n",
            "[BATCH] Processing Batch_5\n",
            "============================================================\n",
            "\n",
            "[STEP 1/2] Extracting training features...\n",
            "[INFO] Extracting features from 6475 images in 810 steps...\n",
            "\n",
            "============================================================\n",
            "[BATCH] Processing Batch_5\n",
            "============================================================\n",
            "\n",
            "[STEP 1/2] Extracting training features...\n",
            "[INFO] Extracting features from 6475 images in 810 steps...\n",
            "  [Progress] 10/810 batches processed...\n",
            "  [Progress] 10/810 batches processed...\n",
            "  [Progress] 20/810 batches processed...\n",
            "  [Progress] 20/810 batches processed...\n",
            "  [Progress] 30/810 batches processed...\n",
            "  [Progress] 30/810 batches processed...\n",
            "  [Progress] 40/810 batches processed...\n",
            "  [Progress] 40/810 batches processed...\n",
            "  [Progress] 50/810 batches processed...\n",
            "  [Progress] 50/810 batches processed...\n",
            "  [Progress] 60/810 batches processed...\n",
            "  [Progress] 60/810 batches processed...\n",
            "  [Progress] 70/810 batches processed...\n",
            "  [Progress] 70/810 batches processed...\n",
            "  [Progress] 80/810 batches processed...\n",
            "  [Progress] 80/810 batches processed...\n",
            "  [Progress] 90/810 batches processed...\n",
            "  [Progress] 90/810 batches processed...\n",
            "  [Progress] 100/810 batches processed...\n",
            "  [Progress] 100/810 batches processed...\n",
            "  [Progress] 110/810 batches processed...\n",
            "  [Progress] 110/810 batches processed...\n",
            "  [Progress] 120/810 batches processed...\n",
            "  [Progress] 120/810 batches processed...\n",
            "  [Progress] 130/810 batches processed...\n",
            "  [Progress] 130/810 batches processed...\n",
            "  [Progress] 140/810 batches processed...\n",
            "  [Progress] 140/810 batches processed...\n",
            "  [Progress] 150/810 batches processed...\n",
            "  [Progress] 150/810 batches processed...\n",
            "  [Progress] 160/810 batches processed...\n",
            "  [Progress] 160/810 batches processed...\n",
            "  [Progress] 170/810 batches processed...\n",
            "  [Progress] 170/810 batches processed...\n",
            "  [Progress] 180/810 batches processed...\n",
            "  [Progress] 180/810 batches processed...\n",
            "  [Progress] 190/810 batches processed...\n",
            "  [Progress] 190/810 batches processed...\n",
            "  [Progress] 200/810 batches processed...\n",
            "  [Progress] 200/810 batches processed...\n",
            "  [Progress] 210/810 batches processed...\n",
            "  [Progress] 210/810 batches processed...\n",
            "  [Progress] 220/810 batches processed...\n",
            "  [Progress] 220/810 batches processed...\n",
            "  [Progress] 230/810 batches processed...\n",
            "  [Progress] 230/810 batches processed...\n",
            "  [Progress] 240/810 batches processed...\n",
            "  [Progress] 240/810 batches processed...\n",
            "  [Progress] 250/810 batches processed...\n",
            "  [Progress] 250/810 batches processed...\n",
            "  [Progress] 260/810 batches processed...\n",
            "  [Progress] 260/810 batches processed...\n",
            "  [Progress] 270/810 batches processed...\n",
            "  [Progress] 270/810 batches processed...\n",
            "  [Progress] 280/810 batches processed...\n",
            "  [Progress] 280/810 batches processed...\n",
            "  [Progress] 290/810 batches processed...\n",
            "  [Progress] 290/810 batches processed...\n",
            "  [Progress] 300/810 batches processed...\n",
            "  [Progress] 300/810 batches processed...\n",
            "  [Progress] 310/810 batches processed...\n",
            "  [Progress] 310/810 batches processed...\n",
            "  [Progress] 320/810 batches processed...\n",
            "  [Progress] 320/810 batches processed...\n",
            "  [Progress] 330/810 batches processed...\n",
            "  [Progress] 330/810 batches processed...\n",
            "  [Progress] 340/810 batches processed...\n",
            "  [Progress] 340/810 batches processed...\n",
            "  [Progress] 350/810 batches processed...\n",
            "  [Progress] 350/810 batches processed...\n",
            "  [Progress] 360/810 batches processed...\n",
            "  [Progress] 360/810 batches processed...\n",
            "  [Progress] 370/810 batches processed...\n",
            "  [Progress] 370/810 batches processed...\n",
            "  [Progress] 380/810 batches processed...\n",
            "  [Progress] 380/810 batches processed...\n",
            "  [Progress] 390/810 batches processed...\n",
            "  [Progress] 390/810 batches processed...\n",
            "  [Progress] 400/810 batches processed...\n",
            "  [Progress] 400/810 batches processed...\n",
            "  [Progress] 410/810 batches processed...\n",
            "  [Progress] 410/810 batches processed...\n",
            "  [Progress] 420/810 batches processed...\n",
            "  [Progress] 420/810 batches processed...\n",
            "  [Progress] 430/810 batches processed...\n",
            "  [Progress] 430/810 batches processed...\n",
            "  [Progress] 440/810 batches processed...\n",
            "  [Progress] 440/810 batches processed...\n",
            "  [Progress] 450/810 batches processed...\n",
            "  [Progress] 450/810 batches processed...\n",
            "  [Progress] 460/810 batches processed...\n",
            "  [Progress] 460/810 batches processed...\n",
            "  [Progress] 470/810 batches processed...\n",
            "  [Progress] 470/810 batches processed...\n",
            "  [Progress] 480/810 batches processed...\n",
            "  [Progress] 480/810 batches processed...\n",
            "  [Progress] 490/810 batches processed...\n",
            "  [Progress] 490/810 batches processed...\n",
            "  [Progress] 500/810 batches processed...\n",
            "  [Progress] 500/810 batches processed...\n",
            "  [Progress] 510/810 batches processed...\n",
            "  [Progress] 510/810 batches processed...\n",
            "  [Progress] 520/810 batches processed...\n",
            "  [Progress] 520/810 batches processed...\n",
            "  [Progress] 530/810 batches processed...\n",
            "  [Progress] 530/810 batches processed...\n",
            "  [Progress] 540/810 batches processed...\n",
            "  [Progress] 540/810 batches processed...\n",
            "  [Progress] 550/810 batches processed...\n",
            "  [Progress] 550/810 batches processed...\n",
            "  [Progress] 560/810 batches processed...\n",
            "  [Progress] 560/810 batches processed...\n",
            "  [Progress] 570/810 batches processed...\n",
            "  [Progress] 570/810 batches processed...\n",
            "  [Progress] 580/810 batches processed...\n",
            "  [Progress] 580/810 batches processed...\n",
            "  [Progress] 590/810 batches processed...\n",
            "  [Progress] 590/810 batches processed...\n",
            "  [Progress] 600/810 batches processed...\n",
            "  [Progress] 600/810 batches processed...\n",
            "  [Progress] 610/810 batches processed...\n",
            "  [Progress] 610/810 batches processed...\n",
            "  [Progress] 620/810 batches processed...\n",
            "  [Progress] 620/810 batches processed...\n",
            "  [Progress] 630/810 batches processed...\n",
            "  [Progress] 630/810 batches processed...\n",
            "  [Progress] 640/810 batches processed...\n",
            "  [Progress] 640/810 batches processed...\n",
            "  [Progress] 650/810 batches processed...\n",
            "  [Progress] 650/810 batches processed...\n",
            "  [Progress] 660/810 batches processed...\n",
            "  [Progress] 660/810 batches processed...\n",
            "  [Progress] 670/810 batches processed...\n",
            "  [Progress] 670/810 batches processed...\n",
            "  [Progress] 680/810 batches processed...\n",
            "  [Progress] 680/810 batches processed...\n",
            "  [Progress] 690/810 batches processed...\n",
            "  [Progress] 690/810 batches processed...\n",
            "  [Progress] 700/810 batches processed...\n",
            "  [Progress] 700/810 batches processed...\n",
            "  [Progress] 710/810 batches processed...\n",
            "  [Progress] 710/810 batches processed...\n",
            "  [Progress] 720/810 batches processed...\n",
            "  [Progress] 720/810 batches processed...\n",
            "  [Progress] 730/810 batches processed...\n",
            "  [Progress] 730/810 batches processed...\n",
            "  [Progress] 740/810 batches processed...\n",
            "  [Progress] 740/810 batches processed...\n",
            "  [Progress] 750/810 batches processed...\n",
            "  [Progress] 750/810 batches processed...\n",
            "  [Progress] 760/810 batches processed...\n",
            "  [Progress] 760/810 batches processed...\n",
            "  [Progress] 770/810 batches processed...\n",
            "  [Progress] 770/810 batches processed...\n",
            "  [Progress] 780/810 batches processed...\n",
            "  [Progress] 780/810 batches processed...\n",
            "  [Progress] 790/810 batches processed...\n",
            "  [Progress] 790/810 batches processed...\n",
            "  [Progress] 800/810 batches processed...\n",
            "  [Progress] 800/810 batches processed...\n",
            "  [Progress] 810/810 batches processed...\n",
            "[INFO]  Total features extracted: (6475, 2048)\n",
            "\n",
            "[STEP 1/2] Extracting testing features...\n",
            "[INFO] Extracting features from 1215 images in 152 steps...\n",
            "  [Progress] 810/810 batches processed...\n",
            "[INFO]  Total features extracted: (6475, 2048)\n",
            "\n",
            "[STEP 1/2] Extracting testing features...\n",
            "[INFO] Extracting features from 1215 images in 152 steps...\n",
            "  [Progress] 10/152 batches processed...\n",
            "  [Progress] 10/152 batches processed...\n",
            "  [Progress] 20/152 batches processed...\n",
            "  [Progress] 20/152 batches processed...\n",
            "  [Progress] 30/152 batches processed...\n",
            "  [Progress] 30/152 batches processed...\n",
            "  [Progress] 40/152 batches processed...\n",
            "  [Progress] 40/152 batches processed...\n",
            "  [Progress] 50/152 batches processed...\n",
            "  [Progress] 50/152 batches processed...\n",
            "  [Progress] 60/152 batches processed...\n",
            "  [Progress] 60/152 batches processed...\n",
            "  [Progress] 70/152 batches processed...\n",
            "  [Progress] 70/152 batches processed...\n",
            "  [Progress] 80/152 batches processed...\n",
            "  [Progress] 80/152 batches processed...\n",
            "  [Progress] 90/152 batches processed...\n",
            "  [Progress] 90/152 batches processed...\n",
            "  [Progress] 100/152 batches processed...\n",
            "  [Progress] 100/152 batches processed...\n",
            "  [Progress] 110/152 batches processed...\n",
            "  [Progress] 110/152 batches processed...\n",
            "  [Progress] 120/152 batches processed...\n",
            "  [Progress] 120/152 batches processed...\n",
            "  [Progress] 130/152 batches processed...\n",
            "  [Progress] 130/152 batches processed...\n",
            "  [Progress] 140/152 batches processed...\n",
            "  [Progress] 140/152 batches processed...\n",
            "  [Progress] 150/152 batches processed...\n",
            "  [Progress] 150/152 batches processed...\n",
            "[INFO]  Total features extracted: (1215, 2048)\n",
            "[INFO]  Total features extracted: (1215, 2048)\n",
            "[STEP 2/2]  Training features saved: 6475 samples\n",
            "[STEP 2/2]  Training features saved: 6475 samples\n",
            "[STEP 2/2]  Testing features saved: 1215 samples\n",
            "\n",
            "[INFO]  Batch_5 completed successfully!\n",
            "\n",
            "Found 6354 images belonging to 2 classes.\n",
            "[STEP 2/2]  Testing features saved: 1215 samples\n",
            "\n",
            "[INFO]  Batch_5 completed successfully!\n",
            "\n",
            "Found 6354 images belonging to 2 classes.\n",
            "Found 149 images belonging to 2 classes.\n",
            "Found 149 images belonging to 2 classes.\n",
            "\n",
            "============================================================\n",
            "[BATCH] Processing Batch_6\n",
            "============================================================\n",
            "\n",
            "[STEP 1/2] Extracting training features...\n",
            "[INFO] Extracting features from 6354 images in 795 steps...\n",
            "\n",
            "============================================================\n",
            "[BATCH] Processing Batch_6\n",
            "============================================================\n",
            "\n",
            "[STEP 1/2] Extracting training features...\n",
            "[INFO] Extracting features from 6354 images in 795 steps...\n",
            "  [Progress] 10/795 batches processed...\n",
            "  [Progress] 10/795 batches processed...\n",
            "  [Progress] 20/795 batches processed...\n",
            "  [Progress] 20/795 batches processed...\n",
            "  [Progress] 30/795 batches processed...\n",
            "  [Progress] 30/795 batches processed...\n",
            "  [Progress] 40/795 batches processed...\n",
            "  [Progress] 40/795 batches processed...\n",
            "  [Progress] 50/795 batches processed...\n",
            "  [Progress] 50/795 batches processed...\n",
            "  [Progress] 60/795 batches processed...\n",
            "  [Progress] 60/795 batches processed...\n",
            "  [Progress] 70/795 batches processed...\n",
            "  [Progress] 70/795 batches processed...\n",
            "  [Progress] 80/795 batches processed...\n",
            "  [Progress] 80/795 batches processed...\n",
            "  [Progress] 90/795 batches processed...\n",
            "  [Progress] 90/795 batches processed...\n",
            "  [Progress] 100/795 batches processed...\n",
            "  [Progress] 100/795 batches processed...\n",
            "  [Progress] 110/795 batches processed...\n",
            "  [Progress] 110/795 batches processed...\n",
            "  [Progress] 120/795 batches processed...\n",
            "  [Progress] 120/795 batches processed...\n",
            "  [Progress] 130/795 batches processed...\n",
            "  [Progress] 130/795 batches processed...\n",
            "  [Progress] 140/795 batches processed...\n",
            "  [Progress] 140/795 batches processed...\n",
            "  [Progress] 150/795 batches processed...\n",
            "  [Progress] 150/795 batches processed...\n",
            "  [Progress] 160/795 batches processed...\n",
            "  [Progress] 160/795 batches processed...\n",
            "  [Progress] 170/795 batches processed...\n",
            "  [Progress] 170/795 batches processed...\n",
            "  [Progress] 180/795 batches processed...\n",
            "  [Progress] 180/795 batches processed...\n",
            "  [Progress] 190/795 batches processed...\n",
            "  [Progress] 190/795 batches processed...\n",
            "  [Progress] 200/795 batches processed...\n",
            "  [Progress] 200/795 batches processed...\n",
            "  [Progress] 210/795 batches processed...\n",
            "  [Progress] 210/795 batches processed...\n",
            "  [Progress] 220/795 batches processed...\n",
            "  [Progress] 220/795 batches processed...\n",
            "  [Progress] 230/795 batches processed...\n",
            "  [Progress] 230/795 batches processed...\n",
            "  [Progress] 240/795 batches processed...\n",
            "  [Progress] 240/795 batches processed...\n",
            "  [Progress] 250/795 batches processed...\n",
            "  [Progress] 250/795 batches processed...\n",
            "  [Progress] 260/795 batches processed...\n",
            "  [Progress] 260/795 batches processed...\n",
            "  [Progress] 270/795 batches processed...\n",
            "  [Progress] 270/795 batches processed...\n",
            "  [Progress] 280/795 batches processed...\n",
            "  [Progress] 280/795 batches processed...\n",
            "  [Progress] 290/795 batches processed...\n",
            "  [Progress] 290/795 batches processed...\n",
            "  [Progress] 300/795 batches processed...\n",
            "  [Progress] 300/795 batches processed...\n",
            "  [Progress] 310/795 batches processed...\n",
            "  [Progress] 310/795 batches processed...\n",
            "  [Progress] 320/795 batches processed...\n",
            "  [Progress] 320/795 batches processed...\n",
            "  [Progress] 330/795 batches processed...\n",
            "  [Progress] 330/795 batches processed...\n",
            "  [Progress] 340/795 batches processed...\n",
            "  [Progress] 340/795 batches processed...\n",
            "  [Progress] 350/795 batches processed...\n",
            "  [Progress] 350/795 batches processed...\n",
            "  [Progress] 360/795 batches processed...\n",
            "  [Progress] 360/795 batches processed...\n",
            "  [Progress] 370/795 batches processed...\n",
            "  [Progress] 370/795 batches processed...\n",
            "  [Progress] 380/795 batches processed...\n",
            "  [Progress] 380/795 batches processed...\n",
            "  [Progress] 390/795 batches processed...\n",
            "  [Progress] 390/795 batches processed...\n",
            "  [Progress] 400/795 batches processed...\n",
            "  [Progress] 400/795 batches processed...\n",
            "  [Progress] 410/795 batches processed...\n",
            "  [Progress] 410/795 batches processed...\n",
            "  [Progress] 420/795 batches processed...\n",
            "  [Progress] 420/795 batches processed...\n",
            "  [Progress] 430/795 batches processed...\n",
            "  [Progress] 430/795 batches processed...\n",
            "  [Progress] 440/795 batches processed...\n",
            "  [Progress] 440/795 batches processed...\n",
            "  [Progress] 450/795 batches processed...\n",
            "  [Progress] 450/795 batches processed...\n",
            "  [Progress] 460/795 batches processed...\n",
            "  [Progress] 460/795 batches processed...\n",
            "  [Progress] 470/795 batches processed...\n",
            "  [Progress] 470/795 batches processed...\n",
            "  [Progress] 480/795 batches processed...\n",
            "  [Progress] 480/795 batches processed...\n",
            "  [Progress] 490/795 batches processed...\n",
            "  [Progress] 490/795 batches processed...\n",
            "  [Progress] 500/795 batches processed...\n",
            "  [Progress] 500/795 batches processed...\n",
            "  [Progress] 510/795 batches processed...\n",
            "  [Progress] 510/795 batches processed...\n",
            "  [Progress] 520/795 batches processed...\n",
            "  [Progress] 520/795 batches processed...\n",
            "  [Progress] 530/795 batches processed...\n",
            "  [Progress] 530/795 batches processed...\n",
            "  [Progress] 540/795 batches processed...\n",
            "  [Progress] 540/795 batches processed...\n",
            "  [Progress] 550/795 batches processed...\n",
            "  [Progress] 550/795 batches processed...\n",
            "  [Progress] 560/795 batches processed...\n",
            "  [Progress] 560/795 batches processed...\n",
            "  [Progress] 570/795 batches processed...\n",
            "  [Progress] 570/795 batches processed...\n",
            "  [Progress] 580/795 batches processed...\n",
            "  [Progress] 580/795 batches processed...\n",
            "  [Progress] 590/795 batches processed...\n",
            "  [Progress] 590/795 batches processed...\n",
            "  [Progress] 600/795 batches processed...\n",
            "  [Progress] 600/795 batches processed...\n",
            "  [Progress] 610/795 batches processed...\n",
            "  [Progress] 610/795 batches processed...\n",
            "  [Progress] 620/795 batches processed...\n",
            "  [Progress] 620/795 batches processed...\n",
            "  [Progress] 630/795 batches processed...\n",
            "  [Progress] 630/795 batches processed...\n",
            "  [Progress] 640/795 batches processed...\n",
            "  [Progress] 640/795 batches processed...\n",
            "  [Progress] 650/795 batches processed...\n",
            "  [Progress] 650/795 batches processed...\n",
            "  [Progress] 660/795 batches processed...\n",
            "  [Progress] 660/795 batches processed...\n",
            "  [Progress] 670/795 batches processed...\n",
            "  [Progress] 670/795 batches processed...\n",
            "  [Progress] 680/795 batches processed...\n",
            "  [Progress] 680/795 batches processed...\n",
            "  [Progress] 690/795 batches processed...\n",
            "  [Progress] 690/795 batches processed...\n",
            "  [Progress] 700/795 batches processed...\n",
            "  [Progress] 700/795 batches processed...\n",
            "  [Progress] 710/795 batches processed...\n",
            "  [Progress] 710/795 batches processed...\n",
            "  [Progress] 720/795 batches processed...\n",
            "  [Progress] 720/795 batches processed...\n",
            "  [Progress] 730/795 batches processed...\n",
            "  [Progress] 730/795 batches processed...\n",
            "  [Progress] 740/795 batches processed...\n",
            "  [Progress] 740/795 batches processed...\n",
            "  [Progress] 750/795 batches processed...\n",
            "  [Progress] 750/795 batches processed...\n",
            "  [Progress] 760/795 batches processed...\n",
            "  [Progress] 760/795 batches processed...\n",
            "  [Progress] 770/795 batches processed...\n",
            "  [Progress] 770/795 batches processed...\n",
            "  [Progress] 780/795 batches processed...\n",
            "  [Progress] 780/795 batches processed...\n",
            "  [Progress] 790/795 batches processed...\n",
            "  [Progress] 790/795 batches processed...\n",
            "[INFO]  Total features extracted: (6354, 2048)\n",
            "\n",
            "[STEP 1/2] Extracting testing features...\n",
            "[INFO] Extracting features from 149 images in 19 steps...\n",
            "[INFO]  Total features extracted: (6354, 2048)\n",
            "\n",
            "[STEP 1/2] Extracting testing features...\n",
            "[INFO] Extracting features from 149 images in 19 steps...\n",
            "  [Progress] 10/19 batches processed...\n",
            "  [Progress] 10/19 batches processed...\n",
            "[INFO]  Total features extracted: (149, 2048)\n",
            "[INFO]  Total features extracted: (149, 2048)\n",
            "[STEP 2/2]  Training features saved: 6354 samples\n",
            "[STEP 2/2]  Testing features saved: 149 samples\n",
            "[STEP 2/2]  Training features saved: 6354 samples\n",
            "[STEP 2/2]  Testing features saved: 149 samples\n",
            "\n",
            "[INFO]  Batch_6 completed successfully!\n",
            "\n",
            "Found 6236 images belonging to 2 classes.\n",
            "\n",
            "[INFO]  Batch_6 completed successfully!\n",
            "\n",
            "Found 6236 images belonging to 2 classes.\n",
            "Found 59 images belonging to 2 classes.\n",
            "Found 59 images belonging to 2 classes.\n",
            "\n",
            "============================================================\n",
            "[BATCH] Processing Batch_7\n",
            "============================================================\n",
            "\n",
            "[STEP 1/2] Extracting training features...\n",
            "[INFO] Extracting features from 6236 images in 780 steps...\n",
            "\n",
            "============================================================\n",
            "[BATCH] Processing Batch_7\n",
            "============================================================\n",
            "\n",
            "[STEP 1/2] Extracting training features...\n",
            "[INFO] Extracting features from 6236 images in 780 steps...\n",
            "  [Progress] 10/780 batches processed...\n",
            "  [Progress] 10/780 batches processed...\n",
            "  [Progress] 20/780 batches processed...\n",
            "  [Progress] 20/780 batches processed...\n",
            "  [Progress] 30/780 batches processed...\n",
            "  [Progress] 30/780 batches processed...\n",
            "  [Progress] 40/780 batches processed...\n",
            "  [Progress] 40/780 batches processed...\n",
            "  [Progress] 50/780 batches processed...\n",
            "  [Progress] 50/780 batches processed...\n",
            "  [Progress] 60/780 batches processed...\n",
            "  [Progress] 60/780 batches processed...\n",
            "  [Progress] 70/780 batches processed...\n",
            "  [Progress] 70/780 batches processed...\n",
            "  [Progress] 80/780 batches processed...\n",
            "  [Progress] 80/780 batches processed...\n",
            "  [Progress] 90/780 batches processed...\n",
            "  [Progress] 90/780 batches processed...\n",
            "  [Progress] 100/780 batches processed...\n",
            "  [Progress] 100/780 batches processed...\n",
            "  [Progress] 110/780 batches processed...\n",
            "  [Progress] 110/780 batches processed...\n",
            "  [Progress] 120/780 batches processed...\n",
            "  [Progress] 120/780 batches processed...\n",
            "  [Progress] 130/780 batches processed...\n",
            "  [Progress] 130/780 batches processed...\n",
            "  [Progress] 140/780 batches processed...\n",
            "  [Progress] 140/780 batches processed...\n",
            "  [Progress] 150/780 batches processed...\n",
            "  [Progress] 150/780 batches processed...\n",
            "  [Progress] 160/780 batches processed...\n",
            "  [Progress] 160/780 batches processed...\n",
            "  [Progress] 170/780 batches processed...\n",
            "  [Progress] 170/780 batches processed...\n",
            "  [Progress] 180/780 batches processed...\n",
            "  [Progress] 180/780 batches processed...\n",
            "  [Progress] 190/780 batches processed...\n",
            "  [Progress] 190/780 batches processed...\n",
            "  [Progress] 200/780 batches processed...\n",
            "  [Progress] 200/780 batches processed...\n",
            "  [Progress] 210/780 batches processed...\n",
            "  [Progress] 210/780 batches processed...\n",
            "  [Progress] 220/780 batches processed...\n",
            "  [Progress] 220/780 batches processed...\n",
            "  [Progress] 230/780 batches processed...\n",
            "  [Progress] 230/780 batches processed...\n",
            "  [Progress] 240/780 batches processed...\n",
            "  [Progress] 240/780 batches processed...\n",
            "  [Progress] 250/780 batches processed...\n",
            "  [Progress] 250/780 batches processed...\n",
            "  [Progress] 260/780 batches processed...\n",
            "  [Progress] 260/780 batches processed...\n",
            "  [Progress] 270/780 batches processed...\n",
            "  [Progress] 270/780 batches processed...\n",
            "  [Progress] 280/780 batches processed...\n",
            "  [Progress] 280/780 batches processed...\n",
            "  [Progress] 290/780 batches processed...\n",
            "  [Progress] 290/780 batches processed...\n",
            "  [Progress] 300/780 batches processed...\n",
            "  [Progress] 300/780 batches processed...\n",
            "  [Progress] 310/780 batches processed...\n",
            "  [Progress] 310/780 batches processed...\n",
            "  [Progress] 320/780 batches processed...\n",
            "  [Progress] 320/780 batches processed...\n",
            "  [Progress] 330/780 batches processed...\n",
            "  [Progress] 330/780 batches processed...\n",
            "  [Progress] 340/780 batches processed...\n",
            "  [Progress] 340/780 batches processed...\n",
            "  [Progress] 350/780 batches processed...\n",
            "  [Progress] 350/780 batches processed...\n",
            "  [Progress] 360/780 batches processed...\n",
            "  [Progress] 360/780 batches processed...\n",
            "  [Progress] 370/780 batches processed...\n",
            "  [Progress] 370/780 batches processed...\n",
            "  [Progress] 380/780 batches processed...\n",
            "  [Progress] 380/780 batches processed...\n",
            "  [Progress] 390/780 batches processed...\n",
            "  [Progress] 390/780 batches processed...\n",
            "  [Progress] 400/780 batches processed...\n",
            "  [Progress] 400/780 batches processed...\n",
            "  [Progress] 410/780 batches processed...\n",
            "  [Progress] 410/780 batches processed...\n",
            "  [Progress] 420/780 batches processed...\n",
            "  [Progress] 420/780 batches processed...\n",
            "  [Progress] 430/780 batches processed...\n",
            "  [Progress] 430/780 batches processed...\n",
            "  [Progress] 440/780 batches processed...\n",
            "  [Progress] 440/780 batches processed...\n",
            "  [Progress] 450/780 batches processed...\n",
            "  [Progress] 450/780 batches processed...\n",
            "  [Progress] 460/780 batches processed...\n",
            "  [Progress] 460/780 batches processed...\n",
            "  [Progress] 470/780 batches processed...\n",
            "  [Progress] 470/780 batches processed...\n",
            "  [Progress] 480/780 batches processed...\n",
            "  [Progress] 480/780 batches processed...\n",
            "  [Progress] 490/780 batches processed...\n",
            "  [Progress] 490/780 batches processed...\n",
            "  [Progress] 500/780 batches processed...\n",
            "  [Progress] 500/780 batches processed...\n",
            "  [Progress] 510/780 batches processed...\n",
            "  [Progress] 510/780 batches processed...\n",
            "  [Progress] 520/780 batches processed...\n",
            "  [Progress] 520/780 batches processed...\n",
            "  [Progress] 530/780 batches processed...\n",
            "  [Progress] 530/780 batches processed...\n",
            "  [Progress] 540/780 batches processed...\n",
            "  [Progress] 540/780 batches processed...\n",
            "  [Progress] 550/780 batches processed...\n",
            "  [Progress] 550/780 batches processed...\n",
            "  [Progress] 560/780 batches processed...\n",
            "  [Progress] 560/780 batches processed...\n",
            "  [Progress] 570/780 batches processed...\n",
            "  [Progress] 570/780 batches processed...\n",
            "  [Progress] 580/780 batches processed...\n",
            "  [Progress] 580/780 batches processed...\n",
            "  [Progress] 590/780 batches processed...\n",
            "  [Progress] 590/780 batches processed...\n",
            "  [Progress] 600/780 batches processed...\n",
            "  [Progress] 600/780 batches processed...\n",
            "  [Progress] 610/780 batches processed...\n",
            "  [Progress] 610/780 batches processed...\n",
            "  [Progress] 620/780 batches processed...\n",
            "  [Progress] 620/780 batches processed...\n",
            "  [Progress] 630/780 batches processed...\n",
            "  [Progress] 630/780 batches processed...\n",
            "  [Progress] 640/780 batches processed...\n",
            "  [Progress] 640/780 batches processed...\n",
            "  [Progress] 650/780 batches processed...\n",
            "  [Progress] 650/780 batches processed...\n",
            "  [Progress] 660/780 batches processed...\n",
            "  [Progress] 660/780 batches processed...\n",
            "  [Progress] 670/780 batches processed...\n",
            "  [Progress] 670/780 batches processed...\n",
            "  [Progress] 680/780 batches processed...\n",
            "  [Progress] 680/780 batches processed...\n",
            "  [Progress] 690/780 batches processed...\n",
            "  [Progress] 690/780 batches processed...\n",
            "  [Progress] 700/780 batches processed...\n",
            "  [Progress] 700/780 batches processed...\n",
            "  [Progress] 710/780 batches processed...\n",
            "  [Progress] 710/780 batches processed...\n",
            "  [Progress] 720/780 batches processed...\n",
            "  [Progress] 720/780 batches processed...\n",
            "  [Progress] 730/780 batches processed...\n",
            "  [Progress] 730/780 batches processed...\n",
            "  [Progress] 740/780 batches processed...\n",
            "  [Progress] 740/780 batches processed...\n",
            "  [Progress] 750/780 batches processed...\n",
            "  [Progress] 750/780 batches processed...\n",
            "  [Progress] 760/780 batches processed...\n",
            "  [Progress] 760/780 batches processed...\n",
            "  [Progress] 770/780 batches processed...\n",
            "  [Progress] 770/780 batches processed...\n",
            "  [Progress] 780/780 batches processed...\n",
            "[INFO]  Total features extracted: (6236, 2048)\n",
            "\n",
            "[STEP 1/2] Extracting testing features...\n",
            "[INFO] Extracting features from 59 images in 8 steps...\n",
            "  [Progress] 780/780 batches processed...\n",
            "[INFO]  Total features extracted: (6236, 2048)\n",
            "\n",
            "[STEP 1/2] Extracting testing features...\n",
            "[INFO] Extracting features from 59 images in 8 steps...\n",
            "[INFO]  Total features extracted: (59, 2048)\n",
            "[INFO]  Total features extracted: (59, 2048)\n",
            "[STEP 2/2]  Training features saved: 6236 samples\n",
            "[STEP 2/2]  Testing features saved: 59 samples\n",
            "[STEP 2/2]  Training features saved: 6236 samples\n",
            "[STEP 2/2]  Testing features saved: 59 samples\n",
            "\n",
            "[INFO]  Batch_7 completed successfully!\n",
            "\n",
            "Found 6312 images belonging to 2 classes.\n",
            "\n",
            "[INFO]  Batch_7 completed successfully!\n",
            "\n",
            "Found 6312 images belonging to 2 classes.\n",
            "Found 40 images belonging to 2 classes.\n",
            "Found 40 images belonging to 2 classes.\n",
            "\n",
            "============================================================\n",
            "[BATCH] Processing Batch_8\n",
            "============================================================\n",
            "\n",
            "[STEP 1/2] Extracting training features...\n",
            "[INFO] Extracting features from 6312 images in 789 steps...\n",
            "\n",
            "============================================================\n",
            "[BATCH] Processing Batch_8\n",
            "============================================================\n",
            "\n",
            "[STEP 1/2] Extracting training features...\n",
            "[INFO] Extracting features from 6312 images in 789 steps...\n",
            "  [Progress] 10/789 batches processed...\n",
            "  [Progress] 10/789 batches processed...\n",
            "  [Progress] 20/789 batches processed...\n",
            "  [Progress] 20/789 batches processed...\n",
            "  [Progress] 30/789 batches processed...\n",
            "  [Progress] 30/789 batches processed...\n",
            "  [Progress] 40/789 batches processed...\n",
            "  [Progress] 40/789 batches processed...\n",
            "  [Progress] 50/789 batches processed...\n",
            "  [Progress] 50/789 batches processed...\n",
            "  [Progress] 60/789 batches processed...\n",
            "  [Progress] 60/789 batches processed...\n",
            "  [Progress] 70/789 batches processed...\n",
            "  [Progress] 70/789 batches processed...\n",
            "  [Progress] 80/789 batches processed...\n",
            "  [Progress] 80/789 batches processed...\n",
            "  [Progress] 90/789 batches processed...\n",
            "  [Progress] 90/789 batches processed...\n",
            "  [Progress] 100/789 batches processed...\n",
            "  [Progress] 100/789 batches processed...\n",
            "  [Progress] 110/789 batches processed...\n",
            "  [Progress] 110/789 batches processed...\n",
            "  [Progress] 120/789 batches processed...\n",
            "  [Progress] 120/789 batches processed...\n",
            "  [Progress] 130/789 batches processed...\n",
            "  [Progress] 130/789 batches processed...\n",
            "  [Progress] 140/789 batches processed...\n",
            "  [Progress] 140/789 batches processed...\n",
            "  [Progress] 150/789 batches processed...\n",
            "  [Progress] 150/789 batches processed...\n",
            "  [Progress] 160/789 batches processed...\n",
            "  [Progress] 160/789 batches processed...\n",
            "  [Progress] 170/789 batches processed...\n",
            "  [Progress] 170/789 batches processed...\n",
            "  [Progress] 180/789 batches processed...\n",
            "  [Progress] 180/789 batches processed...\n",
            "  [Progress] 190/789 batches processed...\n",
            "  [Progress] 190/789 batches processed...\n",
            "  [Progress] 200/789 batches processed...\n",
            "  [Progress] 200/789 batches processed...\n",
            "  [Progress] 210/789 batches processed...\n",
            "  [Progress] 210/789 batches processed...\n",
            "  [Progress] 220/789 batches processed...\n",
            "  [Progress] 220/789 batches processed...\n",
            "  [Progress] 230/789 batches processed...\n",
            "  [Progress] 230/789 batches processed...\n",
            "  [Progress] 240/789 batches processed...\n",
            "  [Progress] 240/789 batches processed...\n",
            "  [Progress] 250/789 batches processed...\n",
            "  [Progress] 250/789 batches processed...\n",
            "  [Progress] 260/789 batches processed...\n",
            "  [Progress] 260/789 batches processed...\n",
            "  [Progress] 270/789 batches processed...\n",
            "  [Progress] 270/789 batches processed...\n",
            "  [Progress] 280/789 batches processed...\n",
            "  [Progress] 280/789 batches processed...\n",
            "  [Progress] 290/789 batches processed...\n",
            "  [Progress] 290/789 batches processed...\n",
            "  [Progress] 300/789 batches processed...\n",
            "  [Progress] 300/789 batches processed...\n",
            "  [Progress] 310/789 batches processed...\n",
            "  [Progress] 310/789 batches processed...\n",
            "  [Progress] 320/789 batches processed...\n",
            "  [Progress] 320/789 batches processed...\n",
            "  [Progress] 330/789 batches processed...\n",
            "  [Progress] 330/789 batches processed...\n",
            "  [Progress] 340/789 batches processed...\n",
            "  [Progress] 340/789 batches processed...\n",
            "  [Progress] 350/789 batches processed...\n",
            "  [Progress] 350/789 batches processed...\n",
            "  [Progress] 360/789 batches processed...\n",
            "  [Progress] 360/789 batches processed...\n",
            "  [Progress] 370/789 batches processed...\n",
            "  [Progress] 370/789 batches processed...\n",
            "  [Progress] 380/789 batches processed...\n",
            "  [Progress] 380/789 batches processed...\n",
            "  [Progress] 390/789 batches processed...\n",
            "  [Progress] 390/789 batches processed...\n",
            "  [Progress] 400/789 batches processed...\n",
            "  [Progress] 400/789 batches processed...\n",
            "  [Progress] 410/789 batches processed...\n",
            "  [Progress] 410/789 batches processed...\n",
            "  [Progress] 420/789 batches processed...\n",
            "  [Progress] 420/789 batches processed...\n",
            "  [Progress] 430/789 batches processed...\n",
            "  [Progress] 430/789 batches processed...\n",
            "  [Progress] 440/789 batches processed...\n",
            "  [Progress] 440/789 batches processed...\n",
            "  [Progress] 450/789 batches processed...\n",
            "  [Progress] 450/789 batches processed...\n",
            "  [Progress] 460/789 batches processed...\n",
            "  [Progress] 460/789 batches processed...\n",
            "  [Progress] 470/789 batches processed...\n",
            "  [Progress] 470/789 batches processed...\n",
            "  [Progress] 480/789 batches processed...\n",
            "  [Progress] 480/789 batches processed...\n",
            "  [Progress] 490/789 batches processed...\n",
            "  [Progress] 490/789 batches processed...\n",
            "  [Progress] 500/789 batches processed...\n",
            "  [Progress] 500/789 batches processed...\n",
            "  [Progress] 510/789 batches processed...\n",
            "  [Progress] 510/789 batches processed...\n",
            "  [Progress] 520/789 batches processed...\n",
            "  [Progress] 520/789 batches processed...\n",
            "  [Progress] 530/789 batches processed...\n",
            "  [Progress] 530/789 batches processed...\n",
            "  [Progress] 540/789 batches processed...\n",
            "  [Progress] 540/789 batches processed...\n",
            "  [Progress] 550/789 batches processed...\n",
            "  [Progress] 550/789 batches processed...\n",
            "  [Progress] 560/789 batches processed...\n",
            "  [Progress] 560/789 batches processed...\n",
            "  [Progress] 570/789 batches processed...\n",
            "  [Progress] 570/789 batches processed...\n",
            "  [Progress] 580/789 batches processed...\n",
            "  [Progress] 580/789 batches processed...\n",
            "  [Progress] 590/789 batches processed...\n",
            "  [Progress] 590/789 batches processed...\n",
            "  [Progress] 600/789 batches processed...\n",
            "  [Progress] 600/789 batches processed...\n",
            "  [Progress] 610/789 batches processed...\n",
            "  [Progress] 610/789 batches processed...\n",
            "  [Progress] 620/789 batches processed...\n",
            "  [Progress] 620/789 batches processed...\n",
            "  [Progress] 630/789 batches processed...\n",
            "  [Progress] 630/789 batches processed...\n",
            "  [Progress] 640/789 batches processed...\n",
            "  [Progress] 640/789 batches processed...\n",
            "  [Progress] 650/789 batches processed...\n",
            "  [Progress] 650/789 batches processed...\n",
            "  [Progress] 660/789 batches processed...\n",
            "  [Progress] 660/789 batches processed...\n",
            "  [Progress] 670/789 batches processed...\n",
            "  [Progress] 670/789 batches processed...\n",
            "  [Progress] 680/789 batches processed...\n",
            "  [Progress] 680/789 batches processed...\n",
            "  [Progress] 690/789 batches processed...\n",
            "  [Progress] 690/789 batches processed...\n",
            "  [Progress] 700/789 batches processed...\n",
            "  [Progress] 700/789 batches processed...\n",
            "  [Progress] 710/789 batches processed...\n",
            "  [Progress] 710/789 batches processed...\n",
            "  [Progress] 720/789 batches processed...\n",
            "  [Progress] 720/789 batches processed...\n",
            "  [Progress] 730/789 batches processed...\n",
            "  [Progress] 730/789 batches processed...\n",
            "  [Progress] 740/789 batches processed...\n",
            "  [Progress] 740/789 batches processed...\n",
            "  [Progress] 750/789 batches processed...\n",
            "  [Progress] 750/789 batches processed...\n",
            "  [Progress] 760/789 batches processed...\n",
            "  [Progress] 760/789 batches processed...\n",
            "  [Progress] 770/789 batches processed...\n",
            "  [Progress] 770/789 batches processed...\n",
            "  [Progress] 780/789 batches processed...\n",
            "  [Progress] 780/789 batches processed...\n",
            "[INFO]  Total features extracted: (6312, 2048)\n",
            "\n",
            "[STEP 1/2] Extracting testing features...\n",
            "[INFO] Extracting features from 40 images in 5 steps...\n",
            "[INFO]  Total features extracted: (6312, 2048)\n",
            "\n",
            "[STEP 1/2] Extracting testing features...\n",
            "[INFO] Extracting features from 40 images in 5 steps...\n",
            "[INFO]  Total features extracted: (40, 2048)\n",
            "[INFO]  Total features extracted: (40, 2048)\n",
            "[STEP 2/2]  Training features saved: 6312 samples\n",
            "[STEP 2/2]  Testing features saved: 40 samples\n",
            "[STEP 2/2]  Training features saved: 6312 samples\n",
            "[STEP 2/2]  Testing features saved: 40 samples\n",
            "\n",
            "[INFO]  Batch_8 completed successfully!\n",
            "\n",
            "Found 3665 images belonging to 2 classes.\n",
            "\n",
            "[INFO]  Batch_8 completed successfully!\n",
            "\n",
            "Found 3665 images belonging to 2 classes.\n",
            "Found 702 images belonging to 2 classes.\n",
            "Found 702 images belonging to 2 classes.\n",
            "\n",
            "============================================================\n",
            "[BATCH] Processing Batch_9\n",
            "============================================================\n",
            "\n",
            "[STEP 1/2] Extracting training features...\n",
            "[INFO] Extracting features from 3665 images in 459 steps...\n",
            "\n",
            "============================================================\n",
            "[BATCH] Processing Batch_9\n",
            "============================================================\n",
            "\n",
            "[STEP 1/2] Extracting training features...\n",
            "[INFO] Extracting features from 3665 images in 459 steps...\n",
            "  [Progress] 10/459 batches processed...\n",
            "  [Progress] 10/459 batches processed...\n",
            "  [Progress] 20/459 batches processed...\n",
            "  [Progress] 20/459 batches processed...\n",
            "  [Progress] 30/459 batches processed...\n",
            "  [Progress] 30/459 batches processed...\n",
            "  [Progress] 40/459 batches processed...\n",
            "  [Progress] 40/459 batches processed...\n",
            "  [Progress] 50/459 batches processed...\n",
            "  [Progress] 50/459 batches processed...\n",
            "  [Progress] 60/459 batches processed...\n",
            "  [Progress] 60/459 batches processed...\n",
            "  [Progress] 70/459 batches processed...\n",
            "  [Progress] 70/459 batches processed...\n",
            "  [Progress] 80/459 batches processed...\n",
            "  [Progress] 80/459 batches processed...\n",
            "  [Progress] 90/459 batches processed...\n",
            "  [Progress] 90/459 batches processed...\n",
            "  [Progress] 100/459 batches processed...\n",
            "  [Progress] 100/459 batches processed...\n",
            "  [Progress] 110/459 batches processed...\n",
            "  [Progress] 110/459 batches processed...\n",
            "  [Progress] 120/459 batches processed...\n",
            "  [Progress] 120/459 batches processed...\n",
            "  [Progress] 130/459 batches processed...\n",
            "  [Progress] 130/459 batches processed...\n",
            "  [Progress] 140/459 batches processed...\n",
            "  [Progress] 140/459 batches processed...\n",
            "  [Progress] 150/459 batches processed...\n",
            "  [Progress] 150/459 batches processed...\n",
            "  [Progress] 160/459 batches processed...\n",
            "  [Progress] 160/459 batches processed...\n",
            "  [Progress] 170/459 batches processed...\n",
            "  [Progress] 170/459 batches processed...\n",
            "  [Progress] 180/459 batches processed...\n",
            "  [Progress] 180/459 batches processed...\n",
            "  [Progress] 190/459 batches processed...\n",
            "  [Progress] 190/459 batches processed...\n",
            "  [Progress] 200/459 batches processed...\n",
            "  [Progress] 200/459 batches processed...\n",
            "  [Progress] 210/459 batches processed...\n",
            "  [Progress] 210/459 batches processed...\n",
            "  [Progress] 220/459 batches processed...\n",
            "  [Progress] 220/459 batches processed...\n",
            "  [Progress] 230/459 batches processed...\n",
            "  [Progress] 230/459 batches processed...\n",
            "  [Progress] 240/459 batches processed...\n",
            "  [Progress] 240/459 batches processed...\n",
            "  [Progress] 250/459 batches processed...\n",
            "  [Progress] 250/459 batches processed...\n",
            "  [Progress] 260/459 batches processed...\n",
            "  [Progress] 260/459 batches processed...\n",
            "  [Progress] 270/459 batches processed...\n",
            "  [Progress] 270/459 batches processed...\n",
            "  [Progress] 280/459 batches processed...\n",
            "  [Progress] 280/459 batches processed...\n",
            "  [Progress] 290/459 batches processed...\n",
            "  [Progress] 290/459 batches processed...\n",
            "  [Progress] 300/459 batches processed...\n",
            "  [Progress] 300/459 batches processed...\n",
            "  [Progress] 310/459 batches processed...\n",
            "  [Progress] 310/459 batches processed...\n",
            "  [Progress] 320/459 batches processed...\n",
            "  [Progress] 320/459 batches processed...\n",
            "  [Progress] 330/459 batches processed...\n",
            "  [Progress] 330/459 batches processed...\n",
            "  [Progress] 340/459 batches processed...\n",
            "  [Progress] 340/459 batches processed...\n",
            "  [Progress] 350/459 batches processed...\n",
            "  [Progress] 350/459 batches processed...\n",
            "  [Progress] 360/459 batches processed...\n",
            "  [Progress] 360/459 batches processed...\n",
            "  [Progress] 370/459 batches processed...\n",
            "  [Progress] 370/459 batches processed...\n",
            "  [Progress] 380/459 batches processed...\n",
            "  [Progress] 380/459 batches processed...\n",
            "  [Progress] 390/459 batches processed...\n",
            "  [Progress] 390/459 batches processed...\n",
            "  [Progress] 400/459 batches processed...\n",
            "  [Progress] 400/459 batches processed...\n",
            "  [Progress] 410/459 batches processed...\n",
            "  [Progress] 410/459 batches processed...\n",
            "  [Progress] 420/459 batches processed...\n",
            "  [Progress] 420/459 batches processed...\n",
            "  [Progress] 430/459 batches processed...\n",
            "  [Progress] 430/459 batches processed...\n",
            "  [Progress] 440/459 batches processed...\n",
            "  [Progress] 440/459 batches processed...\n",
            "  [Progress] 450/459 batches processed...\n",
            "  [Progress] 450/459 batches processed...\n",
            "[INFO]  Total features extracted: (3665, 2048)\n",
            "\n",
            "[STEP 1/2] Extracting testing features...\n",
            "[INFO] Extracting features from 702 images in 88 steps...\n",
            "[INFO]  Total features extracted: (3665, 2048)\n",
            "\n",
            "[STEP 1/2] Extracting testing features...\n",
            "[INFO] Extracting features from 702 images in 88 steps...\n",
            "  [Progress] 10/88 batches processed...\n",
            "  [Progress] 10/88 batches processed...\n",
            "  [Progress] 20/88 batches processed...\n",
            "  [Progress] 20/88 batches processed...\n",
            "  [Progress] 30/88 batches processed...\n",
            "  [Progress] 30/88 batches processed...\n",
            "  [Progress] 40/88 batches processed...\n",
            "  [Progress] 40/88 batches processed...\n",
            "  [Progress] 50/88 batches processed...\n",
            "  [Progress] 50/88 batches processed...\n",
            "  [Progress] 60/88 batches processed...\n",
            "  [Progress] 60/88 batches processed...\n",
            "  [Progress] 70/88 batches processed...\n",
            "  [Progress] 70/88 batches processed...\n",
            "  [Progress] 80/88 batches processed...\n",
            "  [Progress] 80/88 batches processed...\n",
            "[INFO]  Total features extracted: (702, 2048)\n",
            "[INFO]  Total features extracted: (702, 2048)\n",
            "[STEP 2/2]  Training features saved: 3665 samples\n",
            "[STEP 2/2]  Training features saved: 3665 samples\n",
            "[STEP 2/2]  Testing features saved: 702 samples\n",
            "\n",
            "[INFO]  Batch_9 completed successfully!\n",
            "\n",
            "Found 5943 images belonging to 2 classes.\n",
            "[STEP 2/2]  Testing features saved: 702 samples\n",
            "\n",
            "[INFO]  Batch_9 completed successfully!\n",
            "\n",
            "Found 5943 images belonging to 2 classes.\n",
            "Found 133 images belonging to 2 classes.\n",
            "\n",
            "============================================================\n",
            "[BATCH] Processing Batch_10\n",
            "============================================================\n",
            "\n",
            "[STEP 1/2] Extracting training features...\n",
            "[INFO] Extracting features from 5943 images in 743 steps...\n",
            "Found 133 images belonging to 2 classes.\n",
            "\n",
            "============================================================\n",
            "[BATCH] Processing Batch_10\n",
            "============================================================\n",
            "\n",
            "[STEP 1/2] Extracting training features...\n",
            "[INFO] Extracting features from 5943 images in 743 steps...\n",
            "  [Progress] 10/743 batches processed...\n",
            "  [Progress] 10/743 batches processed...\n",
            "  [Progress] 20/743 batches processed...\n",
            "  [Progress] 20/743 batches processed...\n",
            "  [Progress] 30/743 batches processed...\n",
            "  [Progress] 30/743 batches processed...\n",
            "  [Progress] 40/743 batches processed...\n",
            "  [Progress] 40/743 batches processed...\n",
            "  [Progress] 50/743 batches processed...\n",
            "  [Progress] 50/743 batches processed...\n",
            "  [Progress] 60/743 batches processed...\n",
            "  [Progress] 60/743 batches processed...\n",
            "  [Progress] 70/743 batches processed...\n",
            "  [Progress] 70/743 batches processed...\n",
            "  [Progress] 80/743 batches processed...\n",
            "  [Progress] 80/743 batches processed...\n",
            "  [Progress] 90/743 batches processed...\n",
            "  [Progress] 90/743 batches processed...\n",
            "  [Progress] 100/743 batches processed...\n",
            "  [Progress] 100/743 batches processed...\n",
            "  [Progress] 110/743 batches processed...\n",
            "  [Progress] 110/743 batches processed...\n",
            "  [Progress] 120/743 batches processed...\n",
            "  [Progress] 120/743 batches processed...\n",
            "  [Progress] 130/743 batches processed...\n",
            "  [Progress] 130/743 batches processed...\n",
            "  [Progress] 140/743 batches processed...\n",
            "  [Progress] 140/743 batches processed...\n",
            "  [Progress] 150/743 batches processed...\n",
            "  [Progress] 150/743 batches processed...\n",
            "  [Progress] 160/743 batches processed...\n",
            "  [Progress] 160/743 batches processed...\n",
            "  [Progress] 170/743 batches processed...\n",
            "  [Progress] 170/743 batches processed...\n",
            "  [Progress] 180/743 batches processed...\n",
            "  [Progress] 180/743 batches processed...\n",
            "  [Progress] 190/743 batches processed...\n",
            "  [Progress] 190/743 batches processed...\n",
            "  [Progress] 200/743 batches processed...\n",
            "  [Progress] 200/743 batches processed...\n",
            "  [Progress] 210/743 batches processed...\n",
            "  [Progress] 210/743 batches processed...\n",
            "  [Progress] 220/743 batches processed...\n",
            "  [Progress] 220/743 batches processed...\n",
            "  [Progress] 230/743 batches processed...\n",
            "  [Progress] 230/743 batches processed...\n",
            "  [Progress] 240/743 batches processed...\n",
            "  [Progress] 240/743 batches processed...\n",
            "  [Progress] 250/743 batches processed...\n",
            "  [Progress] 250/743 batches processed...\n",
            "  [Progress] 260/743 batches processed...\n",
            "  [Progress] 260/743 batches processed...\n",
            "  [Progress] 270/743 batches processed...\n",
            "  [Progress] 270/743 batches processed...\n",
            "  [Progress] 280/743 batches processed...\n",
            "  [Progress] 280/743 batches processed...\n",
            "  [Progress] 290/743 batches processed...\n",
            "  [Progress] 290/743 batches processed...\n",
            "  [Progress] 300/743 batches processed...\n",
            "  [Progress] 300/743 batches processed...\n",
            "  [Progress] 310/743 batches processed...\n",
            "  [Progress] 310/743 batches processed...\n",
            "  [Progress] 320/743 batches processed...\n",
            "  [Progress] 320/743 batches processed...\n",
            "  [Progress] 330/743 batches processed...\n",
            "  [Progress] 330/743 batches processed...\n",
            "  [Progress] 340/743 batches processed...\n",
            "  [Progress] 340/743 batches processed...\n",
            "  [Progress] 350/743 batches processed...\n",
            "  [Progress] 350/743 batches processed...\n",
            "  [Progress] 360/743 batches processed...\n",
            "  [Progress] 360/743 batches processed...\n",
            "  [Progress] 370/743 batches processed...\n",
            "  [Progress] 370/743 batches processed...\n",
            "  [Progress] 380/743 batches processed...\n",
            "  [Progress] 380/743 batches processed...\n",
            "  [Progress] 390/743 batches processed...\n",
            "  [Progress] 390/743 batches processed...\n",
            "  [Progress] 400/743 batches processed...\n",
            "  [Progress] 400/743 batches processed...\n",
            "  [Progress] 410/743 batches processed...\n",
            "  [Progress] 410/743 batches processed...\n",
            "  [Progress] 420/743 batches processed...\n",
            "  [Progress] 420/743 batches processed...\n",
            "  [Progress] 430/743 batches processed...\n",
            "  [Progress] 430/743 batches processed...\n",
            "  [Progress] 440/743 batches processed...\n",
            "  [Progress] 440/743 batches processed...\n",
            "  [Progress] 450/743 batches processed...\n",
            "  [Progress] 450/743 batches processed...\n",
            "  [Progress] 460/743 batches processed...\n",
            "  [Progress] 460/743 batches processed...\n",
            "  [Progress] 470/743 batches processed...\n",
            "  [Progress] 470/743 batches processed...\n",
            "  [Progress] 480/743 batches processed...\n",
            "  [Progress] 480/743 batches processed...\n",
            "  [Progress] 490/743 batches processed...\n",
            "  [Progress] 490/743 batches processed...\n",
            "  [Progress] 500/743 batches processed...\n",
            "  [Progress] 500/743 batches processed...\n",
            "  [Progress] 510/743 batches processed...\n",
            "  [Progress] 510/743 batches processed...\n",
            "  [Progress] 520/743 batches processed...\n",
            "  [Progress] 520/743 batches processed...\n",
            "  [Progress] 530/743 batches processed...\n",
            "  [Progress] 530/743 batches processed...\n",
            "  [Progress] 540/743 batches processed...\n",
            "  [Progress] 540/743 batches processed...\n",
            "  [Progress] 550/743 batches processed...\n",
            "  [Progress] 550/743 batches processed...\n",
            "  [Progress] 560/743 batches processed...\n",
            "  [Progress] 560/743 batches processed...\n",
            "  [Progress] 570/743 batches processed...\n",
            "  [Progress] 570/743 batches processed...\n",
            "  [Progress] 580/743 batches processed...\n",
            "  [Progress] 580/743 batches processed...\n",
            "  [Progress] 590/743 batches processed...\n",
            "  [Progress] 590/743 batches processed...\n",
            "  [Progress] 600/743 batches processed...\n",
            "  [Progress] 600/743 batches processed...\n",
            "  [Progress] 610/743 batches processed...\n",
            "  [Progress] 610/743 batches processed...\n",
            "  [Progress] 620/743 batches processed...\n",
            "  [Progress] 620/743 batches processed...\n",
            "  [Progress] 630/743 batches processed...\n",
            "  [Progress] 630/743 batches processed...\n",
            "  [Progress] 640/743 batches processed...\n",
            "  [Progress] 640/743 batches processed...\n",
            "  [Progress] 650/743 batches processed...\n",
            "  [Progress] 650/743 batches processed...\n",
            "  [Progress] 660/743 batches processed...\n",
            "  [Progress] 660/743 batches processed...\n",
            "  [Progress] 670/743 batches processed...\n",
            "  [Progress] 670/743 batches processed...\n",
            "  [Progress] 680/743 batches processed...\n",
            "  [Progress] 680/743 batches processed...\n",
            "  [Progress] 690/743 batches processed...\n",
            "  [Progress] 690/743 batches processed...\n",
            "  [Progress] 700/743 batches processed...\n",
            "  [Progress] 700/743 batches processed...\n",
            "  [Progress] 710/743 batches processed...\n",
            "  [Progress] 710/743 batches processed...\n",
            "  [Progress] 720/743 batches processed...\n",
            "  [Progress] 720/743 batches processed...\n",
            "  [Progress] 730/743 batches processed...\n",
            "  [Progress] 730/743 batches processed...\n",
            "  [Progress] 740/743 batches processed...\n",
            "  [Progress] 740/743 batches processed...\n",
            "[INFO]  Total features extracted: (5943, 2048)\n",
            "\n",
            "[STEP 1/2] Extracting testing features...\n",
            "[INFO] Extracting features from 133 images in 17 steps...\n",
            "[INFO]  Total features extracted: (5943, 2048)\n",
            "\n",
            "[STEP 1/2] Extracting testing features...\n",
            "[INFO] Extracting features from 133 images in 17 steps...\n",
            "  [Progress] 10/17 batches processed...\n",
            "  [Progress] 10/17 batches processed...\n",
            "[INFO]  Total features extracted: (133, 2048)\n",
            "[INFO]  Total features extracted: (133, 2048)\n",
            "[STEP 2/2]  Training features saved: 5943 samples\n",
            "[STEP 2/2]  Testing features saved: 133 samples\n",
            "[STEP 2/2]  Training features saved: 5943 samples\n",
            "[STEP 2/2]  Testing features saved: 133 samples\n",
            "\n",
            "[INFO]  Batch_10 completed successfully!\n",
            "\n",
            "Found 4288 images belonging to 2 classes.\n",
            "\n",
            "[INFO]  Batch_10 completed successfully!\n",
            "\n",
            "Found 4288 images belonging to 2 classes.\n",
            "Found 546 images belonging to 2 classes.\n",
            "Found 546 images belonging to 2 classes.\n",
            "\n",
            "============================================================\n",
            "[BATCH] Processing Batch_11\n",
            "============================================================\n",
            "\n",
            "[STEP 1/2] Extracting training features...\n",
            "[INFO] Extracting features from 4288 images in 536 steps...\n",
            "\n",
            "============================================================\n",
            "[BATCH] Processing Batch_11\n",
            "============================================================\n",
            "\n",
            "[STEP 1/2] Extracting training features...\n",
            "[INFO] Extracting features from 4288 images in 536 steps...\n",
            "  [Progress] 10/536 batches processed...\n",
            "  [Progress] 10/536 batches processed...\n",
            "  [Progress] 20/536 batches processed...\n",
            "  [Progress] 20/536 batches processed...\n",
            "  [Progress] 30/536 batches processed...\n",
            "  [Progress] 30/536 batches processed...\n",
            "  [Progress] 40/536 batches processed...\n",
            "  [Progress] 40/536 batches processed...\n",
            "  [Progress] 50/536 batches processed...\n",
            "  [Progress] 50/536 batches processed...\n",
            "  [Progress] 60/536 batches processed...\n",
            "  [Progress] 60/536 batches processed...\n",
            "  [Progress] 70/536 batches processed...\n",
            "  [Progress] 70/536 batches processed...\n",
            "  [Progress] 80/536 batches processed...\n",
            "  [Progress] 80/536 batches processed...\n",
            "  [Progress] 90/536 batches processed...\n",
            "  [Progress] 90/536 batches processed...\n",
            "  [Progress] 100/536 batches processed...\n",
            "  [Progress] 100/536 batches processed...\n",
            "  [Progress] 110/536 batches processed...\n",
            "  [Progress] 110/536 batches processed...\n",
            "  [Progress] 120/536 batches processed...\n",
            "  [Progress] 120/536 batches processed...\n",
            "  [Progress] 130/536 batches processed...\n",
            "  [Progress] 130/536 batches processed...\n",
            "  [Progress] 140/536 batches processed...\n",
            "  [Progress] 140/536 batches processed...\n",
            "  [Progress] 150/536 batches processed...\n",
            "  [Progress] 150/536 batches processed...\n",
            "  [Progress] 160/536 batches processed...\n",
            "  [Progress] 160/536 batches processed...\n",
            "  [Progress] 170/536 batches processed...\n",
            "  [Progress] 170/536 batches processed...\n",
            "  [Progress] 180/536 batches processed...\n",
            "  [Progress] 180/536 batches processed...\n",
            "  [Progress] 190/536 batches processed...\n",
            "  [Progress] 190/536 batches processed...\n",
            "  [Progress] 200/536 batches processed...\n",
            "  [Progress] 200/536 batches processed...\n",
            "  [Progress] 210/536 batches processed...\n",
            "  [Progress] 210/536 batches processed...\n",
            "  [Progress] 220/536 batches processed...\n",
            "  [Progress] 220/536 batches processed...\n",
            "  [Progress] 230/536 batches processed...\n",
            "  [Progress] 230/536 batches processed...\n",
            "  [Progress] 240/536 batches processed...\n",
            "  [Progress] 240/536 batches processed...\n",
            "  [Progress] 250/536 batches processed...\n",
            "  [Progress] 250/536 batches processed...\n",
            "  [Progress] 260/536 batches processed...\n",
            "  [Progress] 260/536 batches processed...\n",
            "  [Progress] 270/536 batches processed...\n",
            "  [Progress] 270/536 batches processed...\n",
            "  [Progress] 280/536 batches processed...\n",
            "  [Progress] 280/536 batches processed...\n",
            "  [Progress] 290/536 batches processed...\n",
            "  [Progress] 290/536 batches processed...\n",
            "  [Progress] 300/536 batches processed...\n",
            "  [Progress] 300/536 batches processed...\n",
            "  [Progress] 310/536 batches processed...\n",
            "  [Progress] 310/536 batches processed...\n",
            "  [Progress] 320/536 batches processed...\n",
            "  [Progress] 320/536 batches processed...\n",
            "  [Progress] 330/536 batches processed...\n",
            "  [Progress] 330/536 batches processed...\n",
            "  [Progress] 340/536 batches processed...\n",
            "  [Progress] 340/536 batches processed...\n",
            "  [Progress] 350/536 batches processed...\n",
            "  [Progress] 350/536 batches processed...\n",
            "  [Progress] 360/536 batches processed...\n",
            "  [Progress] 360/536 batches processed...\n",
            "  [Progress] 370/536 batches processed...\n",
            "  [Progress] 370/536 batches processed...\n",
            "  [Progress] 380/536 batches processed...\n",
            "  [Progress] 380/536 batches processed...\n",
            "  [Progress] 390/536 batches processed...\n",
            "  [Progress] 390/536 batches processed...\n",
            "  [Progress] 400/536 batches processed...\n",
            "  [Progress] 400/536 batches processed...\n",
            "  [Progress] 410/536 batches processed...\n",
            "  [Progress] 410/536 batches processed...\n",
            "  [Progress] 420/536 batches processed...\n",
            "  [Progress] 420/536 batches processed...\n",
            "  [Progress] 430/536 batches processed...\n",
            "  [Progress] 430/536 batches processed...\n",
            "  [Progress] 440/536 batches processed...\n",
            "  [Progress] 440/536 batches processed...\n",
            "  [Progress] 450/536 batches processed...\n",
            "  [Progress] 450/536 batches processed...\n",
            "  [Progress] 460/536 batches processed...\n",
            "  [Progress] 460/536 batches processed...\n",
            "  [Progress] 470/536 batches processed...\n",
            "  [Progress] 470/536 batches processed...\n",
            "  [Progress] 480/536 batches processed...\n",
            "  [Progress] 480/536 batches processed...\n",
            "  [Progress] 490/536 batches processed...\n",
            "  [Progress] 490/536 batches processed...\n",
            "  [Progress] 500/536 batches processed...\n",
            "  [Progress] 500/536 batches processed...\n",
            "  [Progress] 510/536 batches processed...\n",
            "  [Progress] 510/536 batches processed...\n",
            "  [Progress] 520/536 batches processed...\n",
            "  [Progress] 520/536 batches processed...\n",
            "  [Progress] 530/536 batches processed...\n",
            "  [Progress] 530/536 batches processed...\n",
            "[INFO]  Total features extracted: (4288, 2048)\n",
            "\n",
            "[STEP 1/2] Extracting testing features...\n",
            "[INFO] Extracting features from 546 images in 69 steps...\n",
            "[INFO]  Total features extracted: (4288, 2048)\n",
            "\n",
            "[STEP 1/2] Extracting testing features...\n",
            "[INFO] Extracting features from 546 images in 69 steps...\n",
            "  [Progress] 10/69 batches processed...\n",
            "  [Progress] 10/69 batches processed...\n",
            "  [Progress] 20/69 batches processed...\n",
            "  [Progress] 20/69 batches processed...\n",
            "  [Progress] 30/69 batches processed...\n",
            "  [Progress] 30/69 batches processed...\n",
            "  [Progress] 40/69 batches processed...\n",
            "  [Progress] 40/69 batches processed...\n",
            "  [Progress] 50/69 batches processed...\n",
            "  [Progress] 50/69 batches processed...\n",
            "  [Progress] 60/69 batches processed...\n",
            "  [Progress] 60/69 batches processed...\n",
            "[INFO]  Total features extracted: (546, 2048)\n",
            "[INFO]  Total features extracted: (546, 2048)\n",
            "[STEP 2/2]  Training features saved: 4288 samples\n",
            "[STEP 2/2]  Training features saved: 4288 samples\n",
            "[STEP 2/2]  Testing features saved: 546 samples\n",
            "\n",
            "[INFO]  Batch_11 completed successfully!\n",
            "\n",
            "Found 5856 images belonging to 2 classes.\n",
            "[STEP 2/2]  Testing features saved: 546 samples\n",
            "\n",
            "[INFO]  Batch_11 completed successfully!\n",
            "\n",
            "Found 5856 images belonging to 2 classes.\n",
            "Found 35 images belonging to 2 classes.\n",
            "\n",
            "============================================================\n",
            "[BATCH] Processing Batch_12\n",
            "============================================================\n",
            "\n",
            "[STEP 1/2] Extracting training features...\n",
            "[INFO] Extracting features from 5856 images in 732 steps...\n",
            "Found 35 images belonging to 2 classes.\n",
            "\n",
            "============================================================\n",
            "[BATCH] Processing Batch_12\n",
            "============================================================\n",
            "\n",
            "[STEP 1/2] Extracting training features...\n",
            "[INFO] Extracting features from 5856 images in 732 steps...\n",
            "  [Progress] 10/732 batches processed...\n",
            "  [Progress] 10/732 batches processed...\n",
            "  [Progress] 20/732 batches processed...\n",
            "  [Progress] 20/732 batches processed...\n",
            "  [Progress] 30/732 batches processed...\n",
            "  [Progress] 30/732 batches processed...\n",
            "  [Progress] 40/732 batches processed...\n",
            "  [Progress] 40/732 batches processed...\n",
            "  [Progress] 50/732 batches processed...\n",
            "  [Progress] 50/732 batches processed...\n",
            "  [Progress] 60/732 batches processed...\n",
            "  [Progress] 60/732 batches processed...\n",
            "  [Progress] 70/732 batches processed...\n",
            "  [Progress] 70/732 batches processed...\n",
            "  [Progress] 80/732 batches processed...\n",
            "  [Progress] 80/732 batches processed...\n",
            "  [Progress] 90/732 batches processed...\n",
            "  [Progress] 90/732 batches processed...\n",
            "  [Progress] 100/732 batches processed...\n",
            "  [Progress] 100/732 batches processed...\n",
            "  [Progress] 110/732 batches processed...\n",
            "  [Progress] 110/732 batches processed...\n",
            "  [Progress] 120/732 batches processed...\n",
            "  [Progress] 120/732 batches processed...\n",
            "  [Progress] 130/732 batches processed...\n",
            "  [Progress] 130/732 batches processed...\n",
            "  [Progress] 140/732 batches processed...\n",
            "  [Progress] 140/732 batches processed...\n",
            "  [Progress] 150/732 batches processed...\n",
            "  [Progress] 150/732 batches processed...\n",
            "  [Progress] 160/732 batches processed...\n",
            "  [Progress] 160/732 batches processed...\n",
            "  [Progress] 170/732 batches processed...\n",
            "  [Progress] 170/732 batches processed...\n",
            "  [Progress] 180/732 batches processed...\n",
            "  [Progress] 180/732 batches processed...\n",
            "  [Progress] 190/732 batches processed...\n",
            "  [Progress] 190/732 batches processed...\n",
            "  [Progress] 200/732 batches processed...\n",
            "  [Progress] 200/732 batches processed...\n",
            "  [Progress] 210/732 batches processed...\n",
            "  [Progress] 210/732 batches processed...\n",
            "  [Progress] 220/732 batches processed...\n",
            "  [Progress] 220/732 batches processed...\n",
            "  [Progress] 230/732 batches processed...\n",
            "  [Progress] 230/732 batches processed...\n",
            "  [Progress] 240/732 batches processed...\n",
            "  [Progress] 240/732 batches processed...\n",
            "  [Progress] 250/732 batches processed...\n",
            "  [Progress] 250/732 batches processed...\n",
            "  [Progress] 260/732 batches processed...\n",
            "  [Progress] 260/732 batches processed...\n",
            "  [Progress] 270/732 batches processed...\n",
            "  [Progress] 270/732 batches processed...\n",
            "  [Progress] 280/732 batches processed...\n",
            "  [Progress] 280/732 batches processed...\n",
            "  [Progress] 290/732 batches processed...\n",
            "  [Progress] 290/732 batches processed...\n",
            "  [Progress] 300/732 batches processed...\n",
            "  [Progress] 300/732 batches processed...\n",
            "  [Progress] 310/732 batches processed...\n",
            "  [Progress] 310/732 batches processed...\n",
            "  [Progress] 320/732 batches processed...\n",
            "  [Progress] 320/732 batches processed...\n",
            "  [Progress] 330/732 batches processed...\n",
            "  [Progress] 330/732 batches processed...\n",
            "  [Progress] 340/732 batches processed...\n",
            "  [Progress] 340/732 batches processed...\n",
            "  [Progress] 350/732 batches processed...\n",
            "  [Progress] 350/732 batches processed...\n",
            "  [Progress] 360/732 batches processed...\n",
            "  [Progress] 360/732 batches processed...\n",
            "  [Progress] 370/732 batches processed...\n",
            "  [Progress] 370/732 batches processed...\n",
            "  [Progress] 380/732 batches processed...\n",
            "  [Progress] 380/732 batches processed...\n",
            "  [Progress] 390/732 batches processed...\n",
            "  [Progress] 390/732 batches processed...\n",
            "  [Progress] 400/732 batches processed...\n",
            "  [Progress] 400/732 batches processed...\n",
            "  [Progress] 410/732 batches processed...\n",
            "  [Progress] 410/732 batches processed...\n",
            "  [Progress] 420/732 batches processed...\n",
            "  [Progress] 420/732 batches processed...\n",
            "  [Progress] 430/732 batches processed...\n",
            "  [Progress] 430/732 batches processed...\n",
            "  [Progress] 440/732 batches processed...\n",
            "  [Progress] 440/732 batches processed...\n",
            "  [Progress] 450/732 batches processed...\n",
            "  [Progress] 450/732 batches processed...\n",
            "  [Progress] 460/732 batches processed...\n",
            "  [Progress] 460/732 batches processed...\n",
            "  [Progress] 470/732 batches processed...\n",
            "  [Progress] 470/732 batches processed...\n",
            "  [Progress] 480/732 batches processed...\n",
            "  [Progress] 480/732 batches processed...\n",
            "  [Progress] 490/732 batches processed...\n",
            "  [Progress] 490/732 batches processed...\n",
            "  [Progress] 500/732 batches processed...\n",
            "  [Progress] 500/732 batches processed...\n",
            "  [Progress] 510/732 batches processed...\n",
            "  [Progress] 510/732 batches processed...\n",
            "  [Progress] 520/732 batches processed...\n",
            "  [Progress] 520/732 batches processed...\n",
            "  [Progress] 530/732 batches processed...\n",
            "  [Progress] 530/732 batches processed...\n",
            "  [Progress] 540/732 batches processed...\n",
            "  [Progress] 540/732 batches processed...\n",
            "  [Progress] 550/732 batches processed...\n",
            "  [Progress] 550/732 batches processed...\n",
            "  [Progress] 560/732 batches processed...\n",
            "  [Progress] 560/732 batches processed...\n",
            "  [Progress] 570/732 batches processed...\n",
            "  [Progress] 570/732 batches processed...\n",
            "  [Progress] 580/732 batches processed...\n",
            "  [Progress] 580/732 batches processed...\n",
            "  [Progress] 590/732 batches processed...\n",
            "  [Progress] 590/732 batches processed...\n",
            "  [Progress] 600/732 batches processed...\n",
            "  [Progress] 600/732 batches processed...\n",
            "  [Progress] 610/732 batches processed...\n",
            "  [Progress] 610/732 batches processed...\n",
            "  [Progress] 620/732 batches processed...\n",
            "  [Progress] 620/732 batches processed...\n",
            "  [Progress] 630/732 batches processed...\n",
            "  [Progress] 630/732 batches processed...\n",
            "  [Progress] 640/732 batches processed...\n",
            "  [Progress] 640/732 batches processed...\n",
            "  [Progress] 650/732 batches processed...\n",
            "  [Progress] 650/732 batches processed...\n",
            "  [Progress] 660/732 batches processed...\n",
            "  [Progress] 660/732 batches processed...\n",
            "  [Progress] 670/732 batches processed...\n",
            "  [Progress] 670/732 batches processed...\n",
            "  [Progress] 680/732 batches processed...\n",
            "  [Progress] 680/732 batches processed...\n",
            "  [Progress] 690/732 batches processed...\n",
            "  [Progress] 690/732 batches processed...\n",
            "  [Progress] 700/732 batches processed...\n",
            "  [Progress] 700/732 batches processed...\n",
            "  [Progress] 710/732 batches processed...\n",
            "  [Progress] 710/732 batches processed...\n",
            "  [Progress] 720/732 batches processed...\n",
            "  [Progress] 720/732 batches processed...\n",
            "  [Progress] 730/732 batches processed...\n",
            "  [Progress] 730/732 batches processed...\n",
            "[INFO]  Total features extracted: (5856, 2048)\n",
            "\n",
            "[STEP 1/2] Extracting testing features...\n",
            "[INFO] Extracting features from 35 images in 5 steps...\n",
            "[INFO]  Total features extracted: (5856, 2048)\n",
            "\n",
            "[STEP 1/2] Extracting testing features...\n",
            "[INFO] Extracting features from 35 images in 5 steps...\n",
            "[INFO]  Total features extracted: (35, 2048)\n",
            "[INFO]  Total features extracted: (35, 2048)\n",
            "[STEP 2/2]  Training features saved: 5856 samples\n",
            "[STEP 2/2]  Testing features saved: 35 samples\n",
            "[STEP 2/2]  Training features saved: 5856 samples\n",
            "[STEP 2/2]  Testing features saved: 35 samples\n",
            "\n",
            "[INFO]  Batch_12 completed successfully!\n",
            "\n",
            "Found 5925 images belonging to 2 classes.\n",
            "\n",
            "[INFO]  Batch_12 completed successfully!\n",
            "\n",
            "Found 5925 images belonging to 2 classes.\n",
            "Found 18 images belonging to 2 classes.\n",
            "Found 18 images belonging to 2 classes.\n",
            "\n",
            "============================================================\n",
            "[BATCH] Processing Batch_13\n",
            "============================================================\n",
            "\n",
            "[STEP 1/2] Extracting training features...\n",
            "[INFO] Extracting features from 5925 images in 741 steps...\n",
            "\n",
            "============================================================\n",
            "[BATCH] Processing Batch_13\n",
            "============================================================\n",
            "\n",
            "[STEP 1/2] Extracting training features...\n",
            "[INFO] Extracting features from 5925 images in 741 steps...\n",
            "  [Progress] 10/741 batches processed...\n",
            "  [Progress] 10/741 batches processed...\n",
            "  [Progress] 20/741 batches processed...\n",
            "  [Progress] 20/741 batches processed...\n",
            "  [Progress] 30/741 batches processed...\n",
            "  [Progress] 30/741 batches processed...\n",
            "  [Progress] 40/741 batches processed...\n",
            "  [Progress] 40/741 batches processed...\n",
            "  [Progress] 50/741 batches processed...\n",
            "  [Progress] 50/741 batches processed...\n",
            "  [Progress] 60/741 batches processed...\n",
            "  [Progress] 60/741 batches processed...\n",
            "  [Progress] 70/741 batches processed...\n",
            "  [Progress] 70/741 batches processed...\n",
            "  [Progress] 80/741 batches processed...\n",
            "  [Progress] 80/741 batches processed...\n",
            "  [Progress] 90/741 batches processed...\n",
            "  [Progress] 90/741 batches processed...\n",
            "  [Progress] 100/741 batches processed...\n",
            "  [Progress] 100/741 batches processed...\n",
            "  [Progress] 110/741 batches processed...\n",
            "  [Progress] 110/741 batches processed...\n",
            "  [Progress] 120/741 batches processed...\n",
            "  [Progress] 120/741 batches processed...\n",
            "  [Progress] 130/741 batches processed...\n",
            "  [Progress] 130/741 batches processed...\n",
            "  [Progress] 140/741 batches processed...\n",
            "  [Progress] 140/741 batches processed...\n",
            "  [Progress] 150/741 batches processed...\n",
            "  [Progress] 150/741 batches processed...\n",
            "  [Progress] 160/741 batches processed...\n",
            "  [Progress] 160/741 batches processed...\n",
            "  [Progress] 170/741 batches processed...\n",
            "  [Progress] 170/741 batches processed...\n",
            "  [Progress] 180/741 batches processed...\n",
            "  [Progress] 180/741 batches processed...\n",
            "  [Progress] 190/741 batches processed...\n",
            "  [Progress] 190/741 batches processed...\n",
            "  [Progress] 200/741 batches processed...\n",
            "  [Progress] 200/741 batches processed...\n",
            "  [Progress] 210/741 batches processed...\n",
            "  [Progress] 210/741 batches processed...\n",
            "  [Progress] 220/741 batches processed...\n",
            "  [Progress] 220/741 batches processed...\n",
            "  [Progress] 230/741 batches processed...\n",
            "  [Progress] 230/741 batches processed...\n",
            "  [Progress] 240/741 batches processed...\n",
            "  [Progress] 240/741 batches processed...\n",
            "  [Progress] 250/741 batches processed...\n",
            "  [Progress] 250/741 batches processed...\n",
            "  [Progress] 260/741 batches processed...\n",
            "  [Progress] 260/741 batches processed...\n",
            "  [Progress] 270/741 batches processed...\n",
            "  [Progress] 270/741 batches processed...\n",
            "  [Progress] 280/741 batches processed...\n",
            "  [Progress] 280/741 batches processed...\n",
            "  [Progress] 290/741 batches processed...\n",
            "  [Progress] 290/741 batches processed...\n",
            "  [Progress] 300/741 batches processed...\n",
            "  [Progress] 300/741 batches processed...\n",
            "  [Progress] 310/741 batches processed...\n",
            "  [Progress] 310/741 batches processed...\n",
            "  [Progress] 320/741 batches processed...\n",
            "  [Progress] 320/741 batches processed...\n",
            "  [Progress] 330/741 batches processed...\n",
            "  [Progress] 330/741 batches processed...\n",
            "  [Progress] 340/741 batches processed...\n",
            "  [Progress] 340/741 batches processed...\n",
            "  [Progress] 350/741 batches processed...\n",
            "  [Progress] 350/741 batches processed...\n",
            "  [Progress] 360/741 batches processed...\n",
            "  [Progress] 360/741 batches processed...\n",
            "  [Progress] 370/741 batches processed...\n",
            "  [Progress] 370/741 batches processed...\n",
            "  [Progress] 380/741 batches processed...\n",
            "  [Progress] 380/741 batches processed...\n",
            "  [Progress] 390/741 batches processed...\n",
            "  [Progress] 390/741 batches processed...\n",
            "  [Progress] 400/741 batches processed...\n",
            "  [Progress] 400/741 batches processed...\n",
            "  [Progress] 410/741 batches processed...\n",
            "  [Progress] 410/741 batches processed...\n",
            "  [Progress] 420/741 batches processed...\n",
            "  [Progress] 420/741 batches processed...\n",
            "  [Progress] 430/741 batches processed...\n",
            "  [Progress] 430/741 batches processed...\n",
            "  [Progress] 440/741 batches processed...\n",
            "  [Progress] 440/741 batches processed...\n",
            "  [Progress] 450/741 batches processed...\n",
            "  [Progress] 450/741 batches processed...\n",
            "  [Progress] 460/741 batches processed...\n",
            "  [Progress] 460/741 batches processed...\n",
            "  [Progress] 470/741 batches processed...\n",
            "  [Progress] 470/741 batches processed...\n",
            "  [Progress] 480/741 batches processed...\n",
            "  [Progress] 480/741 batches processed...\n",
            "  [Progress] 490/741 batches processed...\n",
            "  [Progress] 490/741 batches processed...\n",
            "  [Progress] 500/741 batches processed...\n",
            "  [Progress] 500/741 batches processed...\n",
            "  [Progress] 510/741 batches processed...\n",
            "  [Progress] 510/741 batches processed...\n",
            "  [Progress] 520/741 batches processed...\n",
            "  [Progress] 520/741 batches processed...\n",
            "  [Progress] 530/741 batches processed...\n",
            "  [Progress] 530/741 batches processed...\n",
            "  [Progress] 540/741 batches processed...\n",
            "  [Progress] 540/741 batches processed...\n",
            "  [Progress] 550/741 batches processed...\n",
            "  [Progress] 550/741 batches processed...\n",
            "  [Progress] 560/741 batches processed...\n",
            "  [Progress] 560/741 batches processed...\n",
            "  [Progress] 570/741 batches processed...\n",
            "  [Progress] 570/741 batches processed...\n",
            "  [Progress] 580/741 batches processed...\n",
            "  [Progress] 580/741 batches processed...\n",
            "  [Progress] 590/741 batches processed...\n",
            "  [Progress] 590/741 batches processed...\n",
            "  [Progress] 600/741 batches processed...\n",
            "  [Progress] 600/741 batches processed...\n",
            "  [Progress] 610/741 batches processed...\n",
            "  [Progress] 610/741 batches processed...\n",
            "  [Progress] 620/741 batches processed...\n",
            "  [Progress] 620/741 batches processed...\n",
            "  [Progress] 630/741 batches processed...\n",
            "  [Progress] 630/741 batches processed...\n",
            "  [Progress] 640/741 batches processed...\n",
            "  [Progress] 640/741 batches processed...\n",
            "  [Progress] 650/741 batches processed...\n",
            "  [Progress] 650/741 batches processed...\n",
            "  [Progress] 660/741 batches processed...\n",
            "  [Progress] 660/741 batches processed...\n",
            "  [Progress] 670/741 batches processed...\n",
            "  [Progress] 670/741 batches processed...\n",
            "  [Progress] 680/741 batches processed...\n",
            "  [Progress] 680/741 batches processed...\n",
            "  [Progress] 690/741 batches processed...\n",
            "  [Progress] 690/741 batches processed...\n",
            "  [Progress] 700/741 batches processed...\n",
            "  [Progress] 700/741 batches processed...\n",
            "  [Progress] 710/741 batches processed...\n",
            "  [Progress] 710/741 batches processed...\n",
            "  [Progress] 720/741 batches processed...\n",
            "  [Progress] 720/741 batches processed...\n",
            "  [Progress] 730/741 batches processed...\n",
            "  [Progress] 730/741 batches processed...\n",
            "  [Progress] 740/741 batches processed...\n",
            "  [Progress] 740/741 batches processed...\n",
            "[INFO]  Total features extracted: (5925, 2048)\n",
            "\n",
            "[STEP 1/2] Extracting testing features...\n",
            "[INFO] Extracting features from 18 images in 3 steps...\n",
            "[INFO]  Total features extracted: (5925, 2048)\n",
            "\n",
            "[STEP 1/2] Extracting testing features...\n",
            "[INFO] Extracting features from 18 images in 3 steps...\n",
            "[INFO]  Total features extracted: (18, 2048)\n",
            "[INFO]  Total features extracted: (18, 2048)\n",
            "[STEP 2/2]  Training features saved: 5925 samples\n",
            "[STEP 2/2]  Testing features saved: 18 samples\n",
            "[STEP 2/2]  Training features saved: 5925 samples\n",
            "[STEP 2/2]  Testing features saved: 18 samples\n",
            "\n",
            "[INFO]  Batch_13 completed successfully!\n",
            "\n",
            "Found 5931 images belonging to 2 classes.\n",
            "\n",
            "[INFO]  Batch_13 completed successfully!\n",
            "\n",
            "Found 5931 images belonging to 2 classes.\n",
            "Found 17 images belonging to 2 classes.\n",
            "Found 17 images belonging to 2 classes.\n",
            "\n",
            "============================================================\n",
            "[BATCH] Processing Batch_14\n",
            "============================================================\n",
            "\n",
            "[STEP 1/2] Extracting training features...\n",
            "[INFO] Extracting features from 5931 images in 742 steps...\n",
            "\n",
            "============================================================\n",
            "[BATCH] Processing Batch_14\n",
            "============================================================\n",
            "\n",
            "[STEP 1/2] Extracting training features...\n",
            "[INFO] Extracting features from 5931 images in 742 steps...\n",
            "  [Progress] 10/742 batches processed...\n",
            "  [Progress] 10/742 batches processed...\n",
            "  [Progress] 20/742 batches processed...\n",
            "  [Progress] 20/742 batches processed...\n",
            "  [Progress] 30/742 batches processed...\n",
            "  [Progress] 30/742 batches processed...\n",
            "  [Progress] 40/742 batches processed...\n",
            "  [Progress] 40/742 batches processed...\n",
            "  [Progress] 50/742 batches processed...\n",
            "  [Progress] 50/742 batches processed...\n",
            "  [Progress] 60/742 batches processed...\n",
            "  [Progress] 60/742 batches processed...\n",
            "  [Progress] 70/742 batches processed...\n",
            "  [Progress] 70/742 batches processed...\n",
            "  [Progress] 80/742 batches processed...\n",
            "  [Progress] 80/742 batches processed...\n",
            "  [Progress] 90/742 batches processed...\n",
            "  [Progress] 90/742 batches processed...\n",
            "  [Progress] 100/742 batches processed...\n",
            "  [Progress] 100/742 batches processed...\n",
            "  [Progress] 110/742 batches processed...\n",
            "  [Progress] 110/742 batches processed...\n",
            "  [Progress] 120/742 batches processed...\n",
            "  [Progress] 120/742 batches processed...\n",
            "  [Progress] 130/742 batches processed...\n",
            "  [Progress] 130/742 batches processed...\n",
            "  [Progress] 140/742 batches processed...\n",
            "  [Progress] 140/742 batches processed...\n",
            "  [Progress] 150/742 batches processed...\n",
            "  [Progress] 150/742 batches processed...\n",
            "  [Progress] 160/742 batches processed...\n",
            "  [Progress] 160/742 batches processed...\n",
            "  [Progress] 170/742 batches processed...\n",
            "  [Progress] 170/742 batches processed...\n",
            "  [Progress] 180/742 batches processed...\n",
            "  [Progress] 180/742 batches processed...\n",
            "  [Progress] 190/742 batches processed...\n",
            "  [Progress] 190/742 batches processed...\n",
            "  [Progress] 200/742 batches processed...\n",
            "  [Progress] 200/742 batches processed...\n",
            "  [Progress] 210/742 batches processed...\n",
            "  [Progress] 210/742 batches processed...\n",
            "  [Progress] 220/742 batches processed...\n",
            "  [Progress] 220/742 batches processed...\n",
            "  [Progress] 230/742 batches processed...\n",
            "  [Progress] 230/742 batches processed...\n",
            "  [Progress] 240/742 batches processed...\n",
            "  [Progress] 240/742 batches processed...\n",
            "  [Progress] 250/742 batches processed...\n",
            "  [Progress] 250/742 batches processed...\n",
            "  [Progress] 260/742 batches processed...\n",
            "  [Progress] 260/742 batches processed...\n",
            "  [Progress] 270/742 batches processed...\n",
            "  [Progress] 270/742 batches processed...\n",
            "  [Progress] 280/742 batches processed...\n",
            "  [Progress] 280/742 batches processed...\n",
            "  [Progress] 290/742 batches processed...\n",
            "  [Progress] 290/742 batches processed...\n",
            "  [Progress] 300/742 batches processed...\n",
            "  [Progress] 300/742 batches processed...\n",
            "  [Progress] 310/742 batches processed...\n",
            "  [Progress] 310/742 batches processed...\n",
            "  [Progress] 320/742 batches processed...\n",
            "  [Progress] 320/742 batches processed...\n",
            "  [Progress] 330/742 batches processed...\n",
            "  [Progress] 330/742 batches processed...\n",
            "  [Progress] 340/742 batches processed...\n",
            "  [Progress] 340/742 batches processed...\n",
            "  [Progress] 350/742 batches processed...\n",
            "  [Progress] 350/742 batches processed...\n",
            "  [Progress] 360/742 batches processed...\n",
            "  [Progress] 360/742 batches processed...\n",
            "  [Progress] 370/742 batches processed...\n",
            "  [Progress] 370/742 batches processed...\n",
            "  [Progress] 380/742 batches processed...\n",
            "  [Progress] 380/742 batches processed...\n",
            "  [Progress] 390/742 batches processed...\n",
            "  [Progress] 390/742 batches processed...\n",
            "  [Progress] 400/742 batches processed...\n",
            "  [Progress] 400/742 batches processed...\n",
            "  [Progress] 410/742 batches processed...\n",
            "  [Progress] 410/742 batches processed...\n",
            "  [Progress] 420/742 batches processed...\n",
            "  [Progress] 420/742 batches processed...\n",
            "  [Progress] 430/742 batches processed...\n",
            "  [Progress] 430/742 batches processed...\n",
            "  [Progress] 440/742 batches processed...\n",
            "  [Progress] 440/742 batches processed...\n",
            "  [Progress] 450/742 batches processed...\n",
            "  [Progress] 450/742 batches processed...\n",
            "  [Progress] 460/742 batches processed...\n",
            "  [Progress] 460/742 batches processed...\n",
            "  [Progress] 470/742 batches processed...\n",
            "  [Progress] 470/742 batches processed...\n",
            "  [Progress] 480/742 batches processed...\n",
            "  [Progress] 480/742 batches processed...\n",
            "  [Progress] 490/742 batches processed...\n",
            "  [Progress] 490/742 batches processed...\n",
            "  [Progress] 500/742 batches processed...\n",
            "  [Progress] 500/742 batches processed...\n",
            "  [Progress] 510/742 batches processed...\n",
            "  [Progress] 510/742 batches processed...\n",
            "  [Progress] 520/742 batches processed...\n",
            "  [Progress] 520/742 batches processed...\n",
            "  [Progress] 530/742 batches processed...\n",
            "  [Progress] 530/742 batches processed...\n",
            "  [Progress] 540/742 batches processed...\n",
            "  [Progress] 540/742 batches processed...\n",
            "  [Progress] 550/742 batches processed...\n",
            "  [Progress] 550/742 batches processed...\n",
            "  [Progress] 560/742 batches processed...\n",
            "  [Progress] 560/742 batches processed...\n",
            "  [Progress] 570/742 batches processed...\n",
            "  [Progress] 570/742 batches processed...\n",
            "  [Progress] 580/742 batches processed...\n",
            "  [Progress] 580/742 batches processed...\n",
            "  [Progress] 590/742 batches processed...\n",
            "  [Progress] 590/742 batches processed...\n",
            "  [Progress] 600/742 batches processed...\n",
            "  [Progress] 600/742 batches processed...\n",
            "  [Progress] 610/742 batches processed...\n",
            "  [Progress] 610/742 batches processed...\n",
            "  [Progress] 620/742 batches processed...\n",
            "  [Progress] 620/742 batches processed...\n",
            "  [Progress] 630/742 batches processed...\n",
            "  [Progress] 630/742 batches processed...\n",
            "  [Progress] 640/742 batches processed...\n",
            "  [Progress] 640/742 batches processed...\n",
            "  [Progress] 650/742 batches processed...\n",
            "  [Progress] 650/742 batches processed...\n",
            "  [Progress] 660/742 batches processed...\n",
            "  [Progress] 660/742 batches processed...\n",
            "  [Progress] 670/742 batches processed...\n",
            "  [Progress] 670/742 batches processed...\n",
            "  [Progress] 680/742 batches processed...\n",
            "  [Progress] 680/742 batches processed...\n",
            "  [Progress] 690/742 batches processed...\n",
            "  [Progress] 690/742 batches processed...\n",
            "  [Progress] 700/742 batches processed...\n",
            "  [Progress] 700/742 batches processed...\n",
            "  [Progress] 710/742 batches processed...\n",
            "  [Progress] 710/742 batches processed...\n",
            "  [Progress] 720/742 batches processed...\n",
            "  [Progress] 720/742 batches processed...\n",
            "  [Progress] 730/742 batches processed...\n",
            "  [Progress] 730/742 batches processed...\n",
            "  [Progress] 740/742 batches processed...\n",
            "  [Progress] 740/742 batches processed...\n",
            "[INFO]  Total features extracted: (5931, 2048)\n",
            "\n",
            "[STEP 1/2] Extracting testing features...\n",
            "[INFO] Extracting features from 17 images in 3 steps...\n",
            "[INFO]  Total features extracted: (5931, 2048)\n",
            "\n",
            "[STEP 1/2] Extracting testing features...\n",
            "[INFO] Extracting features from 17 images in 3 steps...\n",
            "[INFO]  Total features extracted: (17, 2048)\n",
            "[INFO]  Total features extracted: (17, 2048)\n",
            "[STEP 2/2]  Training features saved: 5931 samples\n",
            "[STEP 2/2]  Testing features saved: 17 samples\n",
            "[STEP 2/2]  Training features saved: 5931 samples\n",
            "[STEP 2/2]  Testing features saved: 17 samples\n",
            "\n",
            "[INFO]  Batch_14 completed successfully!\n",
            "\n",
            "Found 5265 images belonging to 2 classes.\n",
            "\n",
            "[INFO]  Batch_14 completed successfully!\n",
            "\n",
            "Found 5265 images belonging to 2 classes.\n",
            "Found 183 images belonging to 2 classes.\n",
            "Found 183 images belonging to 2 classes.\n",
            "\n",
            "============================================================\n",
            "[BATCH] Processing Batch_15\n",
            "============================================================\n",
            "\n",
            "[STEP 1/2] Extracting training features...\n",
            "[INFO] Extracting features from 5265 images in 659 steps...\n",
            "\n",
            "============================================================\n",
            "[BATCH] Processing Batch_15\n",
            "============================================================\n",
            "\n",
            "[STEP 1/2] Extracting training features...\n",
            "[INFO] Extracting features from 5265 images in 659 steps...\n",
            "  [Progress] 10/659 batches processed...\n",
            "  [Progress] 10/659 batches processed...\n",
            "  [Progress] 20/659 batches processed...\n",
            "  [Progress] 20/659 batches processed...\n",
            "  [Progress] 30/659 batches processed...\n",
            "  [Progress] 30/659 batches processed...\n",
            "  [Progress] 40/659 batches processed...\n",
            "  [Progress] 40/659 batches processed...\n",
            "  [Progress] 50/659 batches processed...\n",
            "  [Progress] 50/659 batches processed...\n",
            "  [Progress] 60/659 batches processed...\n",
            "  [Progress] 60/659 batches processed...\n",
            "  [Progress] 70/659 batches processed...\n",
            "  [Progress] 70/659 batches processed...\n",
            "  [Progress] 80/659 batches processed...\n",
            "  [Progress] 80/659 batches processed...\n",
            "  [Progress] 90/659 batches processed...\n",
            "  [Progress] 90/659 batches processed...\n",
            "  [Progress] 100/659 batches processed...\n",
            "  [Progress] 100/659 batches processed...\n",
            "  [Progress] 110/659 batches processed...\n",
            "  [Progress] 110/659 batches processed...\n",
            "  [Progress] 120/659 batches processed...\n",
            "  [Progress] 120/659 batches processed...\n",
            "  [Progress] 130/659 batches processed...\n",
            "  [Progress] 130/659 batches processed...\n",
            "  [Progress] 140/659 batches processed...\n",
            "  [Progress] 140/659 batches processed...\n",
            "  [Progress] 150/659 batches processed...\n",
            "  [Progress] 150/659 batches processed...\n",
            "  [Progress] 160/659 batches processed...\n",
            "  [Progress] 160/659 batches processed...\n",
            "  [Progress] 170/659 batches processed...\n",
            "  [Progress] 170/659 batches processed...\n",
            "  [Progress] 180/659 batches processed...\n",
            "  [Progress] 180/659 batches processed...\n",
            "  [Progress] 190/659 batches processed...\n",
            "  [Progress] 190/659 batches processed...\n",
            "  [Progress] 200/659 batches processed...\n",
            "  [Progress] 200/659 batches processed...\n",
            "  [Progress] 210/659 batches processed...\n",
            "  [Progress] 210/659 batches processed...\n",
            "  [Progress] 220/659 batches processed...\n",
            "  [Progress] 220/659 batches processed...\n",
            "  [Progress] 230/659 batches processed...\n",
            "  [Progress] 230/659 batches processed...\n",
            "  [Progress] 240/659 batches processed...\n",
            "  [Progress] 240/659 batches processed...\n",
            "  [Progress] 250/659 batches processed...\n",
            "  [Progress] 250/659 batches processed...\n",
            "  [Progress] 260/659 batches processed...\n",
            "  [Progress] 260/659 batches processed...\n",
            "  [Progress] 270/659 batches processed...\n",
            "  [Progress] 270/659 batches processed...\n",
            "  [Progress] 280/659 batches processed...\n",
            "  [Progress] 280/659 batches processed...\n",
            "  [Progress] 290/659 batches processed...\n",
            "  [Progress] 290/659 batches processed...\n",
            "  [Progress] 300/659 batches processed...\n",
            "  [Progress] 300/659 batches processed...\n",
            "  [Progress] 310/659 batches processed...\n",
            "  [Progress] 310/659 batches processed...\n",
            "  [Progress] 320/659 batches processed...\n",
            "  [Progress] 320/659 batches processed...\n",
            "  [Progress] 330/659 batches processed...\n",
            "  [Progress] 330/659 batches processed...\n",
            "  [Progress] 340/659 batches processed...\n",
            "  [Progress] 340/659 batches processed...\n",
            "  [Progress] 350/659 batches processed...\n",
            "  [Progress] 350/659 batches processed...\n",
            "  [Progress] 360/659 batches processed...\n",
            "  [Progress] 360/659 batches processed...\n",
            "  [Progress] 370/659 batches processed...\n",
            "  [Progress] 370/659 batches processed...\n",
            "  [Progress] 380/659 batches processed...\n",
            "  [Progress] 380/659 batches processed...\n",
            "  [Progress] 390/659 batches processed...\n",
            "  [Progress] 390/659 batches processed...\n",
            "  [Progress] 400/659 batches processed...\n",
            "  [Progress] 400/659 batches processed...\n",
            "  [Progress] 410/659 batches processed...\n",
            "  [Progress] 410/659 batches processed...\n",
            "  [Progress] 420/659 batches processed...\n",
            "  [Progress] 420/659 batches processed...\n",
            "  [Progress] 430/659 batches processed...\n",
            "  [Progress] 430/659 batches processed...\n",
            "  [Progress] 440/659 batches processed...\n",
            "  [Progress] 440/659 batches processed...\n",
            "  [Progress] 450/659 batches processed...\n",
            "  [Progress] 450/659 batches processed...\n",
            "  [Progress] 460/659 batches processed...\n",
            "  [Progress] 460/659 batches processed...\n",
            "  [Progress] 470/659 batches processed...\n",
            "  [Progress] 470/659 batches processed...\n",
            "  [Progress] 480/659 batches processed...\n",
            "  [Progress] 480/659 batches processed...\n",
            "  [Progress] 490/659 batches processed...\n",
            "  [Progress] 490/659 batches processed...\n",
            "  [Progress] 500/659 batches processed...\n",
            "  [Progress] 500/659 batches processed...\n",
            "  [Progress] 510/659 batches processed...\n",
            "  [Progress] 510/659 batches processed...\n",
            "  [Progress] 520/659 batches processed...\n",
            "  [Progress] 520/659 batches processed...\n",
            "  [Progress] 530/659 batches processed...\n",
            "  [Progress] 530/659 batches processed...\n",
            "  [Progress] 540/659 batches processed...\n",
            "  [Progress] 540/659 batches processed...\n",
            "  [Progress] 550/659 batches processed...\n",
            "  [Progress] 550/659 batches processed...\n",
            "  [Progress] 560/659 batches processed...\n",
            "  [Progress] 560/659 batches processed...\n",
            "  [Progress] 570/659 batches processed...\n",
            "  [Progress] 570/659 batches processed...\n",
            "  [Progress] 580/659 batches processed...\n",
            "  [Progress] 580/659 batches processed...\n",
            "  [Progress] 590/659 batches processed...\n",
            "  [Progress] 590/659 batches processed...\n",
            "  [Progress] 600/659 batches processed...\n",
            "  [Progress] 600/659 batches processed...\n",
            "  [Progress] 610/659 batches processed...\n",
            "  [Progress] 610/659 batches processed...\n",
            "  [Progress] 620/659 batches processed...\n",
            "  [Progress] 620/659 batches processed...\n",
            "  [Progress] 630/659 batches processed...\n",
            "  [Progress] 630/659 batches processed...\n",
            "  [Progress] 640/659 batches processed...\n",
            "  [Progress] 640/659 batches processed...\n",
            "  [Progress] 650/659 batches processed...\n",
            "  [Progress] 650/659 batches processed...\n",
            "[INFO]  Total features extracted: (5265, 2048)\n",
            "\n",
            "[STEP 1/2] Extracting testing features...\n",
            "[INFO] Extracting features from 183 images in 23 steps...\n",
            "[INFO]  Total features extracted: (5265, 2048)\n",
            "\n",
            "[STEP 1/2] Extracting testing features...\n",
            "[INFO] Extracting features from 183 images in 23 steps...\n",
            "  [Progress] 10/23 batches processed...\n",
            "  [Progress] 10/23 batches processed...\n",
            "  [Progress] 20/23 batches processed...\n",
            "  [Progress] 20/23 batches processed...\n",
            "[INFO]  Total features extracted: (183, 2048)\n",
            "[INFO]  Total features extracted: (183, 2048)\n",
            "[STEP 2/2]  Training features saved: 5265 samples\n",
            "[STEP 2/2]  Testing features saved: 183 samples\n",
            "\n",
            "[INFO]  Batch_15 completed successfully!\n",
            "\n",
            "\n",
            "============================================================\n",
            " ALL BATCHES PROCESSED SUCCESSFULLY!\n",
            "============================================================\n",
            "[STEP 2/2]  Training features saved: 5265 samples\n",
            "[STEP 2/2]  Testing features saved: 183 samples\n",
            "\n",
            "[INFO]  Batch_15 completed successfully!\n",
            "\n",
            "\n",
            "============================================================\n",
            " ALL BATCHES PROCESSED SUCCESSFULLY!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import Xception\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "import gc\n",
        "\n",
        "# =========================================\n",
        "#  GPU MEMORY MANAGEMENT\n",
        "# =========================================\n",
        "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "# for gpu in gpus:\n",
        "#     tf.config.experimental.set_memory_growth(gpu, True)\n",
        "\n",
        "# Use mixed precision (reduces memory by ~40%)\n",
        "from tensorflow.keras import mixed_precision\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "# =========================================\n",
        "#  GLOBAL MODEL CACHE - REUSE ACROSS BATCHES\n",
        "# =========================================\n",
        "_cached_model = None\n",
        "\n",
        "def get_feature_extraction_model():\n",
        "    \"\"\"Get cached or create new model for feature extraction\"\"\"\n",
        "    global _cached_model\n",
        "    if _cached_model is None:\n",
        "        base_model = Xception(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
        "        output_pooled = GlobalAveragePooling2D()(base_model.output)\n",
        "        _cached_model = Model(inputs=base_model.input, outputs=output_pooled)\n",
        "        \n",
        "        # Freeze all layers\n",
        "        for layer in base_model.layers:\n",
        "            layer.trainable = False\n",
        "        print(\"[INFO]  Feature extraction model loaded (cached)\")\n",
        "    return _cached_model\n",
        "\n",
        "# =========================================\n",
        "#  OPTIMIZED FEATURE EXTRACTION - FULL DATA SUPPORT\n",
        "# =========================================\n",
        "def extract_features_in_chunks(generator, model, chunk_size=32):\n",
        "    \"\"\"\n",
        "    Extract features in chunks to manage memory efficiently.\n",
        "    Handles full dataset regardless of size.\n",
        "    \"\"\"\n",
        "    all_features = []\n",
        "    all_labels = []\n",
        "    all_filenames = []\n",
        "    \n",
        "    # Calculate total steps needed\n",
        "    total_samples = len(generator.filenames)\n",
        "    steps = int(np.ceil(total_samples / generator.batch_size))\n",
        "    \n",
        "    print(f\"[INFO] Extracting features from {total_samples} images in {steps} steps...\")\n",
        "    \n",
        "    for step in range(steps):\n",
        "        try:\n",
        "            batch_x, batch_y = next(generator)\n",
        "            chunk_features = model.predict(batch_x, verbose=0)\n",
        "            all_features.append(chunk_features)\n",
        "            all_labels.extend(batch_y if isinstance(batch_y, (list, np.ndarray)) and len(batch_y.shape) == 1 \n",
        "                            else np.argmax(batch_y, axis=1))\n",
        "            all_filenames.extend(generator.filenames[step*generator.batch_size:(step+1)*generator.batch_size])\n",
        "            \n",
        "            if (step + 1) % 10 == 0:\n",
        "                print(f\"  [Progress] {step + 1}/{steps} batches processed...\")\n",
        "                gc.collect()\n",
        "        except StopIteration:\n",
        "            break\n",
        "    \n",
        "    # Concatenate all features\n",
        "    features_array = np.vstack(all_features)\n",
        "    print(f\"[INFO]  Total features extracted: {features_array.shape}\")\n",
        "    \n",
        "    return features_array, np.array(all_labels), all_filenames[:len(all_labels)]\n",
        "\n",
        "# =========================================\n",
        "#  MAIN OPTIMIZED FUNCTION - FEATURE EXTRACTION ONLY\n",
        "# =========================================\n",
        "def XceptionNet2048_Features_Optimized(train_generator, test_generator, Batch):\n",
        "    \"\"\"\n",
        "    Optimized feature extraction (100% data support).\n",
        "    - Supports 100% data processing (not just 40%)\n",
        "    - Memory-efficient chunked processing\n",
        "    - Cached model across batches\n",
        "    - Saves features to CSV\n",
        "    \"\"\"\n",
        "    \n",
        "    # Setup paths\n",
        "    current_dir = os.path.dirname(os.path.abspath('__file__'))\n",
        "    parent_dir = os.path.dirname(current_dir)\n",
        "    root_dir = os.path.dirname(parent_dir)\n",
        "    save_drive_dir = os.path.join(root_dir, 'pathologyStudentsAug25', 'Results', 'Segmented_unett_XceptionNet2048+SVM')\n",
        "    batch_output_dir = os.path.join(save_drive_dir, Batch)\n",
        "    os.makedirs(batch_output_dir, exist_ok=True)\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"[BATCH] Processing {Batch}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Get cached model\n",
        "    model = get_feature_extraction_model()\n",
        "    \n",
        "    #  FEATURE EXTRACTION - FULL DATA (100%)\n",
        "    print(f\"\\n[STEP 1/2] Extracting training features...\")\n",
        "    features_train, labels_train, filenames_train = extract_features_in_chunks(\n",
        "        train_generator, model, chunk_size=32\n",
        "    )\n",
        "    \n",
        "    print(f\"\\n[STEP 1/2] Extracting testing features...\")\n",
        "    features_test, labels_test, filenames_test = extract_features_in_chunks(\n",
        "        test_generator, model, chunk_size=32\n",
        "    )\n",
        "    \n",
        "    #  SAVE RAW FEATURES\n",
        "    columns_2048 = [f'feature_{c}' for c in range(features_train.shape[1])]\n",
        "    \n",
        "    df_train = pd.DataFrame(features_train, columns=columns_2048)\n",
        "    df_train['label'] = labels_train\n",
        "    df_train['Image_Name'] = filenames_train\n",
        "    df_train.to_csv(os.path.join(batch_output_dir, 'Segmented_XceptionNet2048_Training.csv'), index=False)\n",
        "    print(f\"[STEP 2/2]  Training features saved: {df_train.shape[0]} samples\")\n",
        "    \n",
        "    df_test = pd.DataFrame(features_test, columns=columns_2048)\n",
        "    df_test['label'] = labels_test\n",
        "    df_test['Image_Name'] = filenames_test\n",
        "    df_test.to_csv(os.path.join(batch_output_dir, 'Segmented_XceptionNet2048_Testing.csv'), index=False)\n",
        "    print(f\"[STEP 2/2]  Testing features saved: {df_test.shape[0]} samples\")\n",
        "    \n",
        "    # Memory cleanup\n",
        "    del features_train, features_test, df_train, df_test\n",
        "    gc.collect()\n",
        "    tf.keras.backend.clear_session()\n",
        "    \n",
        "    print(f\"\\n[INFO]  {Batch} completed successfully!\\n\")\n",
        "\n",
        "# =========================================\n",
        "#  LOOP THROUGH BATCHES - PROCESS ALL DATA\n",
        "# =========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STARTING OPTIMIZED BATCH PROCESSING - FULL DATA (100%)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for x in range(1, 16):\n",
        "    batch_size = 8  #  Optimized batch size\n",
        "    Batch_Name = f'Batch_{x}'\n",
        "    \n",
        "    current_dir = os.path.dirname(os.path.abspath('__file__'))\n",
        "    parent_dir = os.path.dirname(current_dir)\n",
        "    root_dir = os.path.dirname(parent_dir)\n",
        "    \n",
        "    save_dir = os.path.join(root_dir, 'pathologyStudentsAug25', 'Results', 'Segmented_unett_XceptionNet2048+SVM')\n",
        "    os.makedirs(os.path.join(save_dir, Batch_Name), exist_ok=True)\n",
        "    \n",
        "    Training_Path = os.path.join(root_dir, 'pathologyStudentsAug25','Segmented_Batch' , Batch_Name, 'Training')\n",
        "    Testing_Path = os.path.join(root_dir, 'pathologyStudentsAug25','Segmented_Batch' , Batch_Name, 'Testing')\n",
        "    \n",
        "    if not os.path.exists(Training_Path):\n",
        "        print(f\"[WARNING] Skipping {Batch_Name} - path not found: {Training_Path}\")\n",
        "        continue\n",
        "    \n",
        "    # Data generators with NO shuffle (ensures proper ordering)\n",
        "    train_datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "    test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "    \n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        Training_Path,\n",
        "        target_size=(256, 256),\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        classes=['Non_Necrosis', 'Necrosis'],\n",
        "        seed=42\n",
        "    )\n",
        "    \n",
        "    test_generator = test_datagen.flow_from_directory(\n",
        "        Testing_Path,\n",
        "        target_size=(256, 256),\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        classes=['Non_Necrosis', 'Necrosis'],\n",
        "        seed=42\n",
        "    )\n",
        "    \n",
        "    # Process batch with optimized function\n",
        "    XceptionNet2048_Features_Optimized(\n",
        "        train_generator, test_generator, Batch_Name\n",
        "    )\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" ALL BATCHES PROCESSED SUCCESSFULLY!\")\n",
        "print(\"=\"*60)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch Name: Batch_1\n",
            "Training Path: /home/llmPathoUser/pathologyStudentsAug25/pathologyStudentsAug25/Segmented_Batch/Batch_1/Training\n",
            "Testing Path: /home/llmPathoUser/pathologyStudentsAug25/pathologyStudentsAug25/Segmented_Batch/Batch_1/Testing\n",
            "Found 3169 images belonging to 2 classes.\n",
            "Found 33 images belonging to 2 classes.\n",
            "\n",
            "[INFO] Extracting features for Batch_1...\n",
            "\u001b[1m793/793\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 26ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step\n",
            "[INFO]  Completed feature extraction for Batch_1\n",
            "\n",
            "Batch Name: Batch_2\n",
            "Training Path: /home/llmPathoUser/pathologyStudentsAug25/pathologyStudentsAug25/Segmented_Batch/Batch_2/Training\n",
            "Testing Path: /home/llmPathoUser/pathologyStudentsAug25/pathologyStudentsAug25/Segmented_Batch/Batch_2/Testing\n",
            "Found 2868 images belonging to 2 classes.\n",
            "Found 183 images belonging to 2 classes.\n",
            "\n",
            "[INFO] Extracting features for Batch_2...\n",
            "\u001b[1m717/717\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 9ms/step\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 262ms/step\n",
            "[INFO]  Completed feature extraction for Batch_2\n",
            "\n",
            "Batch Name: Batch_3\n",
            "Training Path: /home/llmPathoUser/pathologyStudentsAug25/pathologyStudentsAug25/Segmented_Batch/Batch_3/Training\n",
            "Testing Path: /home/llmPathoUser/pathologyStudentsAug25/pathologyStudentsAug25/Segmented_Batch/Batch_3/Testing\n",
            "Found 3204 images belonging to 2 classes.\n",
            "Found 15 images belonging to 2 classes.\n",
            "\n",
            "[INFO] Extracting features for Batch_3...\n",
            "\u001b[1m801/801\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 9ms/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 4s/step\n",
            "[INFO]  Completed feature extraction for Batch_3\n",
            "\n",
            "Batch Name: Batch_4\n",
            "Training Path: /home/llmPathoUser/pathologyStudentsAug25/pathologyStudentsAug25/Segmented_Batch/Batch_4/Training\n",
            "Testing Path: /home/llmPathoUser/pathologyStudentsAug25/pathologyStudentsAug25/Segmented_Batch/Batch_4/Testing\n",
            "Found 3192 images belonging to 2 classes.\n",
            "Found 564 images belonging to 2 classes.\n",
            "\n",
            "[INFO] Extracting features for Batch_4...\n",
            "\u001b[1m798/798\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 9ms/step\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step\n",
            "[INFO]  Completed feature extraction for Batch_4\n",
            "\n",
            "Batch Name: Batch_5\n",
            "Training Path: /home/llmPathoUser/pathologyStudentsAug25/pathologyStudentsAug25/Segmented_Batch/Batch_5/Training\n",
            "Testing Path: /home/llmPathoUser/pathologyStudentsAug25/pathologyStudentsAug25/Segmented_Batch/Batch_5/Testing\n",
            "Found 3235 images belonging to 2 classes.\n",
            "Found 1215 images belonging to 2 classes.\n",
            "\n",
            "[INFO] Extracting features for Batch_5...\n",
            "\u001b[1m809/809\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 25ms/step\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 46ms/step\n",
            "[INFO]  Completed feature extraction for Batch_5\n",
            "\n",
            "Batch Name: Batch_6\n",
            "Training Path: /home/llmPathoUser/pathologyStudentsAug25/pathologyStudentsAug25/Segmented_Batch/Batch_6/Training\n",
            "Testing Path: /home/llmPathoUser/pathologyStudentsAug25/pathologyStudentsAug25/Segmented_Batch/Batch_6/Testing\n",
            "Found 3175 images belonging to 2 classes.\n",
            "Found 149 images belonging to 2 classes.\n",
            "\n",
            "[INFO] Extracting features for Batch_6...\n",
            "\u001b[1m794/794\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 25ms/step\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 289ms/step\n",
            "[INFO]  Completed feature extraction for Batch_6\n",
            "\n",
            "Batch Name: Batch_7\n",
            "Training Path: /home/llmPathoUser/pathologyStudentsAug25/pathologyStudentsAug25/Segmented_Batch/Batch_7/Training\n",
            "Testing Path: /home/llmPathoUser/pathologyStudentsAug25/pathologyStudentsAug25/Segmented_Batch/Batch_7/Testing\n",
            "Found 3116 images belonging to 2 classes.\n",
            "Found 59 images belonging to 2 classes.\n",
            "\n",
            "[INFO] Extracting features for Batch_7...\n",
            "\u001b[1m779/779\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 10ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 849ms/step\n",
            "[INFO]  Completed feature extraction for Batch_7\n",
            "\n",
            "Batch Name: Batch_8\n",
            "Training Path: /home/llmPathoUser/pathologyStudentsAug25/pathologyStudentsAug25/Segmented_Batch/Batch_8/Training\n",
            "Testing Path: /home/llmPathoUser/pathologyStudentsAug25/pathologyStudentsAug25/Segmented_Batch/Batch_8/Testing\n",
            "Found 3154 images belonging to 2 classes.\n",
            "Found 40 images belonging to 2 classes.\n",
            "\n",
            "[INFO] Extracting features for Batch_8...\n",
            "\u001b[1m786/789\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-11 15:12:22.590731: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-11-11 15:12:22.710833: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-11-11 15:12:24.252662: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-11-11 15:12:24.378277: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-11-11 15:12:24.499657: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-11-11 15:12:24.617209: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-11-11 15:12:24.737037: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-11-11 15:12:25.414120: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-11-11 15:12:25.527554: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-11-11 15:12:26.252925: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-11-11 15:12:26.368599: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-11-11 15:12:27.044235: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-11-11 15:12:27.169899: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m789/789\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 34ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 10ms/step\n",
            "[INFO]  Completed feature extraction for Batch_8\n",
            "\n",
            "Batch Name: Batch_9\n",
            "Training Path: /home/llmPathoUser/pathologyStudentsAug25/pathologyStudentsAug25/Segmented_Batch/Batch_9/Training\n",
            "Testing Path: /home/llmPathoUser/pathologyStudentsAug25/pathologyStudentsAug25/Segmented_Batch/Batch_9/Testing\n",
            "Found 1831 images belonging to 2 classes.\n",
            "Found 702 images belonging to 2 classes.\n",
            "\n",
            "[INFO] Extracting features for Batch_9...\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step\n",
            "\u001b[1m176/176\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 68ms/step\n",
            "[INFO]  Completed feature extraction for Batch_9\n",
            "\n",
            "Batch Name: Batch_10\n",
            "Training Path: /home/llmPathoUser/pathologyStudentsAug25/pathologyStudentsAug25/Segmented_Batch/Batch_10/Training\n",
            "Testing Path: /home/llmPathoUser/pathologyStudentsAug25/pathologyStudentsAug25/Segmented_Batch/Batch_10/Testing\n",
            "Found 2969 images belonging to 2 classes.\n",
            "Found 133 images belonging to 2 classes.\n",
            "\n",
            "[INFO] Extracting features for Batch_10...\n",
            "\u001b[1m743/743\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 25ms/step\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 329ms/step\n",
            "[INFO]  Completed feature extraction for Batch_10\n",
            "\n",
            "Batch Name: Batch_11\n",
            "Training Path: /home/llmPathoUser/pathologyStudentsAug25/pathologyStudentsAug25/Segmented_Batch/Batch_11/Training\n",
            "Testing Path: /home/llmPathoUser/pathologyStudentsAug25/pathologyStudentsAug25/Segmented_Batch/Batch_11/Testing\n",
            "Found 2142 images belonging to 2 classes.\n",
            "Found 546 images belonging to 2 classes.\n",
            "\n",
            "[INFO] Extracting features for Batch_11...\n",
            "\u001b[1m536/536\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 31ms/step\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 92ms/step\n",
            "[INFO]  Completed feature extraction for Batch_11\n",
            "\n",
            "Batch Name: Batch_12\n",
            "Training Path: /home/llmPathoUser/pathologyStudentsAug25/pathologyStudentsAug25/Segmented_Batch/Batch_12/Training\n",
            "Testing Path: /home/llmPathoUser/pathologyStudentsAug25/pathologyStudentsAug25/Segmented_Batch/Batch_12/Testing\n",
            "Found 2926 images belonging to 2 classes.\n",
            "Found 35 images belonging to 2 classes.\n",
            "\n",
            "[INFO] Extracting features for Batch_12...\n",
            "\u001b[1m732/732\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 26ms/step\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step\n",
            "[INFO]  Completed feature extraction for Batch_12\n",
            "\n",
            "Batch Name: Batch_13\n",
            "Training Path: /home/llmPathoUser/pathologyStudentsAug25/pathologyStudentsAug25/Segmented_Batch/Batch_13/Training\n",
            "Testing Path: /home/llmPathoUser/pathologyStudentsAug25/pathologyStudentsAug25/Segmented_Batch/Batch_13/Testing\n",
            "Found 2960 images belonging to 2 classes.\n",
            "Found 18 images belonging to 2 classes.\n",
            "\n",
            "[INFO] Extracting features for Batch_13...\n",
            "\u001b[1m740/740\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2s/step\n",
            "[INFO]  Completed feature extraction for Batch_13\n",
            "\n",
            "Batch Name: Batch_14\n",
            "Training Path: /home/llmPathoUser/pathologyStudentsAug25/pathologyStudentsAug25/Segmented_Batch/Batch_14/Training\n",
            "Testing Path: /home/llmPathoUser/pathologyStudentsAug25/pathologyStudentsAug25/Segmented_Batch/Batch_14/Testing\n",
            "Found 2963 images belonging to 2 classes.\n",
            "Found 17 images belonging to 2 classes.\n",
            "\n",
            "[INFO] Extracting features for Batch_14...\n",
            "\u001b[1m741/741\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 26ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3s/step\n",
            "[INFO]  Completed feature extraction for Batch_14\n",
            "\n",
            "Batch Name: Batch_15\n",
            "Training Path: /home/llmPathoUser/pathologyStudentsAug25/pathologyStudentsAug25/Segmented_Batch/Batch_15/Training\n",
            "Testing Path: /home/llmPathoUser/pathologyStudentsAug25/pathologyStudentsAug25/Segmented_Batch/Batch_15/Testing\n",
            "Found 2630 images belonging to 2 classes.\n",
            "Found 183 images belonging to 2 classes.\n",
            "\n",
            "[INFO] Extracting features for Batch_15...\n",
            "WARNING:tensorflow:5 out of the last 747 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x704af5d76fc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "\u001b[1m658/658\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 26ms/step\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 258ms/step\n",
            "[INFO]  Completed feature extraction for Batch_15\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import Xception\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "\n",
        "# =========================================\n",
        "#  GPU MEMORY MANAGEMENT\n",
        "# =========================================\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "for gpu in gpus:\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)\n",
        "\n",
        "# Use mixed precision (reduces memory by ~40%)\n",
        "from tensorflow.keras import mixed_precision\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "# =========================================\n",
        "#  DEFINE THE FEATURE EXTRACTION FUNCTION\n",
        "# =========================================\n",
        "def XceptionNet2048_SVM(train_generator, test_generator, Batch):\n",
        "    # Base paths\n",
        "    current_dir = os.path.dirname(os.path.abspath('__file__'))\n",
        "    parent_dir = os.path.dirname(current_dir)\n",
        "    root_dir = os.path.dirname(parent_dir)\n",
        "    save_drive_dir = os.path.join(root_dir, 'pathologyStudentsAug25', 'Results', 'Segmented_XceptionNet2048+SVM')\n",
        "\n",
        "    os.makedirs(os.path.join(save_drive_dir, Batch), exist_ok=True)\n",
        "\n",
        "    # =========================================\n",
        "    #  LOAD BASE XCEPTION MODEL\n",
        "    # =========================================\n",
        "    base_model_Xception = Xception(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
        "    output_4 = GlobalAveragePooling2D()(base_model_Xception.output)\n",
        "    model_Xception_features_2048 = Model(inputs=base_model_Xception.input, outputs=output_4)\n",
        "\n",
        "    # Freeze all layers for feature extraction\n",
        "    for layer in base_model_Xception.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # =========================================\n",
        "    #  FEATURE EXTRACTION\n",
        "    # =========================================\n",
        "    columns_2048 = [f'feature_{c}' for c in range(2048)]\n",
        "\n",
        "    print(f\"\\n[INFO] Extracting features for {Batch}...\")\n",
        "\n",
        "    # Smaller steps_per_epoch for generators (avoids memory spikes)\n",
        "    features_2048_train = model_Xception_features_2048.predict(\n",
        "        train_generator,\n",
        "        verbose=1,\n",
        "        steps=len(train_generator)\n",
        "    )\n",
        "\n",
        "    df_2048_train = pd.DataFrame(features_2048_train, columns=columns_2048)\n",
        "    df_2048_train['label'] = train_generator.labels\n",
        "    df_2048_train['Image_Name'] = train_generator.filenames\n",
        "    df_2048_train.to_csv(os.path.join(save_drive_dir, Batch, 'Segmented_XceptionNet2048_Training.csv'), index=False)\n",
        "    del features_2048_train\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "    # Recreate model to ensure GPU memory cleared\n",
        "    base_model_Xception = Xception(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
        "    output_4 = GlobalAveragePooling2D()(base_model_Xception.output)\n",
        "    model_Xception_features_2048 = Model(inputs=base_model_Xception.input, outputs=output_4)\n",
        "    for layer in base_model_Xception.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    features_2048_test = model_Xception_features_2048.predict(\n",
        "        test_generator,\n",
        "        verbose=1,\n",
        "        steps=len(test_generator)\n",
        "    )\n",
        "\n",
        "    df_2048_test = pd.DataFrame(features_2048_test, columns=columns_2048)\n",
        "    df_2048_test['label'] = test_generator.labels\n",
        "    df_2048_test['Image_Name'] = test_generator.filenames\n",
        "    df_2048_test.to_csv(os.path.join(save_drive_dir, Batch, 'Segmented_XceptionNet2048_Testing.csv'), index=False)\n",
        "\n",
        "    # Final cleanup\n",
        "    del model_Xception_features_2048, base_model_Xception, features_2048_test\n",
        "    tf.keras.backend.clear_session()\n",
        "    print(f\"[INFO]  Completed feature extraction for {Batch}\\n\")\n",
        "\n",
        "\n",
        "# =========================================\n",
        "#  LOOP THROUGH BATCHES\n",
        "# =========================================\n",
        "for x in range(1, 16):\n",
        "    batch_size = 4  #  smaller batch size for GPU safety\n",
        "    Batch_Name = f'Batch_{x}'\n",
        "    print('Batch Name:', Batch_Name)\n",
        "\n",
        "    current_dir = os.path.dirname(os.path.abspath('__file__'))\n",
        "    parent_dir = os.path.dirname(current_dir)\n",
        "    root_dir = os.path.dirname(parent_dir)\n",
        "\n",
        "    save_dir = os.path.join(root_dir, 'pathologyStudentsAug25', 'Results', 'Segmented_XceptionNet2048+SVM')\n",
        "    os.makedirs(os.path.join(save_dir, Batch_Name), exist_ok=True)\n",
        "\n",
        "    Training_Path = os.path.join(root_dir, 'pathologyStudentsAug25', 'Segmented_Batch', Batch_Name, 'Training')\n",
        "    Testing_Path = os.path.join(root_dir, 'pathologyStudentsAug25', 'Segmented_Batch', Batch_Name, 'Testing')\n",
        "\n",
        "    print('Training Path:', Training_Path)\n",
        "    print('Testing Path:', Testing_Path)\n",
        "\n",
        "    # Use smaller input size for lighter memory load\n",
        "    train_datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "    test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        Training_Path,\n",
        "        target_size=(256, 256),  #  reduced from 256x256\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        classes=['Non_Necrosis', 'Necrosis'],\n",
        "        seed=42\n",
        "    )\n",
        "\n",
        "    test_generator = test_datagen.flow_from_directory(\n",
        "        Testing_Path,\n",
        "        target_size=(256, 256),\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        classes=['Non_Necrosis', 'Necrosis'],\n",
        "        seed=42\n",
        "    )\n",
        "\n",
        "    XceptionNet2048_SVM(train_generator, test_generator, Batch_Name)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env (3.12.3)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
